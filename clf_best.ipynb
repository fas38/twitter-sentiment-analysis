{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:11.360838Z",
     "iopub.status.busy": "2023-08-14T04:57:11.360370Z",
     "iopub.status.idle": "2023-08-14T04:57:11.775196Z",
     "shell.execute_reply": "2023-08-14T04:57:11.774257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:11.778851Z",
     "iopub.status.busy": "2023-08-14T04:57:11.778182Z",
     "iopub.status.idle": "2023-08-14T04:57:11.839405Z",
     "shell.execute_reply": "2023-08-14T04:57:11.838457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:11.842278Z",
     "iopub.status.busy": "2023-08-14T04:57:11.841664Z",
     "iopub.status.idle": "2023-08-14T04:57:12.043205Z",
     "shell.execute_reply": "2023-08-14T04:57:12.042362Z"
    }
   },
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:12.045661Z",
     "iopub.status.busy": "2023-08-14T04:57:12.045280Z",
     "iopub.status.idle": "2023-08-14T04:57:12.049175Z",
     "shell.execute_reply": "2023-08-14T04:57:12.048577Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:12.051332Z",
     "iopub.status.busy": "2023-08-14T04:57:12.050927Z",
     "iopub.status.idle": "2023-08-14T04:57:12.121068Z",
     "shell.execute_reply": "2023-08-14T04:57:12.120334Z"
    }
   },
   "outputs": [],
   "source": [
    "df['words_str'] = df['text'].apply(preprocess)\n",
    "df_test['words_str'] = df_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:12.123403Z",
     "iopub.status.busy": "2023-08-14T04:57:12.122979Z",
     "iopub.status.idle": "2023-08-14T04:57:16.927858Z",
     "shell.execute_reply": "2023-08-14T04:57:16.926807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 13:27:03.650863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 13:27:04.355750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:16.931483Z",
     "iopub.status.busy": "2023-08-14T04:57:16.930710Z",
     "iopub.status.idle": "2023-08-14T04:57:16.935025Z",
     "shell.execute_reply": "2023-08-14T04:57:16.934367Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:16.937343Z",
     "iopub.status.busy": "2023-08-14T04:57:16.936893Z",
     "iopub.status.idle": "2023-08-14T04:57:16.943692Z",
     "shell.execute_reply": "2023-08-14T04:57:16.943009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes ['negative' 'neutral' 'positive']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "X: (8000,)\n",
      "y: (8000,) [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "X = df['words_str']\n",
    "y_text = df['sentiment']\n",
    "# y_text = df.sentiment.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_text)\n",
    "print(f'Original classes {le.classes_}')\n",
    "print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "y =le.transform(y_text)\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape} {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:16.946015Z",
     "iopub.status.busy": "2023-08-14T04:57:16.945547Z",
     "iopub.status.idle": "2023-08-14T04:57:16.951160Z",
     "shell.execute_reply": "2023-08-14T04:57:16.950499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:16.953846Z",
     "iopub.status.busy": "2023-08-14T04:57:16.953390Z",
     "iopub.status.idle": "2023-08-14T04:57:18.619501Z",
     "shell.execute_reply": "2023-08-14T04:57:18.618332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_twitter = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "tokenizer_twitter_sentiment = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:18.623618Z",
     "iopub.status.busy": "2023-08-14T04:57:18.623135Z",
     "iopub.status.idle": "2023-08-14T04:57:19.076975Z",
     "shell.execute_reply": "2023-08-14T04:57:19.075813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_twitter_sentiment\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:19.081018Z",
     "iopub.status.busy": "2023-08-14T04:57:19.080335Z",
     "iopub.status.idle": "2023-08-14T04:57:19.103364Z",
     "shell.execute_reply": "2023-08-14T04:57:19.102689Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.astype('int') # Change to integer type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to long type for classification\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            targets,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "\n",
    "class BertClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class RobertaClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    \n",
    "class RobertaClassificationTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter, self).__init__()\n",
    "        self.num_labels = 3\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Function to compute f1_macro\n",
    "def f1_macro(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {'f1_macro': f1_score(labels, predictions, average='macro')}\n",
    "\n",
    "\n",
    "\n",
    "class RobertaClassificationTwitter_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter_2, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "# class RobertaClassificationTwitter_3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RobertaClassificationTwitter_3, self).__init__()\n",
    "#         self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "#         # Adding an additional hidden layer\n",
    "#         self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "#         # Adding L2 regularization (weight decay) to the hidden layer\n",
    "#         self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "#         # Final classification layer with 3 classes\n",
    "#         self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, labels=None):\n",
    "#         outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs[1]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "#         # Passing through the hidden layer with ReLU activation\n",
    "#         hidden_output = self.hidden_layer(pooled_output)\n",
    "#         hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "#         # Applying Layer Normalization (regularization)\n",
    "#         hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "#         logits = self.classifier(hidden_output)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "#         return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "class RobertaClassificationTwitter_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter_3, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "    \n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics['eval_f1_macro']\n",
    "        if f1 > 0.77:\n",
    "            control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:19.105636Z",
     "iopub.status.busy": "2023-08-14T04:57:19.105183Z",
     "iopub.status.idle": "2023-08-14T04:57:19.108330Z",
     "shell.execute_reply": "2023-08-14T04:57:19.107684Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:19.110583Z",
     "iopub.status.busy": "2023-08-14T04:57:19.110137Z",
     "iopub.status.idle": "2023-08-14T04:57:22.103674Z",
     "shell.execute_reply": "2023-08-14T04:57:22.102586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertClassification.from_pretrained('bert-base-uncased')\n",
    "model_roberta = RobertaClassification.from_pretrained('roberta-base')\n",
    "model_twitter = RobertaClassificationTwitter_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T04:57:22.107657Z",
     "iopub.status.busy": "2023-08-14T04:57:22.107224Z",
     "iopub.status.idle": "2023-08-14T08:53:02.387511Z",
     "shell.execute_reply": "2023-08-14T08:53:02.386807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10400' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10400/25000 3:55:33 < 5:30:44, 0.74 it/s, Epoch 416/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.157991</td>\n",
       "      <td>0.696736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.158308</td>\n",
       "      <td>0.739416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.190606</td>\n",
       "      <td>0.735959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.247891</td>\n",
       "      <td>0.739504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.326379</td>\n",
       "      <td>0.736227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.369469</td>\n",
       "      <td>0.737477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.378922</td>\n",
       "      <td>0.757081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.377149</td>\n",
       "      <td>0.747492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.742174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.443613</td>\n",
       "      <td>0.757841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.450474</td>\n",
       "      <td>0.741171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.446873</td>\n",
       "      <td>0.746221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.450732</td>\n",
       "      <td>0.763317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.497277</td>\n",
       "      <td>0.744538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.463231</td>\n",
       "      <td>0.750019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.439003</td>\n",
       "      <td>0.750820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.484358</td>\n",
       "      <td>0.746465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.470409</td>\n",
       "      <td>0.762656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.458322</td>\n",
       "      <td>0.756179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.489701</td>\n",
       "      <td>0.753044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.513649</td>\n",
       "      <td>0.763188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.493531</td>\n",
       "      <td>0.750735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.485849</td>\n",
       "      <td>0.751786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.558142</td>\n",
       "      <td>0.753888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.524894</td>\n",
       "      <td>0.753740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.460024</td>\n",
       "      <td>0.744542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.504716</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.505454</td>\n",
       "      <td>0.745647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.455847</td>\n",
       "      <td>0.756744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.486118</td>\n",
       "      <td>0.748366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.508170</td>\n",
       "      <td>0.759826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.488463</td>\n",
       "      <td>0.750836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.757233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.485425</td>\n",
       "      <td>0.759606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.475060</td>\n",
       "      <td>0.750360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.491991</td>\n",
       "      <td>0.754589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.531379</td>\n",
       "      <td>0.758990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.516025</td>\n",
       "      <td>0.759343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.496414</td>\n",
       "      <td>0.758671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.507919</td>\n",
       "      <td>0.762936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.491603</td>\n",
       "      <td>0.756885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.500420</td>\n",
       "      <td>0.744303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.523920</td>\n",
       "      <td>0.739623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.557331</td>\n",
       "      <td>0.734969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.573222</td>\n",
       "      <td>0.733195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.493010</td>\n",
       "      <td>0.749926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.507116</td>\n",
       "      <td>0.755443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.472097</td>\n",
       "      <td>0.760029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.551776</td>\n",
       "      <td>0.728606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.502759</td>\n",
       "      <td>0.753629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.530356</td>\n",
       "      <td>0.765308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.552855</td>\n",
       "      <td>0.749418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.489505</td>\n",
       "      <td>0.754512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.507046</td>\n",
       "      <td>0.765070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.519840</td>\n",
       "      <td>0.765038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.540555</td>\n",
       "      <td>0.757029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.568121</td>\n",
       "      <td>0.741081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.518992</td>\n",
       "      <td>0.749139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.539507</td>\n",
       "      <td>0.763760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.542623</td>\n",
       "      <td>0.764819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.528868</td>\n",
       "      <td>0.752131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.529360</td>\n",
       "      <td>0.740775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.525196</td>\n",
       "      <td>0.756559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.541640</td>\n",
       "      <td>0.768322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.588586</td>\n",
       "      <td>0.760394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.588053</td>\n",
       "      <td>0.759216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.523321</td>\n",
       "      <td>0.749037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.753358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.573334</td>\n",
       "      <td>0.740028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.574943</td>\n",
       "      <td>0.734254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.524624</td>\n",
       "      <td>0.750403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.521650</td>\n",
       "      <td>0.751479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.562025</td>\n",
       "      <td>0.757606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.526921</td>\n",
       "      <td>0.749128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.533561</td>\n",
       "      <td>0.737829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.578552</td>\n",
       "      <td>0.744028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.582999</td>\n",
       "      <td>0.728154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.561926</td>\n",
       "      <td>0.761396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.553849</td>\n",
       "      <td>0.751882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.605843</td>\n",
       "      <td>0.742059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.451001</td>\n",
       "      <td>0.740637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.538475</td>\n",
       "      <td>0.745712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.482275</td>\n",
       "      <td>0.753853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.501161</td>\n",
       "      <td>0.759890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.492660</td>\n",
       "      <td>0.765959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.492679</td>\n",
       "      <td>0.767053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.521241</td>\n",
       "      <td>0.765209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.590936</td>\n",
       "      <td>0.743567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.543389</td>\n",
       "      <td>0.745054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.563519</td>\n",
       "      <td>0.754165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.530289</td>\n",
       "      <td>0.761270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575765</td>\n",
       "      <td>0.762332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.566413</td>\n",
       "      <td>0.756317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559076</td>\n",
       "      <td>0.746961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.565670</td>\n",
       "      <td>0.754239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699238</td>\n",
       "      <td>0.736831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.494840</td>\n",
       "      <td>0.756543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.539442</td>\n",
       "      <td>0.761160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.566988</td>\n",
       "      <td>0.759191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.486453</td>\n",
       "      <td>0.768622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.512247</td>\n",
       "      <td>0.761499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498986</td>\n",
       "      <td>0.752148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527161</td>\n",
       "      <td>0.761893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521261</td>\n",
       "      <td>0.775724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5212607383728027, 'eval_f1_macro': 0.7757241371340746, 'eval_runtime': 2.0557, 'eval_samples_per_second': 778.326, 'eval_steps_per_second': 3.405, 'epoch': 416.0}\n"
     ]
    }
   ],
   "source": [
    "model = model_twitter.to(device)\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=1000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type='cosine',  # Using a cosine scheduler\n",
    "    warmup_steps=100  # Number of warmup steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=f1_macro,\n",
    "    callbacks=[ThresholdEarlyStoppingCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T08:53:02.389776Z",
     "iopub.status.busy": "2023-08-14T08:53:02.389599Z",
     "iopub.status.idle": "2023-08-14T08:53:03.075208Z",
     "shell.execute_reply": "2023-08-14T08:53:03.074381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('pretrained_models/roberta-base-twitter-clf')\n",
    "\n",
    "# model = BertRegression.from_pretrained(\"./path/to/save/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T08:53:03.078435Z",
     "iopub.status.busy": "2023-08-14T08:53:03.078258Z",
     "iopub.status.idle": "2023-08-14T08:53:03.082333Z",
     "shell.execute_reply": "2023-08-14T08:53:03.081910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a dataset without labels for testing\n",
    "class ClassificationTestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T08:53:03.084181Z",
     "iopub.status.busy": "2023-08-14T08:53:03.084023Z",
     "iopub.status.idle": "2023-08-14T08:53:04.281863Z",
     "shell.execute_reply": "2023-08-14T08:53:04.280896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the test sentences\n",
    "sentences = list(df_test.words_str.values)\n",
    "test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "# Convert to a PyTorch Dataset (using the renamed class)\n",
    "test_dataset = ClassificationTestDataset(test_encodings)\n",
    "\n",
    "# Get predictions with the neural network\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_hat_prob_tensor = torch.tensor(predictions.predictions, dtype=torch.float32)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "y_hat_labels = torch.argmax(y_hat_prob_tensor, dim=1).cpu().numpy()\n",
    "\n",
    "# revert the label encoding\n",
    "y_hat_labels = le.inverse_transform(y_hat_labels)\n",
    "\n",
    "# Save the results with the specified format\n",
    "directory = 'results'\n",
    "np.save(os.path.join(directory, f'{team_id}__{split}__clf_pred.npy'), y_hat_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T08:53:04.284824Z",
     "iopub.status.busy": "2023-08-14T08:53:04.284244Z",
     "iopub.status.idle": "2023-08-14T08:53:04.289482Z",
     "shell.execute_reply": "2023-08-14T08:53:04.288859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 20__test_1__reg_pred.npy\n",
    "\n",
    "d = np.load('results/20__test_1__clf_pred.npy', allow_pickle=True)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing if the model is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('results/20__test_1__clf_pred.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_path = 'pretrained_models/roberta-base-twitter-clf/training_args.bin'\n",
    "training_args = TrainingArguments(training_args_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClassificationTwitter_3(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_layer): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (regularization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'pretrained_models/roberta-base-twitter-clf/pytorch_model.bin'\n",
    "model = RobertaClassificationTwitter_3()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval() # Set to evaluation mode if doing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df_test.words_str.values)\n",
    "test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "test_dataset = ClassificationTestDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    dataloader = DataLoader(dataset, batch_size=32)  # You can adjust the batch size\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=-1).cpu().numpy()\n",
    "            predictions.extend(predicted_classes)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_labels_indices = predict(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_labels = le.inverse_transform(y_hat_labels_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y is your true labels and y_hat_labels is your predicted labels\n",
    "accuracy = accuracy_score(y, y_hat_labels)\n",
    "precision = precision_score(y, y_hat_labels, average='macro') # or 'micro', 'weighted', depending on your task\n",
    "recall = recall_score(y, y_hat_labels, average='macro')\n",
    "f1 = f1_score(y, y_hat_labels, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
