{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>author_id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>words</th>\n",
       "      <th>score_compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>words_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532325760148590593</td>\n",
       "      <td>@xbresson British Alps :-)</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>62044012</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33022</td>\n",
       "      <td>3383</td>\n",
       "      <td>5246</td>\n",
       "      <td>359</td>\n",
       "      <td>['british', 'alps']</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>british alps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1312410689059913731</td>\n",
       "      <td>RT @Aistats2020: Videos presentations of paper...</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1290762290224984064</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17570</td>\n",
       "      <td>541</td>\n",
       "      <td>703</td>\n",
       "      <td>222</td>\n",
       "      <td>['videos', 'presentations', 'papers', 'keynote...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>videos presentations papers keynote talks aist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1564467011781926913</td>\n",
       "      <td>I hope I would be able to talk more about this...</td>\n",
       "      <td>quoted</td>\n",
       "      <td>3363584909</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>65506</td>\n",
       "      <td>113</td>\n",
       "      <td>15406</td>\n",
       "      <td>856</td>\n",
       "      <td>['hope', 'would', 'able', 'talk', 'balcony', '...</td>\n",
       "      <td>0.4862</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hope would able talk balcony tomorrow pm et al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1473395733344788481</td>\n",
       "      <td>RT @dlbcnai: Keynote by Joan Bruna (@joanbruna...</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1071640880</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21847</td>\n",
       "      <td>555</td>\n",
       "      <td>1346</td>\n",
       "      <td>264</td>\n",
       "      <td>['keynote', 'joan', 'bruna', 'geometric', 'dee...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>neutral</td>\n",
       "      <td>keynote joan bruna geometric deep learning pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1611495430843502593</td>\n",
       "      <td>@annargrs @Michael_J_Black @AllenHW0 @CSProfKG...</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>48008938</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>427851</td>\n",
       "      <td>582</td>\n",
       "      <td>12949</td>\n",
       "      <td>6488</td>\n",
       "      <td>['process', 'science', 'relies', 'much', 'basi...</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>neutral</td>\n",
       "      <td>process science relies much basic honesty part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1228672069333848064</td>\n",
       "      <td>RT @geoffreyhinton: Unsupervised learning of r...</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1071640880</td>\n",
       "      <td>False</td>\n",
       "      <td>519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21847</td>\n",
       "      <td>555</td>\n",
       "      <td>1346</td>\n",
       "      <td>264</td>\n",
       "      <td>['unsupervised', 'learning', 'representations'...</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unsupervised learning representations beginnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1595340090963857410</td>\n",
       "      <td>@prem_k Relax, it's a joke!</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>48008938</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>427851</td>\n",
       "      <td>582</td>\n",
       "      <td>12949</td>\n",
       "      <td>6488</td>\n",
       "      <td>['relax', 'joke']</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>positive</td>\n",
       "      <td>relax joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1585623984858869765</td>\n",
       "      <td>Postdoctoral position available in Computation...</td>\n",
       "      <td>tweet</td>\n",
       "      <td>62044012</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>33022</td>\n",
       "      <td>3383</td>\n",
       "      <td>5246</td>\n",
       "      <td>359</td>\n",
       "      <td>['postdoctoral', 'position', 'available', 'com...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>neutral</td>\n",
       "      <td>postdoctoral position available computational ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1529505378911891457</td>\n",
       "      <td>@textfiles I did note the irony of being somew...</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>175624200</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1031629</td>\n",
       "      <td>225</td>\n",
       "      <td>16316</td>\n",
       "      <td>6967</td>\n",
       "      <td>['note', 'irony', 'somewhat', 'phone', 'phreak...</td>\n",
       "      <td>-0.3969</td>\n",
       "      <td>neutral</td>\n",
       "      <td>note irony somewhat phone phreak ham radio spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1624987333522239489</td>\n",
       "      <td>@repligate @reality__gamer are you sure it nev...</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>232294292</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>86646</td>\n",
       "      <td>5312</td>\n",
       "      <td>26291</td>\n",
       "      <td>2422</td>\n",
       "      <td>['sure', 'never', 'made', 'illegal', 'moves', ...</td>\n",
       "      <td>0.4449</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure never made illegal moves add additional c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1532325760148590593                         @xbresson British Alps :-)   \n",
       "1  1312410689059913731  RT @Aistats2020: Videos presentations of paper...   \n",
       "2  1564467011781926913  I hope I would be able to talk more about this...   \n",
       "3  1473395733344788481  RT @dlbcnai: Keynote by Joan Bruna (@joanbruna...   \n",
       "4  1611495430843502593  @annargrs @Michael_J_Black @AllenHW0 @CSProfKG...   \n",
       "5  1228672069333848064  RT @geoffreyhinton: Unsupervised learning of r...   \n",
       "6  1595340090963857410                        @prem_k Relax, it's a joke!   \n",
       "7  1585623984858869765  Postdoctoral position available in Computation...   \n",
       "8  1529505378911891457  @textfiles I did note the irony of being somew...   \n",
       "9  1624987333522239489  @repligate @reality__gamer are you sure it nev...   \n",
       "\n",
       "         type            author_id  possibly_sensitive  retweet_count  \\\n",
       "0  replied_to             62044012               False              0   \n",
       "1   retweeted  1290762290224984064               False             52   \n",
       "2      quoted           3363584909               False              1   \n",
       "3   retweeted           1071640880               False             35   \n",
       "4  replied_to             48008938               False              0   \n",
       "5   retweeted           1071640880               False            519   \n",
       "6  replied_to             48008938               False              0   \n",
       "7       tweet             62044012               False             18   \n",
       "8  replied_to            175624200               False              0   \n",
       "9  replied_to            232294292               False              0   \n",
       "\n",
       "   quote_count  reply_count  like_count  followers_count  following_count  \\\n",
       "0            0            1           2            33022             3383   \n",
       "1            0            0           0            17570              541   \n",
       "2            0            0           3            65506              113   \n",
       "3            0            0           0            21847              555   \n",
       "4            0            1           5           427851              582   \n",
       "5            0            0           0            21847              555   \n",
       "6            0            2           2           427851              582   \n",
       "7            1            0          68            33022             3383   \n",
       "8            0            2          20          1031629              225   \n",
       "9            0            2           0            86646             5312   \n",
       "\n",
       "   tweet_count  listed_count  \\\n",
       "0         5246           359   \n",
       "1          703           222   \n",
       "2        15406           856   \n",
       "3         1346           264   \n",
       "4        12949          6488   \n",
       "5         1346           264   \n",
       "6        12949          6488   \n",
       "7         5246           359   \n",
       "8        16316          6967   \n",
       "9        26291          2422   \n",
       "\n",
       "                                               words  score_compound  \\\n",
       "0                                ['british', 'alps']          0.0000   \n",
       "1  ['videos', 'presentations', 'papers', 'keynote...          0.0000   \n",
       "2  ['hope', 'would', 'able', 'talk', 'balcony', '...          0.4862   \n",
       "3  ['keynote', 'joan', 'bruna', 'geometric', 'dee...          0.3612   \n",
       "4  ['process', 'science', 'relies', 'much', 'basi...         -0.3182   \n",
       "5  ['unsupervised', 'learning', 'representations'...          0.3384   \n",
       "6                                  ['relax', 'joke']          0.6249   \n",
       "7  ['postdoctoral', 'position', 'available', 'com...          0.4404   \n",
       "8  ['note', 'irony', 'somewhat', 'phone', 'phreak...         -0.3969   \n",
       "9  ['sure', 'never', 'made', 'illegal', 'moves', ...          0.4449   \n",
       "\n",
       "  sentiment                                          words_str  \n",
       "0   neutral                                       british alps  \n",
       "1   neutral  videos presentations papers keynote talks aist...  \n",
       "2   neutral  hope would able talk balcony tomorrow pm et al...  \n",
       "3   neutral  keynote joan bruna geometric deep learning pro...  \n",
       "4   neutral  process science relies much basic honesty part...  \n",
       "5   neutral  unsupervised learning representations beginnin...  \n",
       "6  positive                                         relax joke  \n",
       "7   neutral  postdoctoral position available computational ...  \n",
       "8   neutral  note irony somewhat phone phreak ham radio spa...  \n",
       "9  positive  sure never made illegal moves add additional c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Specify the model name\n",
    "# I downloaded in models directory three models: \n",
    "# all-MiniLM-L6-v2(Fastest but smalles), stsb-roberta-base-v2(middle), and stsb-mpnet-base-v2(Slowest, but best performance)\n",
    "\n",
    "name = 'stsb-mpnet-base-v2'\n",
    "\n",
    "\n",
    "# Download and save the model if not available\n",
    "#model = SentenceTransformer(name)\n",
    "\n",
    "#model.save('models/' + name)\n",
    "\n",
    "# Load the model\n",
    "model_enc = SentenceTransformer('models/' + name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences to get embeddings for\n",
    "sentences = list(df.words_str.values)\n",
    "\n",
    "# Get the sentence embeddings\n",
    "sentence_embeddings = model_enc.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentence_embeddings\n",
    "y = df.score_compound.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split X and y for training and validation purposes\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.5):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        \n",
    "        # First set of layers\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Second set of layers\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First set of layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Second set of layers\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch [111/10000000], Training Loss: 0.1414, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n",
      "Epoch [121/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3762, Validation RMSE: 0.3654\n",
      "Epoch [131/10000000], Training Loss: 0.1413, Validation Loss: 0.1335, Training RMSE: 0.3759, Validation RMSE: 0.3654\n",
      "Epoch [141/10000000], Training Loss: 0.1417, Validation Loss: 0.1335, Training RMSE: 0.3764, Validation RMSE: 0.3654\n",
      "Epoch [151/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n",
      "Epoch [161/10000000], Training Loss: 0.1419, Validation Loss: 0.1335, Training RMSE: 0.3766, Validation RMSE: 0.3654\n",
      "Epoch [171/10000000], Training Loss: 0.1413, Validation Loss: 0.1335, Training RMSE: 0.3758, Validation RMSE: 0.3654\n",
      "Epoch [181/10000000], Training Loss: 0.1414, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n",
      "Epoch [191/10000000], Training Loss: 0.1414, Validation Loss: 0.1335, Training RMSE: 0.3760, Validation RMSE: 0.3654\n",
      "Epoch [201/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/10000000], Training Loss: 0.1418, Validation Loss: 0.1335, Training RMSE: 0.3766, Validation RMSE: 0.3654\n",
      "Epoch [221/10000000], Training Loss: 0.1414, Validation Loss: 0.1335, Training RMSE: 0.3760, Validation RMSE: 0.3654\n",
      "Epoch [231/10000000], Training Loss: 0.1412, Validation Loss: 0.1335, Training RMSE: 0.3757, Validation RMSE: 0.3654\n",
      "Epoch [241/10000000], Training Loss: 0.1418, Validation Loss: 0.1335, Training RMSE: 0.3766, Validation RMSE: 0.3654\n",
      "Epoch [251/10000000], Training Loss: 0.1417, Validation Loss: 0.1335, Training RMSE: 0.3764, Validation RMSE: 0.3654\n",
      "Epoch [261/10000000], Training Loss: 0.1413, Validation Loss: 0.1335, Training RMSE: 0.3758, Validation RMSE: 0.3654\n",
      "Epoch [271/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n",
      "Epoch [281/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3762, Validation RMSE: 0.3654\n",
      "Epoch [291/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3763, Validation RMSE: 0.3654\n",
      "Epoch [351/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3763, Validation RMSE: 0.3654\n",
      "Epoch [401/10000000], Training Loss: 0.1411, Validation Loss: 0.1335, Training RMSE: 0.3756, Validation RMSE: 0.3654\n",
      "Epoch [451/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3763, Validation RMSE: 0.3654\n",
      "Epoch [501/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3762, Validation RMSE: 0.3654\n",
      "Epoch [551/10000000], Training Loss: 0.1410, Validation Loss: 0.1335, Training RMSE: 0.3755, Validation RMSE: 0.3654\n",
      "Epoch [601/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3763, Validation RMSE: 0.3654\n",
      "Epoch [651/10000000], Training Loss: 0.1412, Validation Loss: 0.1335, Training RMSE: 0.3757, Validation RMSE: 0.3654\n",
      "Epoch [701/10000000], Training Loss: 0.1416, Validation Loss: 0.1335, Training RMSE: 0.3763, Validation RMSE: 0.3654\n",
      "Epoch [751/10000000], Training Loss: 0.1412, Validation Loss: 0.1335, Training RMSE: 0.3757, Validation RMSE: 0.3654\n",
      "Epoch [801/10000000], Training Loss: 0.1414, Validation Loss: 0.1335, Training RMSE: 0.3760, Validation RMSE: 0.3654\n",
      "Epoch [851/10000000], Training Loss: 0.1417, Validation Loss: 0.1335, Training RMSE: 0.3764, Validation RMSE: 0.3654\n",
      "Epoch [901/10000000], Training Loss: 0.1415, Validation Loss: 0.1335, Training RMSE: 0.3761, Validation RMSE: 0.3654\n",
      "Epoch [951/10000000], Training Loss: 0.1413, Validation Loss: 0.1335, Training RMSE: 0.3759, Validation RMSE: 0.3654\n",
      "Epoch [2001/10000000], Training Loss: 0.1413, Validation Loss: 0.1334, Training RMSE: 0.3759, Validation RMSE: 0.3653\n",
      "Epoch [3001/10000000], Training Loss: 0.1412, Validation Loss: 0.1333, Training RMSE: 0.3758, Validation RMSE: 0.3652\n",
      "Epoch [4001/10000000], Training Loss: 0.1416, Validation Loss: 0.1333, Training RMSE: 0.3763, Validation RMSE: 0.3651\n",
      "Epoch [5001/10000000], Training Loss: 0.1410, Validation Loss: 0.1332, Training RMSE: 0.3755, Validation RMSE: 0.3650\n",
      "Epoch [6001/10000000], Training Loss: 0.1413, Validation Loss: 0.1331, Training RMSE: 0.3758, Validation RMSE: 0.3649\n",
      "Epoch [7001/10000000], Training Loss: 0.1413, Validation Loss: 0.1331, Training RMSE: 0.3759, Validation RMSE: 0.3648\n",
      "Epoch [8001/10000000], Training Loss: 0.1408, Validation Loss: 0.1330, Training RMSE: 0.3752, Validation RMSE: 0.3647\n",
      "Epoch [9001/10000000], Training Loss: 0.1408, Validation Loss: 0.1329, Training RMSE: 0.3753, Validation RMSE: 0.3646\n",
      "Epoch [10001/10000000], Training Loss: 0.1407, Validation Loss: 0.1329, Training RMSE: 0.3751, Validation RMSE: 0.3645\n",
      "Epoch [11001/10000000], Training Loss: 0.1407, Validation Loss: 0.1328, Training RMSE: 0.3751, Validation RMSE: 0.3644\n",
      "Epoch [12001/10000000], Training Loss: 0.1409, Validation Loss: 0.1327, Training RMSE: 0.3753, Validation RMSE: 0.3643\n",
      "Epoch [13001/10000000], Training Loss: 0.1406, Validation Loss: 0.1327, Training RMSE: 0.3749, Validation RMSE: 0.3643\n",
      "Epoch [14001/10000000], Training Loss: 0.1405, Validation Loss: 0.1326, Training RMSE: 0.3749, Validation RMSE: 0.3642\n",
      "Epoch [15001/10000000], Training Loss: 0.1406, Validation Loss: 0.1326, Training RMSE: 0.3749, Validation RMSE: 0.3641\n",
      "Epoch [16001/10000000], Training Loss: 0.1405, Validation Loss: 0.1325, Training RMSE: 0.3748, Validation RMSE: 0.3640\n",
      "Epoch [17001/10000000], Training Loss: 0.1404, Validation Loss: 0.1324, Training RMSE: 0.3747, Validation RMSE: 0.3639\n",
      "Epoch [18001/10000000], Training Loss: 0.1401, Validation Loss: 0.1324, Training RMSE: 0.3743, Validation RMSE: 0.3638\n",
      "Epoch [19001/10000000], Training Loss: 0.1399, Validation Loss: 0.1323, Training RMSE: 0.3740, Validation RMSE: 0.3637\n",
      "Epoch [20001/10000000], Training Loss: 0.1401, Validation Loss: 0.1322, Training RMSE: 0.3744, Validation RMSE: 0.3636\n",
      "Epoch [21001/10000000], Training Loss: 0.1400, Validation Loss: 0.1322, Training RMSE: 0.3742, Validation RMSE: 0.3635\n",
      "Epoch [22001/10000000], Training Loss: 0.1399, Validation Loss: 0.1321, Training RMSE: 0.3740, Validation RMSE: 0.3634\n",
      "Epoch [23001/10000000], Training Loss: 0.1396, Validation Loss: 0.1320, Training RMSE: 0.3736, Validation RMSE: 0.3633\n",
      "Epoch [24001/10000000], Training Loss: 0.1400, Validation Loss: 0.1320, Training RMSE: 0.3742, Validation RMSE: 0.3633\n",
      "Epoch [25001/10000000], Training Loss: 0.1399, Validation Loss: 0.1319, Training RMSE: 0.3740, Validation RMSE: 0.3632\n",
      "Epoch [26001/10000000], Training Loss: 0.1395, Validation Loss: 0.1318, Training RMSE: 0.3735, Validation RMSE: 0.3631\n",
      "Epoch [27001/10000000], Training Loss: 0.1396, Validation Loss: 0.1318, Training RMSE: 0.3736, Validation RMSE: 0.3630\n",
      "Epoch [28001/10000000], Training Loss: 0.1396, Validation Loss: 0.1317, Training RMSE: 0.3736, Validation RMSE: 0.3629\n",
      "Epoch [29001/10000000], Training Loss: 0.1394, Validation Loss: 0.1316, Training RMSE: 0.3733, Validation RMSE: 0.3628\n",
      "Epoch [30001/10000000], Training Loss: 0.1391, Validation Loss: 0.1316, Training RMSE: 0.3730, Validation RMSE: 0.3627\n",
      "Epoch [31001/10000000], Training Loss: 0.1392, Validation Loss: 0.1315, Training RMSE: 0.3731, Validation RMSE: 0.3626\n",
      "Epoch [32001/10000000], Training Loss: 0.1394, Validation Loss: 0.1314, Training RMSE: 0.3733, Validation RMSE: 0.3625\n",
      "Epoch [33001/10000000], Training Loss: 0.1393, Validation Loss: 0.1314, Training RMSE: 0.3733, Validation RMSE: 0.3624\n",
      "Epoch [34001/10000000], Training Loss: 0.1392, Validation Loss: 0.1313, Training RMSE: 0.3731, Validation RMSE: 0.3623\n",
      "Epoch [35001/10000000], Training Loss: 0.1390, Validation Loss: 0.1312, Training RMSE: 0.3728, Validation RMSE: 0.3623\n",
      "Epoch [36001/10000000], Training Loss: 0.1391, Validation Loss: 0.1312, Training RMSE: 0.3730, Validation RMSE: 0.3622\n",
      "Epoch [37001/10000000], Training Loss: 0.1394, Validation Loss: 0.1311, Training RMSE: 0.3733, Validation RMSE: 0.3621\n",
      "Epoch [38001/10000000], Training Loss: 0.1390, Validation Loss: 0.1310, Training RMSE: 0.3728, Validation RMSE: 0.3620\n",
      "Epoch [39001/10000000], Training Loss: 0.1389, Validation Loss: 0.1310, Training RMSE: 0.3727, Validation RMSE: 0.3619\n",
      "Epoch [40001/10000000], Training Loss: 0.1387, Validation Loss: 0.1309, Training RMSE: 0.3724, Validation RMSE: 0.3618\n",
      "Epoch [41001/10000000], Training Loss: 0.1390, Validation Loss: 0.1308, Training RMSE: 0.3728, Validation RMSE: 0.3617\n",
      "Epoch [42001/10000000], Training Loss: 0.1386, Validation Loss: 0.1308, Training RMSE: 0.3722, Validation RMSE: 0.3616\n",
      "Epoch [43001/10000000], Training Loss: 0.1389, Validation Loss: 0.1307, Training RMSE: 0.3727, Validation RMSE: 0.3615\n",
      "Epoch [44001/10000000], Training Loss: 0.1383, Validation Loss: 0.1306, Training RMSE: 0.3719, Validation RMSE: 0.3614\n",
      "Epoch [45001/10000000], Training Loss: 0.1384, Validation Loss: 0.1306, Training RMSE: 0.3720, Validation RMSE: 0.3613\n",
      "Epoch [46001/10000000], Training Loss: 0.1386, Validation Loss: 0.1305, Training RMSE: 0.3723, Validation RMSE: 0.3613\n",
      "Epoch [47001/10000000], Training Loss: 0.1383, Validation Loss: 0.1304, Training RMSE: 0.3719, Validation RMSE: 0.3612\n",
      "Epoch [48001/10000000], Training Loss: 0.1382, Validation Loss: 0.1304, Training RMSE: 0.3718, Validation RMSE: 0.3611\n",
      "Epoch [49001/10000000], Training Loss: 0.1381, Validation Loss: 0.1303, Training RMSE: 0.3717, Validation RMSE: 0.3610\n",
      "Epoch [50001/10000000], Training Loss: 0.1384, Validation Loss: 0.1302, Training RMSE: 0.3720, Validation RMSE: 0.3609\n",
      "Epoch [51001/10000000], Training Loss: 0.1380, Validation Loss: 0.1302, Training RMSE: 0.3715, Validation RMSE: 0.3608\n",
      "Epoch [52001/10000000], Training Loss: 0.1383, Validation Loss: 0.1301, Training RMSE: 0.3718, Validation RMSE: 0.3607\n",
      "Epoch [53001/10000000], Training Loss: 0.1377, Validation Loss: 0.1300, Training RMSE: 0.3711, Validation RMSE: 0.3606\n",
      "Epoch [54001/10000000], Training Loss: 0.1384, Validation Loss: 0.1300, Training RMSE: 0.3720, Validation RMSE: 0.3605\n",
      "Epoch [55001/10000000], Training Loss: 0.1376, Validation Loss: 0.1299, Training RMSE: 0.3710, Validation RMSE: 0.3604\n",
      "Epoch [56001/10000000], Training Loss: 0.1379, Validation Loss: 0.1298, Training RMSE: 0.3714, Validation RMSE: 0.3603\n",
      "Epoch [57001/10000000], Training Loss: 0.1379, Validation Loss: 0.1298, Training RMSE: 0.3714, Validation RMSE: 0.3602\n",
      "Epoch [58001/10000000], Training Loss: 0.1379, Validation Loss: 0.1297, Training RMSE: 0.3713, Validation RMSE: 0.3601\n",
      "Epoch [59001/10000000], Training Loss: 0.1375, Validation Loss: 0.1296, Training RMSE: 0.3708, Validation RMSE: 0.3601\n",
      "Epoch [60001/10000000], Training Loss: 0.1375, Validation Loss: 0.1296, Training RMSE: 0.3708, Validation RMSE: 0.3600\n",
      "Epoch [61001/10000000], Training Loss: 0.1374, Validation Loss: 0.1295, Training RMSE: 0.3706, Validation RMSE: 0.3599\n",
      "Epoch [62001/10000000], Training Loss: 0.1371, Validation Loss: 0.1294, Training RMSE: 0.3703, Validation RMSE: 0.3598\n",
      "Epoch [63001/10000000], Training Loss: 0.1374, Validation Loss: 0.1294, Training RMSE: 0.3707, Validation RMSE: 0.3597\n",
      "Epoch [64001/10000000], Training Loss: 0.1372, Validation Loss: 0.1293, Training RMSE: 0.3704, Validation RMSE: 0.3596\n",
      "Epoch [65001/10000000], Training Loss: 0.1374, Validation Loss: 0.1292, Training RMSE: 0.3707, Validation RMSE: 0.3595\n",
      "Epoch [66001/10000000], Training Loss: 0.1370, Validation Loss: 0.1292, Training RMSE: 0.3702, Validation RMSE: 0.3594\n",
      "Epoch [67001/10000000], Training Loss: 0.1373, Validation Loss: 0.1291, Training RMSE: 0.3705, Validation RMSE: 0.3593\n",
      "Epoch [68001/10000000], Training Loss: 0.1368, Validation Loss: 0.1290, Training RMSE: 0.3699, Validation RMSE: 0.3592\n",
      "Epoch [69001/10000000], Training Loss: 0.1370, Validation Loss: 0.1290, Training RMSE: 0.3702, Validation RMSE: 0.3591\n",
      "Epoch [70001/10000000], Training Loss: 0.1371, Validation Loss: 0.1289, Training RMSE: 0.3703, Validation RMSE: 0.3590\n",
      "Epoch [71001/10000000], Training Loss: 0.1369, Validation Loss: 0.1288, Training RMSE: 0.3700, Validation RMSE: 0.3589\n",
      "Epoch [72001/10000000], Training Loss: 0.1367, Validation Loss: 0.1288, Training RMSE: 0.3697, Validation RMSE: 0.3588\n",
      "Epoch [73001/10000000], Training Loss: 0.1365, Validation Loss: 0.1287, Training RMSE: 0.3695, Validation RMSE: 0.3588\n",
      "Epoch [74001/10000000], Training Loss: 0.1365, Validation Loss: 0.1286, Training RMSE: 0.3694, Validation RMSE: 0.3587\n",
      "Epoch [75001/10000000], Training Loss: 0.1366, Validation Loss: 0.1286, Training RMSE: 0.3697, Validation RMSE: 0.3586\n",
      "Epoch [76001/10000000], Training Loss: 0.1365, Validation Loss: 0.1285, Training RMSE: 0.3694, Validation RMSE: 0.3585\n",
      "Epoch [77001/10000000], Training Loss: 0.1365, Validation Loss: 0.1284, Training RMSE: 0.3694, Validation RMSE: 0.3584\n",
      "Epoch [78001/10000000], Training Loss: 0.1361, Validation Loss: 0.1284, Training RMSE: 0.3689, Validation RMSE: 0.3583\n",
      "Epoch [79001/10000000], Training Loss: 0.1363, Validation Loss: 0.1283, Training RMSE: 0.3692, Validation RMSE: 0.3582\n",
      "Epoch [80001/10000000], Training Loss: 0.1360, Validation Loss: 0.1282, Training RMSE: 0.3688, Validation RMSE: 0.3581\n",
      "Epoch [81001/10000000], Training Loss: 0.1362, Validation Loss: 0.1282, Training RMSE: 0.3690, Validation RMSE: 0.3580\n",
      "Epoch [82001/10000000], Training Loss: 0.1362, Validation Loss: 0.1281, Training RMSE: 0.3690, Validation RMSE: 0.3579\n",
      "Epoch [83001/10000000], Training Loss: 0.1362, Validation Loss: 0.1280, Training RMSE: 0.3691, Validation RMSE: 0.3578\n",
      "Epoch [84001/10000000], Training Loss: 0.1359, Validation Loss: 0.1280, Training RMSE: 0.3687, Validation RMSE: 0.3577\n",
      "Epoch [85001/10000000], Training Loss: 0.1358, Validation Loss: 0.1279, Training RMSE: 0.3684, Validation RMSE: 0.3576\n",
      "Epoch [86001/10000000], Training Loss: 0.1359, Validation Loss: 0.1278, Training RMSE: 0.3687, Validation RMSE: 0.3575\n",
      "Epoch [87001/10000000], Training Loss: 0.1357, Validation Loss: 0.1278, Training RMSE: 0.3684, Validation RMSE: 0.3574\n",
      "Epoch [88001/10000000], Training Loss: 0.1357, Validation Loss: 0.1277, Training RMSE: 0.3683, Validation RMSE: 0.3573\n",
      "Epoch [89001/10000000], Training Loss: 0.1360, Validation Loss: 0.1276, Training RMSE: 0.3688, Validation RMSE: 0.3572\n",
      "Epoch [90001/10000000], Training Loss: 0.1358, Validation Loss: 0.1276, Training RMSE: 0.3685, Validation RMSE: 0.3571\n",
      "Epoch [91001/10000000], Training Loss: 0.1356, Validation Loss: 0.1275, Training RMSE: 0.3682, Validation RMSE: 0.3571\n",
      "Epoch [92001/10000000], Training Loss: 0.1355, Validation Loss: 0.1274, Training RMSE: 0.3681, Validation RMSE: 0.3570\n",
      "Epoch [93001/10000000], Training Loss: 0.1355, Validation Loss: 0.1273, Training RMSE: 0.3681, Validation RMSE: 0.3569\n",
      "Epoch [94001/10000000], Training Loss: 0.1351, Validation Loss: 0.1273, Training RMSE: 0.3675, Validation RMSE: 0.3568\n",
      "Epoch [95001/10000000], Training Loss: 0.1352, Validation Loss: 0.1272, Training RMSE: 0.3677, Validation RMSE: 0.3567\n",
      "Epoch [96001/10000000], Training Loss: 0.1351, Validation Loss: 0.1271, Training RMSE: 0.3676, Validation RMSE: 0.3566\n",
      "Epoch [97001/10000000], Training Loss: 0.1352, Validation Loss: 0.1271, Training RMSE: 0.3677, Validation RMSE: 0.3565\n",
      "Epoch [98001/10000000], Training Loss: 0.1354, Validation Loss: 0.1270, Training RMSE: 0.3680, Validation RMSE: 0.3564\n",
      "Epoch [99001/10000000], Training Loss: 0.1353, Validation Loss: 0.1269, Training RMSE: 0.3679, Validation RMSE: 0.3563\n",
      "Epoch [100001/10000000], Training Loss: 0.1349, Validation Loss: 0.1269, Training RMSE: 0.3672, Validation RMSE: 0.3562\n",
      "Epoch [101001/10000000], Training Loss: 0.1348, Validation Loss: 0.1268, Training RMSE: 0.3671, Validation RMSE: 0.3561\n",
      "Epoch [102001/10000000], Training Loss: 0.1349, Validation Loss: 0.1267, Training RMSE: 0.3672, Validation RMSE: 0.3560\n",
      "Epoch [103001/10000000], Training Loss: 0.1349, Validation Loss: 0.1267, Training RMSE: 0.3673, Validation RMSE: 0.3559\n",
      "Epoch [104001/10000000], Training Loss: 0.1346, Validation Loss: 0.1266, Training RMSE: 0.3669, Validation RMSE: 0.3558\n",
      "Epoch [105001/10000000], Training Loss: 0.1345, Validation Loss: 0.1265, Training RMSE: 0.3667, Validation RMSE: 0.3557\n",
      "Epoch [106001/10000000], Training Loss: 0.1344, Validation Loss: 0.1265, Training RMSE: 0.3666, Validation RMSE: 0.3556\n",
      "Epoch [107001/10000000], Training Loss: 0.1345, Validation Loss: 0.1264, Training RMSE: 0.3667, Validation RMSE: 0.3555\n",
      "Epoch [108001/10000000], Training Loss: 0.1344, Validation Loss: 0.1263, Training RMSE: 0.3666, Validation RMSE: 0.3554\n",
      "Epoch [109001/10000000], Training Loss: 0.1341, Validation Loss: 0.1263, Training RMSE: 0.3662, Validation RMSE: 0.3553\n",
      "Epoch [110001/10000000], Training Loss: 0.1343, Validation Loss: 0.1262, Training RMSE: 0.3665, Validation RMSE: 0.3552\n",
      "Epoch [111001/10000000], Training Loss: 0.1343, Validation Loss: 0.1261, Training RMSE: 0.3664, Validation RMSE: 0.3551\n",
      "Epoch [112001/10000000], Training Loss: 0.1345, Validation Loss: 0.1261, Training RMSE: 0.3668, Validation RMSE: 0.3550\n",
      "Epoch [113001/10000000], Training Loss: 0.1339, Validation Loss: 0.1260, Training RMSE: 0.3660, Validation RMSE: 0.3549\n",
      "Epoch [114001/10000000], Training Loss: 0.1340, Validation Loss: 0.1259, Training RMSE: 0.3660, Validation RMSE: 0.3548\n",
      "Epoch [115001/10000000], Training Loss: 0.1339, Validation Loss: 0.1258, Training RMSE: 0.3659, Validation RMSE: 0.3547\n",
      "Epoch [116001/10000000], Training Loss: 0.1340, Validation Loss: 0.1258, Training RMSE: 0.3661, Validation RMSE: 0.3546\n",
      "Epoch [117001/10000000], Training Loss: 0.1335, Validation Loss: 0.1257, Training RMSE: 0.3654, Validation RMSE: 0.3546\n",
      "Epoch [118001/10000000], Training Loss: 0.1335, Validation Loss: 0.1256, Training RMSE: 0.3654, Validation RMSE: 0.3545\n",
      "Epoch [119001/10000000], Training Loss: 0.1335, Validation Loss: 0.1256, Training RMSE: 0.3654, Validation RMSE: 0.3544\n",
      "Epoch [120001/10000000], Training Loss: 0.1334, Validation Loss: 0.1255, Training RMSE: 0.3652, Validation RMSE: 0.3543\n",
      "Epoch [121001/10000000], Training Loss: 0.1337, Validation Loss: 0.1254, Training RMSE: 0.3657, Validation RMSE: 0.3542\n",
      "Epoch [122001/10000000], Training Loss: 0.1335, Validation Loss: 0.1254, Training RMSE: 0.3654, Validation RMSE: 0.3541\n",
      "Epoch [123001/10000000], Training Loss: 0.1332, Validation Loss: 0.1253, Training RMSE: 0.3649, Validation RMSE: 0.3540\n",
      "Epoch [124001/10000000], Training Loss: 0.1333, Validation Loss: 0.1252, Training RMSE: 0.3651, Validation RMSE: 0.3539\n",
      "Epoch [125001/10000000], Training Loss: 0.1332, Validation Loss: 0.1252, Training RMSE: 0.3650, Validation RMSE: 0.3538\n",
      "Epoch [126001/10000000], Training Loss: 0.1335, Validation Loss: 0.1251, Training RMSE: 0.3654, Validation RMSE: 0.3537\n",
      "Epoch [127001/10000000], Training Loss: 0.1332, Validation Loss: 0.1250, Training RMSE: 0.3650, Validation RMSE: 0.3536\n",
      "Epoch [128001/10000000], Training Loss: 0.1334, Validation Loss: 0.1249, Training RMSE: 0.3652, Validation RMSE: 0.3535\n",
      "Epoch [129001/10000000], Training Loss: 0.1330, Validation Loss: 0.1249, Training RMSE: 0.3647, Validation RMSE: 0.3534\n",
      "Epoch [130001/10000000], Training Loss: 0.1327, Validation Loss: 0.1248, Training RMSE: 0.3642, Validation RMSE: 0.3533\n",
      "Epoch [131001/10000000], Training Loss: 0.1330, Validation Loss: 0.1247, Training RMSE: 0.3646, Validation RMSE: 0.3532\n",
      "Epoch [132001/10000000], Training Loss: 0.1324, Validation Loss: 0.1247, Training RMSE: 0.3639, Validation RMSE: 0.3531\n",
      "Epoch [133001/10000000], Training Loss: 0.1328, Validation Loss: 0.1246, Training RMSE: 0.3645, Validation RMSE: 0.3530\n",
      "Epoch [134001/10000000], Training Loss: 0.1324, Validation Loss: 0.1245, Training RMSE: 0.3639, Validation RMSE: 0.3529\n",
      "Epoch [135001/10000000], Training Loss: 0.1325, Validation Loss: 0.1245, Training RMSE: 0.3641, Validation RMSE: 0.3528\n",
      "Epoch [136001/10000000], Training Loss: 0.1323, Validation Loss: 0.1244, Training RMSE: 0.3637, Validation RMSE: 0.3527\n",
      "Epoch [137001/10000000], Training Loss: 0.1322, Validation Loss: 0.1243, Training RMSE: 0.3636, Validation RMSE: 0.3526\n",
      "Epoch [138001/10000000], Training Loss: 0.1325, Validation Loss: 0.1243, Training RMSE: 0.3640, Validation RMSE: 0.3525\n",
      "Epoch [139001/10000000], Training Loss: 0.1325, Validation Loss: 0.1242, Training RMSE: 0.3641, Validation RMSE: 0.3524\n",
      "Epoch [140001/10000000], Training Loss: 0.1322, Validation Loss: 0.1241, Training RMSE: 0.3636, Validation RMSE: 0.3523\n",
      "Epoch [141001/10000000], Training Loss: 0.1323, Validation Loss: 0.1241, Training RMSE: 0.3637, Validation RMSE: 0.3522\n",
      "Epoch [142001/10000000], Training Loss: 0.1323, Validation Loss: 0.1240, Training RMSE: 0.3637, Validation RMSE: 0.3521\n",
      "Epoch [143001/10000000], Training Loss: 0.1318, Validation Loss: 0.1239, Training RMSE: 0.3630, Validation RMSE: 0.3520\n",
      "Epoch [144001/10000000], Training Loss: 0.1324, Validation Loss: 0.1238, Training RMSE: 0.3639, Validation RMSE: 0.3519\n",
      "Epoch [145001/10000000], Training Loss: 0.1321, Validation Loss: 0.1238, Training RMSE: 0.3634, Validation RMSE: 0.3518\n",
      "Epoch [146001/10000000], Training Loss: 0.1321, Validation Loss: 0.1237, Training RMSE: 0.3634, Validation RMSE: 0.3517\n",
      "Epoch [147001/10000000], Training Loss: 0.1318, Validation Loss: 0.1236, Training RMSE: 0.3631, Validation RMSE: 0.3516\n",
      "Epoch [148001/10000000], Training Loss: 0.1320, Validation Loss: 0.1236, Training RMSE: 0.3634, Validation RMSE: 0.3515\n",
      "Epoch [149001/10000000], Training Loss: 0.1315, Validation Loss: 0.1235, Training RMSE: 0.3626, Validation RMSE: 0.3514\n",
      "Epoch [150001/10000000], Training Loss: 0.1320, Validation Loss: 0.1234, Training RMSE: 0.3633, Validation RMSE: 0.3513\n",
      "Epoch [151001/10000000], Training Loss: 0.1315, Validation Loss: 0.1234, Training RMSE: 0.3626, Validation RMSE: 0.3512\n",
      "Epoch [152001/10000000], Training Loss: 0.1319, Validation Loss: 0.1233, Training RMSE: 0.3631, Validation RMSE: 0.3511\n",
      "Epoch [153001/10000000], Training Loss: 0.1315, Validation Loss: 0.1232, Training RMSE: 0.3627, Validation RMSE: 0.3510\n",
      "Epoch [154001/10000000], Training Loss: 0.1309, Validation Loss: 0.1232, Training RMSE: 0.3618, Validation RMSE: 0.3509\n",
      "Epoch [155001/10000000], Training Loss: 0.1311, Validation Loss: 0.1231, Training RMSE: 0.3620, Validation RMSE: 0.3508\n",
      "Epoch [156001/10000000], Training Loss: 0.1313, Validation Loss: 0.1230, Training RMSE: 0.3623, Validation RMSE: 0.3507\n",
      "Epoch [157001/10000000], Training Loss: 0.1311, Validation Loss: 0.1229, Training RMSE: 0.3621, Validation RMSE: 0.3506\n",
      "Epoch [158001/10000000], Training Loss: 0.1310, Validation Loss: 0.1229, Training RMSE: 0.3619, Validation RMSE: 0.3505\n",
      "Epoch [159001/10000000], Training Loss: 0.1310, Validation Loss: 0.1228, Training RMSE: 0.3619, Validation RMSE: 0.3504\n",
      "Epoch [160001/10000000], Training Loss: 0.1311, Validation Loss: 0.1227, Training RMSE: 0.3620, Validation RMSE: 0.3503\n",
      "Epoch [161001/10000000], Training Loss: 0.1310, Validation Loss: 0.1227, Training RMSE: 0.3619, Validation RMSE: 0.3502\n",
      "Epoch [162001/10000000], Training Loss: 0.1308, Validation Loss: 0.1226, Training RMSE: 0.3617, Validation RMSE: 0.3501\n",
      "Epoch [163001/10000000], Training Loss: 0.1308, Validation Loss: 0.1225, Training RMSE: 0.3616, Validation RMSE: 0.3500\n",
      "Epoch [164001/10000000], Training Loss: 0.1310, Validation Loss: 0.1225, Training RMSE: 0.3620, Validation RMSE: 0.3499\n",
      "Epoch [165001/10000000], Training Loss: 0.1306, Validation Loss: 0.1224, Training RMSE: 0.3613, Validation RMSE: 0.3498\n",
      "Epoch [166001/10000000], Training Loss: 0.1304, Validation Loss: 0.1223, Training RMSE: 0.3612, Validation RMSE: 0.3497\n",
      "Epoch [167001/10000000], Training Loss: 0.1304, Validation Loss: 0.1222, Training RMSE: 0.3612, Validation RMSE: 0.3496\n",
      "Epoch [168001/10000000], Training Loss: 0.1306, Validation Loss: 0.1222, Training RMSE: 0.3613, Validation RMSE: 0.3495\n",
      "Epoch [169001/10000000], Training Loss: 0.1306, Validation Loss: 0.1221, Training RMSE: 0.3614, Validation RMSE: 0.3494\n",
      "Epoch [170001/10000000], Training Loss: 0.1303, Validation Loss: 0.1220, Training RMSE: 0.3610, Validation RMSE: 0.3493\n",
      "Epoch [171001/10000000], Training Loss: 0.1304, Validation Loss: 0.1220, Training RMSE: 0.3611, Validation RMSE: 0.3492\n",
      "Epoch [172001/10000000], Training Loss: 0.1301, Validation Loss: 0.1219, Training RMSE: 0.3607, Validation RMSE: 0.3491\n",
      "Epoch [173001/10000000], Training Loss: 0.1298, Validation Loss: 0.1218, Training RMSE: 0.3603, Validation RMSE: 0.3490\n",
      "Epoch [174001/10000000], Training Loss: 0.1301, Validation Loss: 0.1217, Training RMSE: 0.3607, Validation RMSE: 0.3489\n",
      "Epoch [175001/10000000], Training Loss: 0.1299, Validation Loss: 0.1217, Training RMSE: 0.3605, Validation RMSE: 0.3488\n",
      "Epoch [176001/10000000], Training Loss: 0.1299, Validation Loss: 0.1216, Training RMSE: 0.3605, Validation RMSE: 0.3487\n",
      "Epoch [177001/10000000], Training Loss: 0.1297, Validation Loss: 0.1215, Training RMSE: 0.3601, Validation RMSE: 0.3486\n",
      "Epoch [178001/10000000], Training Loss: 0.1295, Validation Loss: 0.1215, Training RMSE: 0.3599, Validation RMSE: 0.3485\n",
      "Epoch [179001/10000000], Training Loss: 0.1297, Validation Loss: 0.1214, Training RMSE: 0.3601, Validation RMSE: 0.3484\n",
      "Epoch [180001/10000000], Training Loss: 0.1298, Validation Loss: 0.1213, Training RMSE: 0.3603, Validation RMSE: 0.3483\n",
      "Epoch [181001/10000000], Training Loss: 0.1293, Validation Loss: 0.1212, Training RMSE: 0.3596, Validation RMSE: 0.3482\n",
      "Epoch [182001/10000000], Training Loss: 0.1294, Validation Loss: 0.1212, Training RMSE: 0.3598, Validation RMSE: 0.3481\n",
      "Epoch [183001/10000000], Training Loss: 0.1291, Validation Loss: 0.1211, Training RMSE: 0.3593, Validation RMSE: 0.3480\n",
      "Epoch [184001/10000000], Training Loss: 0.1295, Validation Loss: 0.1210, Training RMSE: 0.3599, Validation RMSE: 0.3479\n",
      "Epoch [185001/10000000], Training Loss: 0.1294, Validation Loss: 0.1210, Training RMSE: 0.3598, Validation RMSE: 0.3478\n",
      "Epoch [186001/10000000], Training Loss: 0.1292, Validation Loss: 0.1209, Training RMSE: 0.3594, Validation RMSE: 0.3477\n",
      "Epoch [187001/10000000], Training Loss: 0.1290, Validation Loss: 0.1208, Training RMSE: 0.3592, Validation RMSE: 0.3476\n",
      "Epoch [188001/10000000], Training Loss: 0.1290, Validation Loss: 0.1207, Training RMSE: 0.3592, Validation RMSE: 0.3475\n",
      "Epoch [189001/10000000], Training Loss: 0.1292, Validation Loss: 0.1207, Training RMSE: 0.3594, Validation RMSE: 0.3474\n",
      "Epoch [190001/10000000], Training Loss: 0.1287, Validation Loss: 0.1206, Training RMSE: 0.3588, Validation RMSE: 0.3473\n",
      "Epoch [191001/10000000], Training Loss: 0.1290, Validation Loss: 0.1205, Training RMSE: 0.3592, Validation RMSE: 0.3472\n",
      "Epoch [192001/10000000], Training Loss: 0.1290, Validation Loss: 0.1204, Training RMSE: 0.3592, Validation RMSE: 0.3470\n",
      "Epoch [193001/10000000], Training Loss: 0.1290, Validation Loss: 0.1204, Training RMSE: 0.3591, Validation RMSE: 0.3469\n",
      "Epoch [194001/10000000], Training Loss: 0.1288, Validation Loss: 0.1203, Training RMSE: 0.3589, Validation RMSE: 0.3468\n",
      "Epoch [195001/10000000], Training Loss: 0.1287, Validation Loss: 0.1202, Training RMSE: 0.3587, Validation RMSE: 0.3467\n",
      "Epoch [196001/10000000], Training Loss: 0.1286, Validation Loss: 0.1201, Training RMSE: 0.3587, Validation RMSE: 0.3466\n",
      "Epoch [197001/10000000], Training Loss: 0.1288, Validation Loss: 0.1201, Training RMSE: 0.3589, Validation RMSE: 0.3465\n",
      "Epoch [198001/10000000], Training Loss: 0.1283, Validation Loss: 0.1200, Training RMSE: 0.3583, Validation RMSE: 0.3464\n",
      "Epoch [199001/10000000], Training Loss: 0.1288, Validation Loss: 0.1199, Training RMSE: 0.3589, Validation RMSE: 0.3463\n",
      "Epoch [200001/10000000], Training Loss: 0.1284, Validation Loss: 0.1198, Training RMSE: 0.3584, Validation RMSE: 0.3462\n",
      "Epoch [201001/10000000], Training Loss: 0.1279, Validation Loss: 0.1198, Training RMSE: 0.3576, Validation RMSE: 0.3461\n",
      "Epoch [202001/10000000], Training Loss: 0.1281, Validation Loss: 0.1197, Training RMSE: 0.3579, Validation RMSE: 0.3460\n",
      "Epoch [203001/10000000], Training Loss: 0.1282, Validation Loss: 0.1196, Training RMSE: 0.3580, Validation RMSE: 0.3459\n",
      "Epoch [204001/10000000], Training Loss: 0.1279, Validation Loss: 0.1196, Training RMSE: 0.3577, Validation RMSE: 0.3458\n",
      "Epoch [205001/10000000], Training Loss: 0.1278, Validation Loss: 0.1195, Training RMSE: 0.3576, Validation RMSE: 0.3457\n",
      "Epoch [206001/10000000], Training Loss: 0.1278, Validation Loss: 0.1194, Training RMSE: 0.3575, Validation RMSE: 0.3455\n",
      "Epoch [207001/10000000], Training Loss: 0.1280, Validation Loss: 0.1193, Training RMSE: 0.3578, Validation RMSE: 0.3454\n",
      "Epoch [208001/10000000], Training Loss: 0.1275, Validation Loss: 0.1193, Training RMSE: 0.3571, Validation RMSE: 0.3453\n",
      "Epoch [209001/10000000], Training Loss: 0.1276, Validation Loss: 0.1192, Training RMSE: 0.3573, Validation RMSE: 0.3452\n",
      "Epoch [210001/10000000], Training Loss: 0.1278, Validation Loss: 0.1191, Training RMSE: 0.3574, Validation RMSE: 0.3451\n",
      "Epoch [211001/10000000], Training Loss: 0.1275, Validation Loss: 0.1190, Training RMSE: 0.3571, Validation RMSE: 0.3450\n",
      "Epoch [212001/10000000], Training Loss: 0.1274, Validation Loss: 0.1189, Training RMSE: 0.3569, Validation RMSE: 0.3449\n",
      "Epoch [213001/10000000], Training Loss: 0.1274, Validation Loss: 0.1189, Training RMSE: 0.3569, Validation RMSE: 0.3448\n",
      "Epoch [214001/10000000], Training Loss: 0.1270, Validation Loss: 0.1188, Training RMSE: 0.3563, Validation RMSE: 0.3447\n",
      "Epoch [215001/10000000], Training Loss: 0.1271, Validation Loss: 0.1187, Training RMSE: 0.3566, Validation RMSE: 0.3446\n",
      "Epoch [216001/10000000], Training Loss: 0.1272, Validation Loss: 0.1186, Training RMSE: 0.3566, Validation RMSE: 0.3444\n",
      "Epoch [217001/10000000], Training Loss: 0.1273, Validation Loss: 0.1186, Training RMSE: 0.3568, Validation RMSE: 0.3443\n",
      "Epoch [218001/10000000], Training Loss: 0.1268, Validation Loss: 0.1185, Training RMSE: 0.3560, Validation RMSE: 0.3442\n",
      "Epoch [219001/10000000], Training Loss: 0.1267, Validation Loss: 0.1184, Training RMSE: 0.3559, Validation RMSE: 0.3441\n",
      "Epoch [220001/10000000], Training Loss: 0.1272, Validation Loss: 0.1183, Training RMSE: 0.3566, Validation RMSE: 0.3440\n",
      "Epoch [221001/10000000], Training Loss: 0.1270, Validation Loss: 0.1183, Training RMSE: 0.3564, Validation RMSE: 0.3439\n",
      "Epoch [222001/10000000], Training Loss: 0.1270, Validation Loss: 0.1182, Training RMSE: 0.3564, Validation RMSE: 0.3438\n",
      "Epoch [223001/10000000], Training Loss: 0.1268, Validation Loss: 0.1181, Training RMSE: 0.3560, Validation RMSE: 0.3437\n",
      "Epoch [224001/10000000], Training Loss: 0.1264, Validation Loss: 0.1180, Training RMSE: 0.3555, Validation RMSE: 0.3436\n",
      "Epoch [225001/10000000], Training Loss: 0.1265, Validation Loss: 0.1180, Training RMSE: 0.3557, Validation RMSE: 0.3434\n",
      "Epoch [226001/10000000], Training Loss: 0.1267, Validation Loss: 0.1179, Training RMSE: 0.3559, Validation RMSE: 0.3433\n",
      "Epoch [227001/10000000], Training Loss: 0.1260, Validation Loss: 0.1178, Training RMSE: 0.3550, Validation RMSE: 0.3432\n",
      "Epoch [228001/10000000], Training Loss: 0.1264, Validation Loss: 0.1177, Training RMSE: 0.3555, Validation RMSE: 0.3431\n",
      "Epoch [229001/10000000], Training Loss: 0.1260, Validation Loss: 0.1176, Training RMSE: 0.3550, Validation RMSE: 0.3430\n",
      "Epoch [230001/10000000], Training Loss: 0.1264, Validation Loss: 0.1176, Training RMSE: 0.3555, Validation RMSE: 0.3429\n",
      "Epoch [231001/10000000], Training Loss: 0.1261, Validation Loss: 0.1175, Training RMSE: 0.3550, Validation RMSE: 0.3428\n",
      "Epoch [232001/10000000], Training Loss: 0.1255, Validation Loss: 0.1174, Training RMSE: 0.3543, Validation RMSE: 0.3427\n",
      "Epoch [233001/10000000], Training Loss: 0.1261, Validation Loss: 0.1173, Training RMSE: 0.3551, Validation RMSE: 0.3425\n",
      "Epoch [234001/10000000], Training Loss: 0.1263, Validation Loss: 0.1173, Training RMSE: 0.3553, Validation RMSE: 0.3424\n",
      "Epoch [235001/10000000], Training Loss: 0.1256, Validation Loss: 0.1172, Training RMSE: 0.3545, Validation RMSE: 0.3423\n",
      "Epoch [236001/10000000], Training Loss: 0.1256, Validation Loss: 0.1171, Training RMSE: 0.3544, Validation RMSE: 0.3422\n",
      "Epoch [237001/10000000], Training Loss: 0.1255, Validation Loss: 0.1170, Training RMSE: 0.3542, Validation RMSE: 0.3421\n",
      "Epoch [238001/10000000], Training Loss: 0.1255, Validation Loss: 0.1169, Training RMSE: 0.3542, Validation RMSE: 0.3420\n",
      "Epoch [239001/10000000], Training Loss: 0.1254, Validation Loss: 0.1169, Training RMSE: 0.3541, Validation RMSE: 0.3419\n",
      "Epoch [240001/10000000], Training Loss: 0.1256, Validation Loss: 0.1168, Training RMSE: 0.3544, Validation RMSE: 0.3417\n",
      "Epoch [241001/10000000], Training Loss: 0.1257, Validation Loss: 0.1167, Training RMSE: 0.3545, Validation RMSE: 0.3416\n",
      "Epoch [242001/10000000], Training Loss: 0.1259, Validation Loss: 0.1166, Training RMSE: 0.3548, Validation RMSE: 0.3415\n",
      "Epoch [243001/10000000], Training Loss: 0.1252, Validation Loss: 0.1166, Training RMSE: 0.3538, Validation RMSE: 0.3414\n",
      "Epoch [244001/10000000], Training Loss: 0.1253, Validation Loss: 0.1165, Training RMSE: 0.3540, Validation RMSE: 0.3413\n",
      "Epoch [245001/10000000], Training Loss: 0.1252, Validation Loss: 0.1164, Training RMSE: 0.3538, Validation RMSE: 0.3412\n",
      "Epoch [246001/10000000], Training Loss: 0.1252, Validation Loss: 0.1163, Training RMSE: 0.3539, Validation RMSE: 0.3411\n",
      "Epoch [247001/10000000], Training Loss: 0.1249, Validation Loss: 0.1162, Training RMSE: 0.3535, Validation RMSE: 0.3409\n",
      "Epoch [248001/10000000], Training Loss: 0.1249, Validation Loss: 0.1162, Training RMSE: 0.3535, Validation RMSE: 0.3408\n",
      "Epoch [249001/10000000], Training Loss: 0.1251, Validation Loss: 0.1161, Training RMSE: 0.3537, Validation RMSE: 0.3407\n",
      "Epoch [250001/10000000], Training Loss: 0.1249, Validation Loss: 0.1160, Training RMSE: 0.3534, Validation RMSE: 0.3406\n",
      "Epoch [251001/10000000], Training Loss: 0.1250, Validation Loss: 0.1159, Training RMSE: 0.3535, Validation RMSE: 0.3405\n",
      "Epoch [252001/10000000], Training Loss: 0.1247, Validation Loss: 0.1158, Training RMSE: 0.3532, Validation RMSE: 0.3404\n",
      "Epoch [253001/10000000], Training Loss: 0.1246, Validation Loss: 0.1158, Training RMSE: 0.3531, Validation RMSE: 0.3402\n",
      "Epoch [254001/10000000], Training Loss: 0.1244, Validation Loss: 0.1157, Training RMSE: 0.3527, Validation RMSE: 0.3401\n",
      "Epoch [255001/10000000], Training Loss: 0.1246, Validation Loss: 0.1156, Training RMSE: 0.3530, Validation RMSE: 0.3400\n",
      "Epoch [256001/10000000], Training Loss: 0.1240, Validation Loss: 0.1155, Training RMSE: 0.3522, Validation RMSE: 0.3399\n",
      "Epoch [257001/10000000], Training Loss: 0.1241, Validation Loss: 0.1154, Training RMSE: 0.3523, Validation RMSE: 0.3398\n",
      "Epoch [258001/10000000], Training Loss: 0.1241, Validation Loss: 0.1154, Training RMSE: 0.3523, Validation RMSE: 0.3396\n",
      "Epoch [259001/10000000], Training Loss: 0.1248, Validation Loss: 0.1153, Training RMSE: 0.3532, Validation RMSE: 0.3395\n",
      "Epoch [260001/10000000], Training Loss: 0.1239, Validation Loss: 0.1152, Training RMSE: 0.3520, Validation RMSE: 0.3394\n",
      "Epoch [261001/10000000], Training Loss: 0.1244, Validation Loss: 0.1151, Training RMSE: 0.3526, Validation RMSE: 0.3393\n",
      "Epoch [262001/10000000], Training Loss: 0.1237, Validation Loss: 0.1150, Training RMSE: 0.3517, Validation RMSE: 0.3392\n",
      "Epoch [263001/10000000], Training Loss: 0.1236, Validation Loss: 0.1150, Training RMSE: 0.3516, Validation RMSE: 0.3391\n",
      "Epoch [264001/10000000], Training Loss: 0.1238, Validation Loss: 0.1149, Training RMSE: 0.3518, Validation RMSE: 0.3389\n",
      "Epoch [265001/10000000], Training Loss: 0.1237, Validation Loss: 0.1148, Training RMSE: 0.3517, Validation RMSE: 0.3388\n",
      "Epoch [266001/10000000], Training Loss: 0.1237, Validation Loss: 0.1147, Training RMSE: 0.3517, Validation RMSE: 0.3387\n",
      "Epoch [267001/10000000], Training Loss: 0.1236, Validation Loss: 0.1146, Training RMSE: 0.3515, Validation RMSE: 0.3386\n",
      "Epoch [268001/10000000], Training Loss: 0.1228, Validation Loss: 0.1145, Training RMSE: 0.3504, Validation RMSE: 0.3385\n",
      "Epoch [269001/10000000], Training Loss: 0.1236, Validation Loss: 0.1145, Training RMSE: 0.3516, Validation RMSE: 0.3383\n",
      "Epoch [270001/10000000], Training Loss: 0.1237, Validation Loss: 0.1144, Training RMSE: 0.3517, Validation RMSE: 0.3382\n",
      "Epoch [271001/10000000], Training Loss: 0.1232, Validation Loss: 0.1143, Training RMSE: 0.3510, Validation RMSE: 0.3381\n",
      "Epoch [272001/10000000], Training Loss: 0.1233, Validation Loss: 0.1142, Training RMSE: 0.3511, Validation RMSE: 0.3380\n",
      "Epoch [273001/10000000], Training Loss: 0.1230, Validation Loss: 0.1141, Training RMSE: 0.3508, Validation RMSE: 0.3378\n",
      "Epoch [274001/10000000], Training Loss: 0.1231, Validation Loss: 0.1141, Training RMSE: 0.3508, Validation RMSE: 0.3377\n",
      "Epoch [275001/10000000], Training Loss: 0.1228, Validation Loss: 0.1140, Training RMSE: 0.3504, Validation RMSE: 0.3376\n",
      "Epoch [276001/10000000], Training Loss: 0.1227, Validation Loss: 0.1139, Training RMSE: 0.3503, Validation RMSE: 0.3375\n",
      "Epoch [277001/10000000], Training Loss: 0.1229, Validation Loss: 0.1138, Training RMSE: 0.3506, Validation RMSE: 0.3374\n",
      "Epoch [278001/10000000], Training Loss: 0.1229, Validation Loss: 0.1137, Training RMSE: 0.3505, Validation RMSE: 0.3372\n",
      "Epoch [279001/10000000], Training Loss: 0.1228, Validation Loss: 0.1136, Training RMSE: 0.3504, Validation RMSE: 0.3371\n",
      "Epoch [280001/10000000], Training Loss: 0.1227, Validation Loss: 0.1136, Training RMSE: 0.3502, Validation RMSE: 0.3370\n",
      "Epoch [281001/10000000], Training Loss: 0.1223, Validation Loss: 0.1135, Training RMSE: 0.3498, Validation RMSE: 0.3369\n",
      "Epoch [282001/10000000], Training Loss: 0.1224, Validation Loss: 0.1134, Training RMSE: 0.3498, Validation RMSE: 0.3367\n",
      "Epoch [283001/10000000], Training Loss: 0.1222, Validation Loss: 0.1133, Training RMSE: 0.3496, Validation RMSE: 0.3366\n",
      "Epoch [284001/10000000], Training Loss: 0.1221, Validation Loss: 0.1132, Training RMSE: 0.3494, Validation RMSE: 0.3365\n",
      "Epoch [285001/10000000], Training Loss: 0.1220, Validation Loss: 0.1131, Training RMSE: 0.3493, Validation RMSE: 0.3364\n",
      "Epoch [286001/10000000], Training Loss: 0.1220, Validation Loss: 0.1131, Training RMSE: 0.3492, Validation RMSE: 0.3363\n",
      "Epoch [287001/10000000], Training Loss: 0.1221, Validation Loss: 0.1130, Training RMSE: 0.3495, Validation RMSE: 0.3361\n",
      "Epoch [288001/10000000], Training Loss: 0.1222, Validation Loss: 0.1129, Training RMSE: 0.3496, Validation RMSE: 0.3360\n",
      "Epoch [289001/10000000], Training Loss: 0.1217, Validation Loss: 0.1128, Training RMSE: 0.3489, Validation RMSE: 0.3359\n",
      "Epoch [290001/10000000], Training Loss: 0.1217, Validation Loss: 0.1127, Training RMSE: 0.3489, Validation RMSE: 0.3358\n",
      "Epoch [291001/10000000], Training Loss: 0.1217, Validation Loss: 0.1126, Training RMSE: 0.3489, Validation RMSE: 0.3356\n",
      "Epoch [292001/10000000], Training Loss: 0.1217, Validation Loss: 0.1126, Training RMSE: 0.3489, Validation RMSE: 0.3355\n",
      "Epoch [293001/10000000], Training Loss: 0.1216, Validation Loss: 0.1125, Training RMSE: 0.3487, Validation RMSE: 0.3354\n",
      "Epoch [294001/10000000], Training Loss: 0.1210, Validation Loss: 0.1124, Training RMSE: 0.3478, Validation RMSE: 0.3353\n",
      "Epoch [295001/10000000], Training Loss: 0.1213, Validation Loss: 0.1123, Training RMSE: 0.3483, Validation RMSE: 0.3351\n",
      "Epoch [296001/10000000], Training Loss: 0.1214, Validation Loss: 0.1122, Training RMSE: 0.3484, Validation RMSE: 0.3350\n",
      "Epoch [297001/10000000], Training Loss: 0.1218, Validation Loss: 0.1121, Training RMSE: 0.3490, Validation RMSE: 0.3349\n",
      "Epoch [298001/10000000], Training Loss: 0.1215, Validation Loss: 0.1121, Training RMSE: 0.3486, Validation RMSE: 0.3348\n",
      "Epoch [299001/10000000], Training Loss: 0.1211, Validation Loss: 0.1120, Training RMSE: 0.3480, Validation RMSE: 0.3346\n",
      "Epoch [300001/10000000], Training Loss: 0.1216, Validation Loss: 0.1119, Training RMSE: 0.3487, Validation RMSE: 0.3345\n",
      "Epoch [301001/10000000], Training Loss: 0.1211, Validation Loss: 0.1118, Training RMSE: 0.3481, Validation RMSE: 0.3344\n",
      "Epoch [302001/10000000], Training Loss: 0.1210, Validation Loss: 0.1117, Training RMSE: 0.3478, Validation RMSE: 0.3343\n",
      "Epoch [303001/10000000], Training Loss: 0.1206, Validation Loss: 0.1116, Training RMSE: 0.3473, Validation RMSE: 0.3341\n",
      "Epoch [304001/10000000], Training Loss: 0.1207, Validation Loss: 0.1116, Training RMSE: 0.3474, Validation RMSE: 0.3340\n",
      "Epoch [305001/10000000], Training Loss: 0.1205, Validation Loss: 0.1115, Training RMSE: 0.3472, Validation RMSE: 0.3339\n",
      "Epoch [306001/10000000], Training Loss: 0.1206, Validation Loss: 0.1114, Training RMSE: 0.3472, Validation RMSE: 0.3337\n",
      "Epoch [307001/10000000], Training Loss: 0.1203, Validation Loss: 0.1113, Training RMSE: 0.3468, Validation RMSE: 0.3336\n",
      "Epoch [308001/10000000], Training Loss: 0.1202, Validation Loss: 0.1112, Training RMSE: 0.3466, Validation RMSE: 0.3335\n",
      "Epoch [309001/10000000], Training Loss: 0.1205, Validation Loss: 0.1111, Training RMSE: 0.3471, Validation RMSE: 0.3334\n",
      "Epoch [310001/10000000], Training Loss: 0.1203, Validation Loss: 0.1111, Training RMSE: 0.3468, Validation RMSE: 0.3332\n",
      "Epoch [311001/10000000], Training Loss: 0.1200, Validation Loss: 0.1110, Training RMSE: 0.3463, Validation RMSE: 0.3331\n",
      "Epoch [312001/10000000], Training Loss: 0.1201, Validation Loss: 0.1109, Training RMSE: 0.3466, Validation RMSE: 0.3330\n",
      "Epoch [313001/10000000], Training Loss: 0.1200, Validation Loss: 0.1108, Training RMSE: 0.3463, Validation RMSE: 0.3329\n",
      "Epoch [314001/10000000], Training Loss: 0.1198, Validation Loss: 0.1107, Training RMSE: 0.3462, Validation RMSE: 0.3327\n",
      "Epoch [315001/10000000], Training Loss: 0.1197, Validation Loss: 0.1106, Training RMSE: 0.3460, Validation RMSE: 0.3326\n",
      "Epoch [316001/10000000], Training Loss: 0.1196, Validation Loss: 0.1105, Training RMSE: 0.3458, Validation RMSE: 0.3325\n",
      "Epoch [317001/10000000], Training Loss: 0.1199, Validation Loss: 0.1105, Training RMSE: 0.3463, Validation RMSE: 0.3324\n",
      "Epoch [318001/10000000], Training Loss: 0.1198, Validation Loss: 0.1104, Training RMSE: 0.3461, Validation RMSE: 0.3322\n",
      "Epoch [319001/10000000], Training Loss: 0.1189, Validation Loss: 0.1103, Training RMSE: 0.3449, Validation RMSE: 0.3321\n",
      "Epoch [320001/10000000], Training Loss: 0.1195, Validation Loss: 0.1102, Training RMSE: 0.3457, Validation RMSE: 0.3320\n",
      "Epoch [321001/10000000], Training Loss: 0.1193, Validation Loss: 0.1101, Training RMSE: 0.3454, Validation RMSE: 0.3319\n",
      "Epoch [322001/10000000], Training Loss: 0.1197, Validation Loss: 0.1100, Training RMSE: 0.3459, Validation RMSE: 0.3317\n",
      "Epoch [323001/10000000], Training Loss: 0.1190, Validation Loss: 0.1100, Training RMSE: 0.3449, Validation RMSE: 0.3316\n",
      "Epoch [324001/10000000], Training Loss: 0.1189, Validation Loss: 0.1099, Training RMSE: 0.3448, Validation RMSE: 0.3315\n",
      "Epoch [325001/10000000], Training Loss: 0.1189, Validation Loss: 0.1098, Training RMSE: 0.3448, Validation RMSE: 0.3314\n",
      "Epoch [326001/10000000], Training Loss: 0.1187, Validation Loss: 0.1097, Training RMSE: 0.3445, Validation RMSE: 0.3312\n",
      "Epoch [327001/10000000], Training Loss: 0.1189, Validation Loss: 0.1096, Training RMSE: 0.3448, Validation RMSE: 0.3311\n",
      "Epoch [328001/10000000], Training Loss: 0.1187, Validation Loss: 0.1095, Training RMSE: 0.3445, Validation RMSE: 0.3310\n",
      "Epoch [329001/10000000], Training Loss: 0.1180, Validation Loss: 0.1095, Training RMSE: 0.3436, Validation RMSE: 0.3309\n",
      "Epoch [330001/10000000], Training Loss: 0.1188, Validation Loss: 0.1094, Training RMSE: 0.3446, Validation RMSE: 0.3307\n",
      "Epoch [331001/10000000], Training Loss: 0.1184, Validation Loss: 0.1093, Training RMSE: 0.3441, Validation RMSE: 0.3306\n",
      "Epoch [332001/10000000], Training Loss: 0.1190, Validation Loss: 0.1092, Training RMSE: 0.3450, Validation RMSE: 0.3305\n",
      "Epoch [333001/10000000], Training Loss: 0.1186, Validation Loss: 0.1091, Training RMSE: 0.3444, Validation RMSE: 0.3304\n",
      "Epoch [334001/10000000], Training Loss: 0.1186, Validation Loss: 0.1091, Training RMSE: 0.3444, Validation RMSE: 0.3302\n",
      "Epoch [335001/10000000], Training Loss: 0.1184, Validation Loss: 0.1090, Training RMSE: 0.3441, Validation RMSE: 0.3301\n",
      "Epoch [336001/10000000], Training Loss: 0.1177, Validation Loss: 0.1089, Training RMSE: 0.3431, Validation RMSE: 0.3300\n",
      "Epoch [337001/10000000], Training Loss: 0.1185, Validation Loss: 0.1088, Training RMSE: 0.3443, Validation RMSE: 0.3299\n",
      "Epoch [338001/10000000], Training Loss: 0.1179, Validation Loss: 0.1087, Training RMSE: 0.3433, Validation RMSE: 0.3297\n",
      "Epoch [339001/10000000], Training Loss: 0.1179, Validation Loss: 0.1086, Training RMSE: 0.3434, Validation RMSE: 0.3296\n",
      "Epoch [340001/10000000], Training Loss: 0.1179, Validation Loss: 0.1086, Training RMSE: 0.3433, Validation RMSE: 0.3295\n",
      "Epoch [341001/10000000], Training Loss: 0.1178, Validation Loss: 0.1085, Training RMSE: 0.3433, Validation RMSE: 0.3293\n",
      "Epoch [342001/10000000], Training Loss: 0.1176, Validation Loss: 0.1084, Training RMSE: 0.3429, Validation RMSE: 0.3292\n",
      "Epoch [343001/10000000], Training Loss: 0.1174, Validation Loss: 0.1083, Training RMSE: 0.3427, Validation RMSE: 0.3291\n",
      "Epoch [344001/10000000], Training Loss: 0.1178, Validation Loss: 0.1082, Training RMSE: 0.3433, Validation RMSE: 0.3290\n",
      "Epoch [345001/10000000], Training Loss: 0.1172, Validation Loss: 0.1081, Training RMSE: 0.3424, Validation RMSE: 0.3288\n",
      "Epoch [346001/10000000], Training Loss: 0.1172, Validation Loss: 0.1081, Training RMSE: 0.3424, Validation RMSE: 0.3287\n",
      "Epoch [347001/10000000], Training Loss: 0.1171, Validation Loss: 0.1080, Training RMSE: 0.3422, Validation RMSE: 0.3286\n",
      "Epoch [348001/10000000], Training Loss: 0.1173, Validation Loss: 0.1079, Training RMSE: 0.3425, Validation RMSE: 0.3285\n",
      "Epoch [349001/10000000], Training Loss: 0.1170, Validation Loss: 0.1078, Training RMSE: 0.3420, Validation RMSE: 0.3283\n",
      "Epoch [350001/10000000], Training Loss: 0.1173, Validation Loss: 0.1077, Training RMSE: 0.3425, Validation RMSE: 0.3282\n",
      "Epoch [351001/10000000], Training Loss: 0.1164, Validation Loss: 0.1076, Training RMSE: 0.3412, Validation RMSE: 0.3281\n",
      "Epoch [352001/10000000], Training Loss: 0.1169, Validation Loss: 0.1076, Training RMSE: 0.3419, Validation RMSE: 0.3280\n",
      "Epoch [353001/10000000], Training Loss: 0.1165, Validation Loss: 0.1075, Training RMSE: 0.3413, Validation RMSE: 0.3278\n",
      "Epoch [354001/10000000], Training Loss: 0.1167, Validation Loss: 0.1074, Training RMSE: 0.3417, Validation RMSE: 0.3277\n",
      "Epoch [355001/10000000], Training Loss: 0.1166, Validation Loss: 0.1073, Training RMSE: 0.3415, Validation RMSE: 0.3276\n",
      "Epoch [356001/10000000], Training Loss: 0.1162, Validation Loss: 0.1072, Training RMSE: 0.3409, Validation RMSE: 0.3275\n",
      "Epoch [357001/10000000], Training Loss: 0.1163, Validation Loss: 0.1071, Training RMSE: 0.3410, Validation RMSE: 0.3273\n",
      "Epoch [358001/10000000], Training Loss: 0.1164, Validation Loss: 0.1071, Training RMSE: 0.3412, Validation RMSE: 0.3272\n",
      "Epoch [359001/10000000], Training Loss: 0.1158, Validation Loss: 0.1070, Training RMSE: 0.3404, Validation RMSE: 0.3271\n",
      "Epoch [360001/10000000], Training Loss: 0.1165, Validation Loss: 0.1069, Training RMSE: 0.3413, Validation RMSE: 0.3270\n",
      "Epoch [361001/10000000], Training Loss: 0.1165, Validation Loss: 0.1068, Training RMSE: 0.3413, Validation RMSE: 0.3268\n",
      "Epoch [362001/10000000], Training Loss: 0.1161, Validation Loss: 0.1067, Training RMSE: 0.3407, Validation RMSE: 0.3267\n",
      "Epoch [363001/10000000], Training Loss: 0.1159, Validation Loss: 0.1067, Training RMSE: 0.3405, Validation RMSE: 0.3266\n",
      "Epoch [364001/10000000], Training Loss: 0.1151, Validation Loss: 0.1066, Training RMSE: 0.3393, Validation RMSE: 0.3264\n",
      "Epoch [365001/10000000], Training Loss: 0.1160, Validation Loss: 0.1065, Training RMSE: 0.3405, Validation RMSE: 0.3263\n",
      "Epoch [366001/10000000], Training Loss: 0.1152, Validation Loss: 0.1064, Training RMSE: 0.3394, Validation RMSE: 0.3262\n",
      "Epoch [367001/10000000], Training Loss: 0.1157, Validation Loss: 0.1063, Training RMSE: 0.3401, Validation RMSE: 0.3261\n",
      "Epoch [368001/10000000], Training Loss: 0.1153, Validation Loss: 0.1062, Training RMSE: 0.3395, Validation RMSE: 0.3259\n",
      "Epoch [369001/10000000], Training Loss: 0.1155, Validation Loss: 0.1062, Training RMSE: 0.3398, Validation RMSE: 0.3258\n",
      "Epoch [370001/10000000], Training Loss: 0.1152, Validation Loss: 0.1061, Training RMSE: 0.3394, Validation RMSE: 0.3257\n",
      "Epoch [371001/10000000], Training Loss: 0.1156, Validation Loss: 0.1060, Training RMSE: 0.3400, Validation RMSE: 0.3256\n",
      "Epoch [372001/10000000], Training Loss: 0.1148, Validation Loss: 0.1059, Training RMSE: 0.3389, Validation RMSE: 0.3254\n",
      "Epoch [373001/10000000], Training Loss: 0.1153, Validation Loss: 0.1058, Training RMSE: 0.3396, Validation RMSE: 0.3253\n",
      "Epoch [374001/10000000], Training Loss: 0.1154, Validation Loss: 0.1057, Training RMSE: 0.3397, Validation RMSE: 0.3252\n",
      "Epoch [375001/10000000], Training Loss: 0.1146, Validation Loss: 0.1057, Training RMSE: 0.3385, Validation RMSE: 0.3250\n",
      "Epoch [376001/10000000], Training Loss: 0.1149, Validation Loss: 0.1056, Training RMSE: 0.3389, Validation RMSE: 0.3249\n",
      "Epoch [377001/10000000], Training Loss: 0.1146, Validation Loss: 0.1055, Training RMSE: 0.3385, Validation RMSE: 0.3248\n",
      "Epoch [378001/10000000], Training Loss: 0.1146, Validation Loss: 0.1054, Training RMSE: 0.3386, Validation RMSE: 0.3247\n",
      "Epoch [379001/10000000], Training Loss: 0.1144, Validation Loss: 0.1053, Training RMSE: 0.3382, Validation RMSE: 0.3245\n",
      "Epoch [380001/10000000], Training Loss: 0.1140, Validation Loss: 0.1052, Training RMSE: 0.3377, Validation RMSE: 0.3244\n",
      "Epoch [381001/10000000], Training Loss: 0.1148, Validation Loss: 0.1051, Training RMSE: 0.3388, Validation RMSE: 0.3243\n",
      "Epoch [382001/10000000], Training Loss: 0.1143, Validation Loss: 0.1051, Training RMSE: 0.3381, Validation RMSE: 0.3241\n",
      "Epoch [383001/10000000], Training Loss: 0.1141, Validation Loss: 0.1050, Training RMSE: 0.3377, Validation RMSE: 0.3240\n",
      "Epoch [384001/10000000], Training Loss: 0.1141, Validation Loss: 0.1049, Training RMSE: 0.3378, Validation RMSE: 0.3239\n",
      "Epoch [385001/10000000], Training Loss: 0.1143, Validation Loss: 0.1048, Training RMSE: 0.3380, Validation RMSE: 0.3238\n",
      "Epoch [386001/10000000], Training Loss: 0.1136, Validation Loss: 0.1047, Training RMSE: 0.3371, Validation RMSE: 0.3236\n",
      "Epoch [387001/10000000], Training Loss: 0.1139, Validation Loss: 0.1046, Training RMSE: 0.3375, Validation RMSE: 0.3235\n",
      "Epoch [388001/10000000], Training Loss: 0.1143, Validation Loss: 0.1046, Training RMSE: 0.3380, Validation RMSE: 0.3234\n",
      "Epoch [389001/10000000], Training Loss: 0.1135, Validation Loss: 0.1045, Training RMSE: 0.3369, Validation RMSE: 0.3232\n",
      "Epoch [390001/10000000], Training Loss: 0.1142, Validation Loss: 0.1044, Training RMSE: 0.3380, Validation RMSE: 0.3231\n",
      "Epoch [391001/10000000], Training Loss: 0.1134, Validation Loss: 0.1043, Training RMSE: 0.3367, Validation RMSE: 0.3230\n",
      "Epoch [392001/10000000], Training Loss: 0.1129, Validation Loss: 0.1042, Training RMSE: 0.3361, Validation RMSE: 0.3228\n",
      "Epoch [393001/10000000], Training Loss: 0.1133, Validation Loss: 0.1041, Training RMSE: 0.3366, Validation RMSE: 0.3227\n",
      "Epoch [394001/10000000], Training Loss: 0.1133, Validation Loss: 0.1041, Training RMSE: 0.3366, Validation RMSE: 0.3226\n",
      "Epoch [395001/10000000], Training Loss: 0.1131, Validation Loss: 0.1040, Training RMSE: 0.3362, Validation RMSE: 0.3225\n",
      "Epoch [396001/10000000], Training Loss: 0.1124, Validation Loss: 0.1039, Training RMSE: 0.3352, Validation RMSE: 0.3223\n",
      "Epoch [397001/10000000], Training Loss: 0.1133, Validation Loss: 0.1038, Training RMSE: 0.3366, Validation RMSE: 0.3222\n",
      "Epoch [398001/10000000], Training Loss: 0.1133, Validation Loss: 0.1037, Training RMSE: 0.3366, Validation RMSE: 0.3221\n",
      "Epoch [399001/10000000], Training Loss: 0.1131, Validation Loss: 0.1036, Training RMSE: 0.3363, Validation RMSE: 0.3219\n",
      "Epoch [400001/10000000], Training Loss: 0.1126, Validation Loss: 0.1036, Training RMSE: 0.3356, Validation RMSE: 0.3218\n",
      "Epoch [401001/10000000], Training Loss: 0.1126, Validation Loss: 0.1035, Training RMSE: 0.3355, Validation RMSE: 0.3217\n",
      "Epoch [402001/10000000], Training Loss: 0.1125, Validation Loss: 0.1034, Training RMSE: 0.3354, Validation RMSE: 0.3215\n",
      "Epoch [403001/10000000], Training Loss: 0.1122, Validation Loss: 0.1033, Training RMSE: 0.3349, Validation RMSE: 0.3214\n",
      "Epoch [404001/10000000], Training Loss: 0.1121, Validation Loss: 0.1032, Training RMSE: 0.3349, Validation RMSE: 0.3213\n",
      "Epoch [405001/10000000], Training Loss: 0.1126, Validation Loss: 0.1031, Training RMSE: 0.3356, Validation RMSE: 0.3211\n",
      "Epoch [406001/10000000], Training Loss: 0.1125, Validation Loss: 0.1030, Training RMSE: 0.3354, Validation RMSE: 0.3210\n",
      "Epoch [407001/10000000], Training Loss: 0.1120, Validation Loss: 0.1030, Training RMSE: 0.3347, Validation RMSE: 0.3209\n",
      "Epoch [408001/10000000], Training Loss: 0.1121, Validation Loss: 0.1029, Training RMSE: 0.3348, Validation RMSE: 0.3207\n",
      "Epoch [409001/10000000], Training Loss: 0.1121, Validation Loss: 0.1028, Training RMSE: 0.3348, Validation RMSE: 0.3206\n",
      "Epoch [410001/10000000], Training Loss: 0.1118, Validation Loss: 0.1027, Training RMSE: 0.3343, Validation RMSE: 0.3205\n",
      "Epoch [411001/10000000], Training Loss: 0.1120, Validation Loss: 0.1026, Training RMSE: 0.3346, Validation RMSE: 0.3204\n",
      "Epoch [412001/10000000], Training Loss: 0.1112, Validation Loss: 0.1025, Training RMSE: 0.3335, Validation RMSE: 0.3202\n",
      "Epoch [413001/10000000], Training Loss: 0.1114, Validation Loss: 0.1025, Training RMSE: 0.3338, Validation RMSE: 0.3201\n",
      "Epoch [414001/10000000], Training Loss: 0.1116, Validation Loss: 0.1024, Training RMSE: 0.3340, Validation RMSE: 0.3200\n",
      "Epoch [415001/10000000], Training Loss: 0.1111, Validation Loss: 0.1023, Training RMSE: 0.3334, Validation RMSE: 0.3198\n",
      "Epoch [416001/10000000], Training Loss: 0.1117, Validation Loss: 0.1022, Training RMSE: 0.3342, Validation RMSE: 0.3197\n",
      "Epoch [417001/10000000], Training Loss: 0.1109, Validation Loss: 0.1021, Training RMSE: 0.3330, Validation RMSE: 0.3196\n",
      "Epoch [418001/10000000], Training Loss: 0.1106, Validation Loss: 0.1020, Training RMSE: 0.3326, Validation RMSE: 0.3194\n",
      "Epoch [419001/10000000], Training Loss: 0.1111, Validation Loss: 0.1020, Training RMSE: 0.3333, Validation RMSE: 0.3193\n",
      "Epoch [420001/10000000], Training Loss: 0.1110, Validation Loss: 0.1019, Training RMSE: 0.3332, Validation RMSE: 0.3192\n",
      "Epoch [421001/10000000], Training Loss: 0.1114, Validation Loss: 0.1018, Training RMSE: 0.3338, Validation RMSE: 0.3190\n",
      "Epoch [422001/10000000], Training Loss: 0.1113, Validation Loss: 0.1017, Training RMSE: 0.3336, Validation RMSE: 0.3189\n",
      "Epoch [423001/10000000], Training Loss: 0.1112, Validation Loss: 0.1016, Training RMSE: 0.3334, Validation RMSE: 0.3188\n",
      "Epoch [424001/10000000], Training Loss: 0.1105, Validation Loss: 0.1015, Training RMSE: 0.3324, Validation RMSE: 0.3186\n",
      "Epoch [425001/10000000], Training Loss: 0.1095, Validation Loss: 0.1014, Training RMSE: 0.3310, Validation RMSE: 0.3185\n",
      "Epoch [426001/10000000], Training Loss: 0.1105, Validation Loss: 0.1014, Training RMSE: 0.3324, Validation RMSE: 0.3184\n",
      "Epoch [427001/10000000], Training Loss: 0.1103, Validation Loss: 0.1013, Training RMSE: 0.3321, Validation RMSE: 0.3182\n",
      "Epoch [428001/10000000], Training Loss: 0.1102, Validation Loss: 0.1012, Training RMSE: 0.3320, Validation RMSE: 0.3181\n",
      "Epoch [429001/10000000], Training Loss: 0.1103, Validation Loss: 0.1011, Training RMSE: 0.3322, Validation RMSE: 0.3180\n",
      "Epoch [430001/10000000], Training Loss: 0.1100, Validation Loss: 0.1010, Training RMSE: 0.3317, Validation RMSE: 0.3178\n",
      "Epoch [431001/10000000], Training Loss: 0.1099, Validation Loss: 0.1009, Training RMSE: 0.3315, Validation RMSE: 0.3177\n",
      "Epoch [432001/10000000], Training Loss: 0.1098, Validation Loss: 0.1009, Training RMSE: 0.3313, Validation RMSE: 0.3176\n",
      "Epoch [433001/10000000], Training Loss: 0.1097, Validation Loss: 0.1008, Training RMSE: 0.3311, Validation RMSE: 0.3174\n",
      "Epoch [434001/10000000], Training Loss: 0.1096, Validation Loss: 0.1007, Training RMSE: 0.3310, Validation RMSE: 0.3173\n",
      "Epoch [435001/10000000], Training Loss: 0.1088, Validation Loss: 0.1006, Training RMSE: 0.3299, Validation RMSE: 0.3172\n",
      "Epoch [436001/10000000], Training Loss: 0.1094, Validation Loss: 0.1005, Training RMSE: 0.3307, Validation RMSE: 0.3170\n",
      "Epoch [437001/10000000], Training Loss: 0.1089, Validation Loss: 0.1004, Training RMSE: 0.3299, Validation RMSE: 0.3169\n",
      "Epoch [438001/10000000], Training Loss: 0.1095, Validation Loss: 0.1003, Training RMSE: 0.3308, Validation RMSE: 0.3168\n",
      "Epoch [439001/10000000], Training Loss: 0.1085, Validation Loss: 0.1003, Training RMSE: 0.3294, Validation RMSE: 0.3166\n",
      "Epoch [440001/10000000], Training Loss: 0.1093, Validation Loss: 0.1002, Training RMSE: 0.3306, Validation RMSE: 0.3165\n",
      "Epoch [441001/10000000], Training Loss: 0.1086, Validation Loss: 0.1001, Training RMSE: 0.3296, Validation RMSE: 0.3164\n",
      "Epoch [442001/10000000], Training Loss: 0.1087, Validation Loss: 0.1000, Training RMSE: 0.3297, Validation RMSE: 0.3162\n",
      "Epoch [443001/10000000], Training Loss: 0.1094, Validation Loss: 0.0999, Training RMSE: 0.3307, Validation RMSE: 0.3161\n",
      "Epoch [444001/10000000], Training Loss: 0.1081, Validation Loss: 0.0998, Training RMSE: 0.3287, Validation RMSE: 0.3160\n",
      "Epoch [445001/10000000], Training Loss: 0.1086, Validation Loss: 0.0998, Training RMSE: 0.3296, Validation RMSE: 0.3158\n",
      "Epoch [446001/10000000], Training Loss: 0.1083, Validation Loss: 0.0997, Training RMSE: 0.3291, Validation RMSE: 0.3157\n",
      "Epoch [447001/10000000], Training Loss: 0.1082, Validation Loss: 0.0996, Training RMSE: 0.3289, Validation RMSE: 0.3156\n",
      "Epoch [448001/10000000], Training Loss: 0.1085, Validation Loss: 0.0995, Training RMSE: 0.3294, Validation RMSE: 0.3154\n",
      "Epoch [449001/10000000], Training Loss: 0.1083, Validation Loss: 0.0994, Training RMSE: 0.3290, Validation RMSE: 0.3153\n",
      "Epoch [450001/10000000], Training Loss: 0.1080, Validation Loss: 0.0993, Training RMSE: 0.3287, Validation RMSE: 0.3152\n",
      "Epoch [451001/10000000], Training Loss: 0.1079, Validation Loss: 0.0992, Training RMSE: 0.3285, Validation RMSE: 0.3150\n",
      "Epoch [452001/10000000], Training Loss: 0.1080, Validation Loss: 0.0992, Training RMSE: 0.3286, Validation RMSE: 0.3149\n",
      "Epoch [453001/10000000], Training Loss: 0.1077, Validation Loss: 0.0991, Training RMSE: 0.3282, Validation RMSE: 0.3148\n",
      "Epoch [454001/10000000], Training Loss: 0.1079, Validation Loss: 0.0990, Training RMSE: 0.3284, Validation RMSE: 0.3146\n",
      "Epoch [455001/10000000], Training Loss: 0.1081, Validation Loss: 0.0989, Training RMSE: 0.3289, Validation RMSE: 0.3145\n",
      "Epoch [456001/10000000], Training Loss: 0.1074, Validation Loss: 0.0988, Training RMSE: 0.3278, Validation RMSE: 0.3144\n",
      "Epoch [457001/10000000], Training Loss: 0.1069, Validation Loss: 0.0987, Training RMSE: 0.3269, Validation RMSE: 0.3142\n",
      "Epoch [458001/10000000], Training Loss: 0.1078, Validation Loss: 0.0987, Training RMSE: 0.3283, Validation RMSE: 0.3141\n",
      "Epoch [459001/10000000], Training Loss: 0.1068, Validation Loss: 0.0986, Training RMSE: 0.3269, Validation RMSE: 0.3140\n",
      "Epoch [460001/10000000], Training Loss: 0.1072, Validation Loss: 0.0985, Training RMSE: 0.3275, Validation RMSE: 0.3138\n",
      "Epoch [461001/10000000], Training Loss: 0.1066, Validation Loss: 0.0984, Training RMSE: 0.3265, Validation RMSE: 0.3137\n",
      "Epoch [462001/10000000], Training Loss: 0.1063, Validation Loss: 0.0983, Training RMSE: 0.3260, Validation RMSE: 0.3136\n",
      "Epoch [463001/10000000], Training Loss: 0.1067, Validation Loss: 0.0982, Training RMSE: 0.3266, Validation RMSE: 0.3134\n",
      "Epoch [464001/10000000], Training Loss: 0.1069, Validation Loss: 0.0982, Training RMSE: 0.3270, Validation RMSE: 0.3133\n",
      "Epoch [465001/10000000], Training Loss: 0.1068, Validation Loss: 0.0981, Training RMSE: 0.3268, Validation RMSE: 0.3132\n",
      "Epoch [466001/10000000], Training Loss: 0.1071, Validation Loss: 0.0980, Training RMSE: 0.3272, Validation RMSE: 0.3130\n",
      "Epoch [467001/10000000], Training Loss: 0.1069, Validation Loss: 0.0979, Training RMSE: 0.3270, Validation RMSE: 0.3129\n",
      "Epoch [468001/10000000], Training Loss: 0.1065, Validation Loss: 0.0978, Training RMSE: 0.3264, Validation RMSE: 0.3128\n",
      "Epoch [469001/10000000], Training Loss: 0.1058, Validation Loss: 0.0977, Training RMSE: 0.3253, Validation RMSE: 0.3126\n",
      "Epoch [470001/10000000], Training Loss: 0.1068, Validation Loss: 0.0976, Training RMSE: 0.3268, Validation RMSE: 0.3125\n",
      "Epoch [471001/10000000], Training Loss: 0.1060, Validation Loss: 0.0976, Training RMSE: 0.3256, Validation RMSE: 0.3123\n",
      "Epoch [472001/10000000], Training Loss: 0.1063, Validation Loss: 0.0975, Training RMSE: 0.3261, Validation RMSE: 0.3122\n",
      "Epoch [473001/10000000], Training Loss: 0.1060, Validation Loss: 0.0974, Training RMSE: 0.3256, Validation RMSE: 0.3121\n",
      "Epoch [474001/10000000], Training Loss: 0.1056, Validation Loss: 0.0973, Training RMSE: 0.3250, Validation RMSE: 0.3119\n",
      "Epoch [475001/10000000], Training Loss: 0.1060, Validation Loss: 0.0972, Training RMSE: 0.3256, Validation RMSE: 0.3118\n",
      "Epoch [476001/10000000], Training Loss: 0.1054, Validation Loss: 0.0971, Training RMSE: 0.3247, Validation RMSE: 0.3117\n",
      "Epoch [477001/10000000], Training Loss: 0.1052, Validation Loss: 0.0970, Training RMSE: 0.3243, Validation RMSE: 0.3115\n",
      "Epoch [478001/10000000], Training Loss: 0.1047, Validation Loss: 0.0970, Training RMSE: 0.3235, Validation RMSE: 0.3114\n",
      "Epoch [479001/10000000], Training Loss: 0.1057, Validation Loss: 0.0969, Training RMSE: 0.3251, Validation RMSE: 0.3113\n",
      "Epoch [480001/10000000], Training Loss: 0.1058, Validation Loss: 0.0968, Training RMSE: 0.3253, Validation RMSE: 0.3111\n",
      "Epoch [481001/10000000], Training Loss: 0.1047, Validation Loss: 0.0967, Training RMSE: 0.3236, Validation RMSE: 0.3110\n",
      "Epoch [482001/10000000], Training Loss: 0.1046, Validation Loss: 0.0966, Training RMSE: 0.3235, Validation RMSE: 0.3108\n",
      "Epoch [483001/10000000], Training Loss: 0.1048, Validation Loss: 0.0965, Training RMSE: 0.3237, Validation RMSE: 0.3107\n",
      "Epoch [484001/10000000], Training Loss: 0.1051, Validation Loss: 0.0965, Training RMSE: 0.3241, Validation RMSE: 0.3106\n",
      "Epoch [485001/10000000], Training Loss: 0.1047, Validation Loss: 0.0964, Training RMSE: 0.3236, Validation RMSE: 0.3104\n",
      "Epoch [486001/10000000], Training Loss: 0.1045, Validation Loss: 0.0963, Training RMSE: 0.3233, Validation RMSE: 0.3103\n",
      "Epoch [487001/10000000], Training Loss: 0.1048, Validation Loss: 0.0962, Training RMSE: 0.3237, Validation RMSE: 0.3102\n",
      "Epoch [488001/10000000], Training Loss: 0.1049, Validation Loss: 0.0961, Training RMSE: 0.3239, Validation RMSE: 0.3100\n",
      "Epoch [489001/10000000], Training Loss: 0.1054, Validation Loss: 0.0960, Training RMSE: 0.3246, Validation RMSE: 0.3099\n",
      "Epoch [490001/10000000], Training Loss: 0.1046, Validation Loss: 0.0959, Training RMSE: 0.3235, Validation RMSE: 0.3097\n",
      "Epoch [491001/10000000], Training Loss: 0.1044, Validation Loss: 0.0959, Training RMSE: 0.3231, Validation RMSE: 0.3096\n",
      "Epoch [492001/10000000], Training Loss: 0.1038, Validation Loss: 0.0958, Training RMSE: 0.3222, Validation RMSE: 0.3095\n",
      "Epoch [493001/10000000], Training Loss: 0.1042, Validation Loss: 0.0957, Training RMSE: 0.3228, Validation RMSE: 0.3093\n",
      "Epoch [494001/10000000], Training Loss: 0.1044, Validation Loss: 0.0956, Training RMSE: 0.3232, Validation RMSE: 0.3092\n",
      "Epoch [495001/10000000], Training Loss: 0.1047, Validation Loss: 0.0955, Training RMSE: 0.3236, Validation RMSE: 0.3091\n",
      "Epoch [496001/10000000], Training Loss: 0.1038, Validation Loss: 0.0954, Training RMSE: 0.3221, Validation RMSE: 0.3089\n",
      "Epoch [497001/10000000], Training Loss: 0.1040, Validation Loss: 0.0953, Training RMSE: 0.3225, Validation RMSE: 0.3088\n",
      "Epoch [498001/10000000], Training Loss: 0.1045, Validation Loss: 0.0953, Training RMSE: 0.3233, Validation RMSE: 0.3086\n",
      "Epoch [499001/10000000], Training Loss: 0.1031, Validation Loss: 0.0952, Training RMSE: 0.3211, Validation RMSE: 0.3085\n",
      "Epoch [500001/10000000], Training Loss: 0.1037, Validation Loss: 0.0951, Training RMSE: 0.3221, Validation RMSE: 0.3084\n",
      "Epoch [501001/10000000], Training Loss: 0.1039, Validation Loss: 0.0950, Training RMSE: 0.3223, Validation RMSE: 0.3082\n",
      "Epoch [502001/10000000], Training Loss: 0.1035, Validation Loss: 0.0949, Training RMSE: 0.3217, Validation RMSE: 0.3081\n",
      "Epoch [503001/10000000], Training Loss: 0.1031, Validation Loss: 0.0948, Training RMSE: 0.3211, Validation RMSE: 0.3079\n",
      "Epoch [504001/10000000], Training Loss: 0.1030, Validation Loss: 0.0947, Training RMSE: 0.3209, Validation RMSE: 0.3078\n",
      "Epoch [505001/10000000], Training Loss: 0.1028, Validation Loss: 0.0947, Training RMSE: 0.3206, Validation RMSE: 0.3077\n",
      "Epoch [506001/10000000], Training Loss: 0.1033, Validation Loss: 0.0946, Training RMSE: 0.3214, Validation RMSE: 0.3075\n",
      "Epoch [507001/10000000], Training Loss: 0.1026, Validation Loss: 0.0945, Training RMSE: 0.3204, Validation RMSE: 0.3074\n",
      "Epoch [508001/10000000], Training Loss: 0.1028, Validation Loss: 0.0944, Training RMSE: 0.3207, Validation RMSE: 0.3072\n",
      "Epoch [509001/10000000], Training Loss: 0.1025, Validation Loss: 0.0943, Training RMSE: 0.3202, Validation RMSE: 0.3071\n",
      "Epoch [510001/10000000], Training Loss: 0.1029, Validation Loss: 0.0942, Training RMSE: 0.3208, Validation RMSE: 0.3070\n",
      "Epoch [511001/10000000], Training Loss: 0.1030, Validation Loss: 0.0941, Training RMSE: 0.3209, Validation RMSE: 0.3068\n",
      "Epoch [512001/10000000], Training Loss: 0.1028, Validation Loss: 0.0940, Training RMSE: 0.3206, Validation RMSE: 0.3067\n",
      "Epoch [513001/10000000], Training Loss: 0.1029, Validation Loss: 0.0940, Training RMSE: 0.3208, Validation RMSE: 0.3065\n",
      "Epoch [514001/10000000], Training Loss: 0.1021, Validation Loss: 0.0939, Training RMSE: 0.3195, Validation RMSE: 0.3064\n",
      "Epoch [515001/10000000], Training Loss: 0.1022, Validation Loss: 0.0938, Training RMSE: 0.3196, Validation RMSE: 0.3062\n",
      "Epoch [516001/10000000], Training Loss: 0.1018, Validation Loss: 0.0937, Training RMSE: 0.3191, Validation RMSE: 0.3061\n",
      "Epoch [517001/10000000], Training Loss: 0.1017, Validation Loss: 0.0936, Training RMSE: 0.3188, Validation RMSE: 0.3060\n",
      "Epoch [518001/10000000], Training Loss: 0.1017, Validation Loss: 0.0935, Training RMSE: 0.3189, Validation RMSE: 0.3058\n",
      "Epoch [519001/10000000], Training Loss: 0.1019, Validation Loss: 0.0934, Training RMSE: 0.3192, Validation RMSE: 0.3057\n",
      "Epoch [520001/10000000], Training Loss: 0.1016, Validation Loss: 0.0933, Training RMSE: 0.3188, Validation RMSE: 0.3055\n",
      "Epoch [521001/10000000], Training Loss: 0.1013, Validation Loss: 0.0933, Training RMSE: 0.3183, Validation RMSE: 0.3054\n",
      "Epoch [522001/10000000], Training Loss: 0.1018, Validation Loss: 0.0932, Training RMSE: 0.3191, Validation RMSE: 0.3052\n",
      "Epoch [523001/10000000], Training Loss: 0.1022, Validation Loss: 0.0931, Training RMSE: 0.3197, Validation RMSE: 0.3051\n",
      "Epoch [524001/10000000], Training Loss: 0.1017, Validation Loss: 0.0930, Training RMSE: 0.3189, Validation RMSE: 0.3050\n",
      "Epoch [525001/10000000], Training Loss: 0.1022, Validation Loss: 0.0929, Training RMSE: 0.3197, Validation RMSE: 0.3048\n",
      "Epoch [526001/10000000], Training Loss: 0.1011, Validation Loss: 0.0928, Training RMSE: 0.3180, Validation RMSE: 0.3047\n",
      "Epoch [527001/10000000], Training Loss: 0.1016, Validation Loss: 0.0927, Training RMSE: 0.3187, Validation RMSE: 0.3045\n",
      "Epoch [528001/10000000], Training Loss: 0.1014, Validation Loss: 0.0927, Training RMSE: 0.3184, Validation RMSE: 0.3044\n",
      "Epoch [529001/10000000], Training Loss: 0.1011, Validation Loss: 0.0926, Training RMSE: 0.3180, Validation RMSE: 0.3042\n",
      "Epoch [530001/10000000], Training Loss: 0.1012, Validation Loss: 0.0925, Training RMSE: 0.3182, Validation RMSE: 0.3041\n",
      "Epoch [531001/10000000], Training Loss: 0.1005, Validation Loss: 0.0924, Training RMSE: 0.3171, Validation RMSE: 0.3040\n",
      "Epoch [532001/10000000], Training Loss: 0.1004, Validation Loss: 0.0923, Training RMSE: 0.3169, Validation RMSE: 0.3038\n",
      "Epoch [533001/10000000], Training Loss: 0.1010, Validation Loss: 0.0922, Training RMSE: 0.3178, Validation RMSE: 0.3037\n",
      "Epoch [534001/10000000], Training Loss: 0.1008, Validation Loss: 0.0921, Training RMSE: 0.3174, Validation RMSE: 0.3035\n",
      "Epoch [535001/10000000], Training Loss: 0.1000, Validation Loss: 0.0920, Training RMSE: 0.3163, Validation RMSE: 0.3034\n",
      "Epoch [536001/10000000], Training Loss: 0.1006, Validation Loss: 0.0920, Training RMSE: 0.3172, Validation RMSE: 0.3032\n",
      "Epoch [537001/10000000], Training Loss: 0.0997, Validation Loss: 0.0919, Training RMSE: 0.3158, Validation RMSE: 0.3031\n",
      "Epoch [538001/10000000], Training Loss: 0.0998, Validation Loss: 0.0918, Training RMSE: 0.3159, Validation RMSE: 0.3029\n",
      "Epoch [539001/10000000], Training Loss: 0.1002, Validation Loss: 0.0917, Training RMSE: 0.3165, Validation RMSE: 0.3028\n",
      "Epoch [540001/10000000], Training Loss: 0.0998, Validation Loss: 0.0916, Training RMSE: 0.3159, Validation RMSE: 0.3027\n",
      "Epoch [541001/10000000], Training Loss: 0.1002, Validation Loss: 0.0915, Training RMSE: 0.3165, Validation RMSE: 0.3025\n",
      "Epoch [542001/10000000], Training Loss: 0.0997, Validation Loss: 0.0914, Training RMSE: 0.3157, Validation RMSE: 0.3024\n",
      "Epoch [543001/10000000], Training Loss: 0.0994, Validation Loss: 0.0913, Training RMSE: 0.3153, Validation RMSE: 0.3022\n",
      "Epoch [544001/10000000], Training Loss: 0.0995, Validation Loss: 0.0912, Training RMSE: 0.3155, Validation RMSE: 0.3021\n",
      "Epoch [545001/10000000], Training Loss: 0.0995, Validation Loss: 0.0912, Training RMSE: 0.3154, Validation RMSE: 0.3019\n",
      "Epoch [546001/10000000], Training Loss: 0.0994, Validation Loss: 0.0911, Training RMSE: 0.3153, Validation RMSE: 0.3018\n",
      "Epoch [547001/10000000], Training Loss: 0.0995, Validation Loss: 0.0910, Training RMSE: 0.3155, Validation RMSE: 0.3016\n",
      "Epoch [548001/10000000], Training Loss: 0.0991, Validation Loss: 0.0909, Training RMSE: 0.3147, Validation RMSE: 0.3015\n",
      "Epoch [549001/10000000], Training Loss: 0.0989, Validation Loss: 0.0908, Training RMSE: 0.3145, Validation RMSE: 0.3013\n",
      "Epoch [550001/10000000], Training Loss: 0.0989, Validation Loss: 0.0907, Training RMSE: 0.3146, Validation RMSE: 0.3012\n",
      "Epoch [551001/10000000], Training Loss: 0.0994, Validation Loss: 0.0906, Training RMSE: 0.3153, Validation RMSE: 0.3011\n",
      "Epoch [552001/10000000], Training Loss: 0.0996, Validation Loss: 0.0905, Training RMSE: 0.3156, Validation RMSE: 0.3009\n",
      "Epoch [553001/10000000], Training Loss: 0.0988, Validation Loss: 0.0905, Training RMSE: 0.3144, Validation RMSE: 0.3008\n",
      "Epoch [554001/10000000], Training Loss: 0.0986, Validation Loss: 0.0904, Training RMSE: 0.3140, Validation RMSE: 0.3006\n",
      "Epoch [555001/10000000], Training Loss: 0.0985, Validation Loss: 0.0903, Training RMSE: 0.3139, Validation RMSE: 0.3005\n",
      "Epoch [556001/10000000], Training Loss: 0.0991, Validation Loss: 0.0902, Training RMSE: 0.3147, Validation RMSE: 0.3003\n",
      "Epoch [557001/10000000], Training Loss: 0.0984, Validation Loss: 0.0901, Training RMSE: 0.3138, Validation RMSE: 0.3002\n",
      "Epoch [558001/10000000], Training Loss: 0.0978, Validation Loss: 0.0900, Training RMSE: 0.3128, Validation RMSE: 0.3000\n",
      "Epoch [559001/10000000], Training Loss: 0.0983, Validation Loss: 0.0899, Training RMSE: 0.3136, Validation RMSE: 0.2999\n",
      "Epoch [560001/10000000], Training Loss: 0.0981, Validation Loss: 0.0898, Training RMSE: 0.3133, Validation RMSE: 0.2997\n",
      "Epoch [561001/10000000], Training Loss: 0.0982, Validation Loss: 0.0898, Training RMSE: 0.3134, Validation RMSE: 0.2996\n",
      "Epoch [562001/10000000], Training Loss: 0.0979, Validation Loss: 0.0897, Training RMSE: 0.3129, Validation RMSE: 0.2995\n",
      "Epoch [563001/10000000], Training Loss: 0.0984, Validation Loss: 0.0896, Training RMSE: 0.3136, Validation RMSE: 0.2993\n",
      "Epoch [564001/10000000], Training Loss: 0.0984, Validation Loss: 0.0895, Training RMSE: 0.3136, Validation RMSE: 0.2992\n",
      "Epoch [565001/10000000], Training Loss: 0.0983, Validation Loss: 0.0894, Training RMSE: 0.3135, Validation RMSE: 0.2990\n",
      "Epoch [566001/10000000], Training Loss: 0.0974, Validation Loss: 0.0893, Training RMSE: 0.3120, Validation RMSE: 0.2989\n",
      "Epoch [567001/10000000], Training Loss: 0.0968, Validation Loss: 0.0892, Training RMSE: 0.3111, Validation RMSE: 0.2987\n",
      "Epoch [568001/10000000], Training Loss: 0.0975, Validation Loss: 0.0892, Training RMSE: 0.3123, Validation RMSE: 0.2986\n",
      "Epoch [569001/10000000], Training Loss: 0.0971, Validation Loss: 0.0891, Training RMSE: 0.3116, Validation RMSE: 0.2984\n",
      "Epoch [570001/10000000], Training Loss: 0.0969, Validation Loss: 0.0890, Training RMSE: 0.3113, Validation RMSE: 0.2983\n",
      "Epoch [571001/10000000], Training Loss: 0.0974, Validation Loss: 0.0889, Training RMSE: 0.3121, Validation RMSE: 0.2981\n",
      "Epoch [572001/10000000], Training Loss: 0.0967, Validation Loss: 0.0888, Training RMSE: 0.3110, Validation RMSE: 0.2980\n",
      "Epoch [573001/10000000], Training Loss: 0.0971, Validation Loss: 0.0887, Training RMSE: 0.3117, Validation RMSE: 0.2979\n",
      "Epoch [574001/10000000], Training Loss: 0.0973, Validation Loss: 0.0886, Training RMSE: 0.3120, Validation RMSE: 0.2977\n",
      "Epoch [575001/10000000], Training Loss: 0.0971, Validation Loss: 0.0885, Training RMSE: 0.3115, Validation RMSE: 0.2976\n",
      "Epoch [576001/10000000], Training Loss: 0.0968, Validation Loss: 0.0885, Training RMSE: 0.3111, Validation RMSE: 0.2974\n",
      "Epoch [577001/10000000], Training Loss: 0.0966, Validation Loss: 0.0884, Training RMSE: 0.3108, Validation RMSE: 0.2973\n",
      "Epoch [578001/10000000], Training Loss: 0.0968, Validation Loss: 0.0883, Training RMSE: 0.3111, Validation RMSE: 0.2971\n",
      "Epoch [579001/10000000], Training Loss: 0.0966, Validation Loss: 0.0882, Training RMSE: 0.3108, Validation RMSE: 0.2970\n",
      "Epoch [580001/10000000], Training Loss: 0.0960, Validation Loss: 0.0881, Training RMSE: 0.3098, Validation RMSE: 0.2968\n",
      "Epoch [581001/10000000], Training Loss: 0.0961, Validation Loss: 0.0880, Training RMSE: 0.3100, Validation RMSE: 0.2967\n",
      "Epoch [582001/10000000], Training Loss: 0.0966, Validation Loss: 0.0879, Training RMSE: 0.3108, Validation RMSE: 0.2966\n",
      "Epoch [583001/10000000], Training Loss: 0.0965, Validation Loss: 0.0879, Training RMSE: 0.3107, Validation RMSE: 0.2964\n",
      "Epoch [584001/10000000], Training Loss: 0.0964, Validation Loss: 0.0878, Training RMSE: 0.3106, Validation RMSE: 0.2963\n",
      "Epoch [585001/10000000], Training Loss: 0.0955, Validation Loss: 0.0877, Training RMSE: 0.3091, Validation RMSE: 0.2961\n",
      "Epoch [586001/10000000], Training Loss: 0.0958, Validation Loss: 0.0876, Training RMSE: 0.3096, Validation RMSE: 0.2960\n",
      "Epoch [587001/10000000], Training Loss: 0.0961, Validation Loss: 0.0875, Training RMSE: 0.3100, Validation RMSE: 0.2958\n",
      "Epoch [588001/10000000], Training Loss: 0.0950, Validation Loss: 0.0874, Training RMSE: 0.3082, Validation RMSE: 0.2957\n",
      "Epoch [589001/10000000], Training Loss: 0.0954, Validation Loss: 0.0874, Training RMSE: 0.3088, Validation RMSE: 0.2956\n",
      "Epoch [590001/10000000], Training Loss: 0.0954, Validation Loss: 0.0873, Training RMSE: 0.3088, Validation RMSE: 0.2954\n",
      "Epoch [591001/10000000], Training Loss: 0.0965, Validation Loss: 0.0872, Training RMSE: 0.3107, Validation RMSE: 0.2953\n",
      "Epoch [592001/10000000], Training Loss: 0.0958, Validation Loss: 0.0871, Training RMSE: 0.3095, Validation RMSE: 0.2951\n",
      "Epoch [593001/10000000], Training Loss: 0.0957, Validation Loss: 0.0870, Training RMSE: 0.3093, Validation RMSE: 0.2950\n",
      "Epoch [594001/10000000], Training Loss: 0.0959, Validation Loss: 0.0869, Training RMSE: 0.3097, Validation RMSE: 0.2949\n",
      "Epoch [595001/10000000], Training Loss: 0.0961, Validation Loss: 0.0869, Training RMSE: 0.3100, Validation RMSE: 0.2947\n",
      "Epoch [596001/10000000], Training Loss: 0.0954, Validation Loss: 0.0868, Training RMSE: 0.3089, Validation RMSE: 0.2946\n",
      "Epoch [597001/10000000], Training Loss: 0.0941, Validation Loss: 0.0867, Training RMSE: 0.3068, Validation RMSE: 0.2944\n",
      "Epoch [598001/10000000], Training Loss: 0.0946, Validation Loss: 0.0866, Training RMSE: 0.3075, Validation RMSE: 0.2943\n",
      "Epoch [599001/10000000], Training Loss: 0.0948, Validation Loss: 0.0865, Training RMSE: 0.3079, Validation RMSE: 0.2942\n",
      "Epoch [600001/10000000], Training Loss: 0.0947, Validation Loss: 0.0864, Training RMSE: 0.3078, Validation RMSE: 0.2940\n",
      "Epoch [601001/10000000], Training Loss: 0.0957, Validation Loss: 0.0864, Training RMSE: 0.3094, Validation RMSE: 0.2939\n",
      "Epoch [602001/10000000], Training Loss: 0.0950, Validation Loss: 0.0863, Training RMSE: 0.3082, Validation RMSE: 0.2937\n",
      "Epoch [603001/10000000], Training Loss: 0.0948, Validation Loss: 0.0862, Training RMSE: 0.3079, Validation RMSE: 0.2936\n",
      "Epoch [604001/10000000], Training Loss: 0.0940, Validation Loss: 0.0861, Training RMSE: 0.3066, Validation RMSE: 0.2935\n",
      "Epoch [605001/10000000], Training Loss: 0.0945, Validation Loss: 0.0860, Training RMSE: 0.3074, Validation RMSE: 0.2933\n",
      "Epoch [606001/10000000], Training Loss: 0.0952, Validation Loss: 0.0860, Training RMSE: 0.3086, Validation RMSE: 0.2932\n",
      "Epoch [607001/10000000], Training Loss: 0.0945, Validation Loss: 0.0859, Training RMSE: 0.3074, Validation RMSE: 0.2930\n",
      "Epoch [608001/10000000], Training Loss: 0.0940, Validation Loss: 0.0858, Training RMSE: 0.3065, Validation RMSE: 0.2929\n",
      "Epoch [609001/10000000], Training Loss: 0.0951, Validation Loss: 0.0857, Training RMSE: 0.3083, Validation RMSE: 0.2928\n",
      "Epoch [610001/10000000], Training Loss: 0.0954, Validation Loss: 0.0856, Training RMSE: 0.3089, Validation RMSE: 0.2926\n",
      "Epoch [611001/10000000], Training Loss: 0.0948, Validation Loss: 0.0856, Training RMSE: 0.3080, Validation RMSE: 0.2925\n",
      "Epoch [612001/10000000], Training Loss: 0.0944, Validation Loss: 0.0855, Training RMSE: 0.3073, Validation RMSE: 0.2924\n",
      "Epoch [613001/10000000], Training Loss: 0.0945, Validation Loss: 0.0854, Training RMSE: 0.3074, Validation RMSE: 0.2922\n",
      "Epoch [614001/10000000], Training Loss: 0.0942, Validation Loss: 0.0853, Training RMSE: 0.3070, Validation RMSE: 0.2921\n",
      "Epoch [615001/10000000], Training Loss: 0.0934, Validation Loss: 0.0852, Training RMSE: 0.3056, Validation RMSE: 0.2920\n",
      "Epoch [616001/10000000], Training Loss: 0.0944, Validation Loss: 0.0852, Training RMSE: 0.3073, Validation RMSE: 0.2918\n",
      "Epoch [617001/10000000], Training Loss: 0.0937, Validation Loss: 0.0851, Training RMSE: 0.3062, Validation RMSE: 0.2917\n",
      "Epoch [618001/10000000], Training Loss: 0.0935, Validation Loss: 0.0850, Training RMSE: 0.3058, Validation RMSE: 0.2916\n",
      "Epoch [619001/10000000], Training Loss: 0.0938, Validation Loss: 0.0849, Training RMSE: 0.3063, Validation RMSE: 0.2914\n",
      "Epoch [620001/10000000], Training Loss: 0.0931, Validation Loss: 0.0849, Training RMSE: 0.3051, Validation RMSE: 0.2913\n",
      "Epoch [621001/10000000], Training Loss: 0.0933, Validation Loss: 0.0848, Training RMSE: 0.3054, Validation RMSE: 0.2912\n",
      "Epoch [622001/10000000], Training Loss: 0.0940, Validation Loss: 0.0847, Training RMSE: 0.3065, Validation RMSE: 0.2910\n",
      "Epoch [623001/10000000], Training Loss: 0.0930, Validation Loss: 0.0846, Training RMSE: 0.3049, Validation RMSE: 0.2909\n",
      "Epoch [624001/10000000], Training Loss: 0.0928, Validation Loss: 0.0845, Training RMSE: 0.3047, Validation RMSE: 0.2908\n",
      "Epoch [625001/10000000], Training Loss: 0.0934, Validation Loss: 0.0845, Training RMSE: 0.3055, Validation RMSE: 0.2906\n",
      "Epoch [626001/10000000], Training Loss: 0.0937, Validation Loss: 0.0844, Training RMSE: 0.3061, Validation RMSE: 0.2905\n",
      "Epoch [627001/10000000], Training Loss: 0.0925, Validation Loss: 0.0843, Training RMSE: 0.3041, Validation RMSE: 0.2904\n",
      "Epoch [628001/10000000], Training Loss: 0.0931, Validation Loss: 0.0842, Training RMSE: 0.3051, Validation RMSE: 0.2902\n",
      "Epoch [629001/10000000], Training Loss: 0.0933, Validation Loss: 0.0842, Training RMSE: 0.3054, Validation RMSE: 0.2901\n",
      "Epoch [630001/10000000], Training Loss: 0.0922, Validation Loss: 0.0841, Training RMSE: 0.3036, Validation RMSE: 0.2900\n",
      "Epoch [631001/10000000], Training Loss: 0.0928, Validation Loss: 0.0840, Training RMSE: 0.3047, Validation RMSE: 0.2899\n",
      "Epoch [632001/10000000], Training Loss: 0.0930, Validation Loss: 0.0839, Training RMSE: 0.3050, Validation RMSE: 0.2897\n",
      "Epoch [633001/10000000], Training Loss: 0.0923, Validation Loss: 0.0839, Training RMSE: 0.3038, Validation RMSE: 0.2896\n",
      "Epoch [634001/10000000], Training Loss: 0.0933, Validation Loss: 0.0838, Training RMSE: 0.3055, Validation RMSE: 0.2895\n",
      "Epoch [635001/10000000], Training Loss: 0.0926, Validation Loss: 0.0837, Training RMSE: 0.3042, Validation RMSE: 0.2893\n",
      "Epoch [636001/10000000], Training Loss: 0.0916, Validation Loss: 0.0836, Training RMSE: 0.3027, Validation RMSE: 0.2892\n",
      "Epoch [637001/10000000], Training Loss: 0.0917, Validation Loss: 0.0836, Training RMSE: 0.3028, Validation RMSE: 0.2891\n",
      "Epoch [638001/10000000], Training Loss: 0.0918, Validation Loss: 0.0835, Training RMSE: 0.3029, Validation RMSE: 0.2890\n",
      "Epoch [639001/10000000], Training Loss: 0.0925, Validation Loss: 0.0834, Training RMSE: 0.3041, Validation RMSE: 0.2888\n",
      "Epoch [640001/10000000], Training Loss: 0.0928, Validation Loss: 0.0834, Training RMSE: 0.3046, Validation RMSE: 0.2887\n",
      "Epoch [641001/10000000], Training Loss: 0.0920, Validation Loss: 0.0833, Training RMSE: 0.3033, Validation RMSE: 0.2886\n",
      "Epoch [642001/10000000], Training Loss: 0.0916, Validation Loss: 0.0832, Training RMSE: 0.3027, Validation RMSE: 0.2885\n",
      "Epoch [643001/10000000], Training Loss: 0.0920, Validation Loss: 0.0831, Training RMSE: 0.3033, Validation RMSE: 0.2884\n",
      "Epoch [644001/10000000], Training Loss: 0.0911, Validation Loss: 0.0831, Training RMSE: 0.3019, Validation RMSE: 0.2882\n",
      "Epoch [645001/10000000], Training Loss: 0.0922, Validation Loss: 0.0830, Training RMSE: 0.3037, Validation RMSE: 0.2881\n",
      "Epoch [646001/10000000], Training Loss: 0.0911, Validation Loss: 0.0829, Training RMSE: 0.3019, Validation RMSE: 0.2880\n",
      "Epoch [647001/10000000], Training Loss: 0.0912, Validation Loss: 0.0829, Training RMSE: 0.3020, Validation RMSE: 0.2879\n",
      "Epoch [648001/10000000], Training Loss: 0.0920, Validation Loss: 0.0828, Training RMSE: 0.3032, Validation RMSE: 0.2877\n",
      "Epoch [649001/10000000], Training Loss: 0.0920, Validation Loss: 0.0827, Training RMSE: 0.3033, Validation RMSE: 0.2876\n",
      "Epoch [650001/10000000], Training Loss: 0.0915, Validation Loss: 0.0827, Training RMSE: 0.3025, Validation RMSE: 0.2875\n",
      "Epoch [651001/10000000], Training Loss: 0.0915, Validation Loss: 0.0826, Training RMSE: 0.3024, Validation RMSE: 0.2874\n",
      "Epoch [652001/10000000], Training Loss: 0.0916, Validation Loss: 0.0825, Training RMSE: 0.3027, Validation RMSE: 0.2873\n",
      "Epoch [653001/10000000], Training Loss: 0.0913, Validation Loss: 0.0824, Training RMSE: 0.3021, Validation RMSE: 0.2871\n",
      "Epoch [654001/10000000], Training Loss: 0.0909, Validation Loss: 0.0824, Training RMSE: 0.3015, Validation RMSE: 0.2870\n",
      "Epoch [655001/10000000], Training Loss: 0.0915, Validation Loss: 0.0823, Training RMSE: 0.3025, Validation RMSE: 0.2869\n",
      "Epoch [656001/10000000], Training Loss: 0.0904, Validation Loss: 0.0822, Training RMSE: 0.3007, Validation RMSE: 0.2868\n",
      "Epoch [657001/10000000], Training Loss: 0.0912, Validation Loss: 0.0822, Training RMSE: 0.3019, Validation RMSE: 0.2867\n",
      "Epoch [658001/10000000], Training Loss: 0.0913, Validation Loss: 0.0821, Training RMSE: 0.3022, Validation RMSE: 0.2865\n",
      "Epoch [659001/10000000], Training Loss: 0.0901, Validation Loss: 0.0820, Training RMSE: 0.3002, Validation RMSE: 0.2864\n",
      "Epoch [660001/10000000], Training Loss: 0.0902, Validation Loss: 0.0820, Training RMSE: 0.3004, Validation RMSE: 0.2863\n",
      "Epoch [661001/10000000], Training Loss: 0.0902, Validation Loss: 0.0819, Training RMSE: 0.3003, Validation RMSE: 0.2862\n",
      "Epoch [662001/10000000], Training Loss: 0.0901, Validation Loss: 0.0818, Training RMSE: 0.3001, Validation RMSE: 0.2861\n",
      "Epoch [663001/10000000], Training Loss: 0.0912, Validation Loss: 0.0818, Training RMSE: 0.3020, Validation RMSE: 0.2860\n",
      "Epoch [664001/10000000], Training Loss: 0.0909, Validation Loss: 0.0817, Training RMSE: 0.3016, Validation RMSE: 0.2858\n",
      "Epoch [665001/10000000], Training Loss: 0.0905, Validation Loss: 0.0816, Training RMSE: 0.3009, Validation RMSE: 0.2857\n",
      "Epoch [666001/10000000], Training Loss: 0.0912, Validation Loss: 0.0816, Training RMSE: 0.3020, Validation RMSE: 0.2856\n",
      "Epoch [667001/10000000], Training Loss: 0.0911, Validation Loss: 0.0815, Training RMSE: 0.3018, Validation RMSE: 0.2855\n",
      "Epoch [668001/10000000], Training Loss: 0.0913, Validation Loss: 0.0814, Training RMSE: 0.3022, Validation RMSE: 0.2854\n",
      "Epoch [669001/10000000], Training Loss: 0.0908, Validation Loss: 0.0814, Training RMSE: 0.3013, Validation RMSE: 0.2853\n",
      "Epoch [670001/10000000], Training Loss: 0.0893, Validation Loss: 0.0813, Training RMSE: 0.2988, Validation RMSE: 0.2852\n",
      "Epoch [671001/10000000], Training Loss: 0.0899, Validation Loss: 0.0812, Training RMSE: 0.2999, Validation RMSE: 0.2850\n",
      "Epoch [672001/10000000], Training Loss: 0.0900, Validation Loss: 0.0812, Training RMSE: 0.3001, Validation RMSE: 0.2849\n",
      "Epoch [673001/10000000], Training Loss: 0.0895, Validation Loss: 0.0811, Training RMSE: 0.2992, Validation RMSE: 0.2848\n",
      "Epoch [674001/10000000], Training Loss: 0.0908, Validation Loss: 0.0811, Training RMSE: 0.3014, Validation RMSE: 0.2847\n",
      "Epoch [675001/10000000], Training Loss: 0.0895, Validation Loss: 0.0810, Training RMSE: 0.2992, Validation RMSE: 0.2846\n",
      "Epoch [676001/10000000], Training Loss: 0.0899, Validation Loss: 0.0809, Training RMSE: 0.2998, Validation RMSE: 0.2845\n",
      "Epoch [677001/10000000], Training Loss: 0.0903, Validation Loss: 0.0809, Training RMSE: 0.3004, Validation RMSE: 0.2844\n",
      "Epoch [678001/10000000], Training Loss: 0.0898, Validation Loss: 0.0808, Training RMSE: 0.2997, Validation RMSE: 0.2843\n",
      "Epoch [679001/10000000], Training Loss: 0.0898, Validation Loss: 0.0807, Training RMSE: 0.2997, Validation RMSE: 0.2841\n",
      "Epoch [680001/10000000], Training Loss: 0.0903, Validation Loss: 0.0807, Training RMSE: 0.3005, Validation RMSE: 0.2840\n",
      "Epoch [681001/10000000], Training Loss: 0.0891, Validation Loss: 0.0806, Training RMSE: 0.2986, Validation RMSE: 0.2839\n",
      "Epoch [682001/10000000], Training Loss: 0.0904, Validation Loss: 0.0806, Training RMSE: 0.3007, Validation RMSE: 0.2838\n",
      "Epoch [683001/10000000], Training Loss: 0.0886, Validation Loss: 0.0805, Training RMSE: 0.2977, Validation RMSE: 0.2837\n",
      "Epoch [684001/10000000], Training Loss: 0.0894, Validation Loss: 0.0804, Training RMSE: 0.2991, Validation RMSE: 0.2836\n",
      "Epoch [685001/10000000], Training Loss: 0.0891, Validation Loss: 0.0804, Training RMSE: 0.2985, Validation RMSE: 0.2835\n",
      "Epoch [686001/10000000], Training Loss: 0.0886, Validation Loss: 0.0803, Training RMSE: 0.2977, Validation RMSE: 0.2834\n",
      "Epoch [687001/10000000], Training Loss: 0.0894, Validation Loss: 0.0802, Training RMSE: 0.2990, Validation RMSE: 0.2833\n",
      "Epoch [688001/10000000], Training Loss: 0.0896, Validation Loss: 0.0802, Training RMSE: 0.2993, Validation RMSE: 0.2832\n",
      "Epoch [689001/10000000], Training Loss: 0.0891, Validation Loss: 0.0801, Training RMSE: 0.2985, Validation RMSE: 0.2831\n",
      "Epoch [690001/10000000], Training Loss: 0.0890, Validation Loss: 0.0801, Training RMSE: 0.2984, Validation RMSE: 0.2829\n",
      "Epoch [691001/10000000], Training Loss: 0.0893, Validation Loss: 0.0800, Training RMSE: 0.2989, Validation RMSE: 0.2828\n",
      "Epoch [692001/10000000], Training Loss: 0.0886, Validation Loss: 0.0799, Training RMSE: 0.2977, Validation RMSE: 0.2827\n",
      "Epoch [693001/10000000], Training Loss: 0.0882, Validation Loss: 0.0799, Training RMSE: 0.2969, Validation RMSE: 0.2826\n",
      "Epoch [694001/10000000], Training Loss: 0.0883, Validation Loss: 0.0798, Training RMSE: 0.2972, Validation RMSE: 0.2825\n",
      "Epoch [695001/10000000], Training Loss: 0.0883, Validation Loss: 0.0798, Training RMSE: 0.2972, Validation RMSE: 0.2824\n",
      "Epoch [696001/10000000], Training Loss: 0.0887, Validation Loss: 0.0797, Training RMSE: 0.2979, Validation RMSE: 0.2823\n",
      "Epoch [697001/10000000], Training Loss: 0.0888, Validation Loss: 0.0796, Training RMSE: 0.2981, Validation RMSE: 0.2822\n",
      "Epoch [698001/10000000], Training Loss: 0.0883, Validation Loss: 0.0796, Training RMSE: 0.2971, Validation RMSE: 0.2821\n",
      "Epoch [699001/10000000], Training Loss: 0.0895, Validation Loss: 0.0795, Training RMSE: 0.2992, Validation RMSE: 0.2820\n",
      "Epoch [700001/10000000], Training Loss: 0.0885, Validation Loss: 0.0795, Training RMSE: 0.2974, Validation RMSE: 0.2819\n",
      "Epoch [701001/10000000], Training Loss: 0.0881, Validation Loss: 0.0794, Training RMSE: 0.2967, Validation RMSE: 0.2818\n",
      "Epoch [702001/10000000], Training Loss: 0.0886, Validation Loss: 0.0794, Training RMSE: 0.2976, Validation RMSE: 0.2817\n",
      "Epoch [703001/10000000], Training Loss: 0.0881, Validation Loss: 0.0793, Training RMSE: 0.2968, Validation RMSE: 0.2816\n",
      "Epoch [704001/10000000], Training Loss: 0.0880, Validation Loss: 0.0792, Training RMSE: 0.2966, Validation RMSE: 0.2815\n",
      "Epoch [705001/10000000], Training Loss: 0.0872, Validation Loss: 0.0792, Training RMSE: 0.2954, Validation RMSE: 0.2814\n",
      "Epoch [706001/10000000], Training Loss: 0.0882, Validation Loss: 0.0791, Training RMSE: 0.2970, Validation RMSE: 0.2813\n",
      "Epoch [707001/10000000], Training Loss: 0.0884, Validation Loss: 0.0791, Training RMSE: 0.2973, Validation RMSE: 0.2812\n",
      "Epoch [708001/10000000], Training Loss: 0.0874, Validation Loss: 0.0790, Training RMSE: 0.2957, Validation RMSE: 0.2811\n",
      "Epoch [709001/10000000], Training Loss: 0.0875, Validation Loss: 0.0789, Training RMSE: 0.2958, Validation RMSE: 0.2810\n",
      "Epoch [710001/10000000], Training Loss: 0.0875, Validation Loss: 0.0789, Training RMSE: 0.2959, Validation RMSE: 0.2809\n",
      "Epoch [711001/10000000], Training Loss: 0.0869, Validation Loss: 0.0788, Training RMSE: 0.2948, Validation RMSE: 0.2808\n",
      "Epoch [712001/10000000], Training Loss: 0.0878, Validation Loss: 0.0788, Training RMSE: 0.2963, Validation RMSE: 0.2807\n",
      "Epoch [713001/10000000], Training Loss: 0.0879, Validation Loss: 0.0787, Training RMSE: 0.2965, Validation RMSE: 0.2806\n",
      "Epoch [714001/10000000], Training Loss: 0.0881, Validation Loss: 0.0787, Training RMSE: 0.2968, Validation RMSE: 0.2805\n",
      "Epoch [715001/10000000], Training Loss: 0.0873, Validation Loss: 0.0786, Training RMSE: 0.2955, Validation RMSE: 0.2804\n",
      "Epoch [716001/10000000], Training Loss: 0.0870, Validation Loss: 0.0786, Training RMSE: 0.2949, Validation RMSE: 0.2803\n",
      "Epoch [717001/10000000], Training Loss: 0.0889, Validation Loss: 0.0785, Training RMSE: 0.2981, Validation RMSE: 0.2802\n",
      "Epoch [718001/10000000], Training Loss: 0.0869, Validation Loss: 0.0784, Training RMSE: 0.2948, Validation RMSE: 0.2801\n",
      "Epoch [719001/10000000], Training Loss: 0.0869, Validation Loss: 0.0784, Training RMSE: 0.2947, Validation RMSE: 0.2800\n",
      "Epoch [720001/10000000], Training Loss: 0.0874, Validation Loss: 0.0783, Training RMSE: 0.2956, Validation RMSE: 0.2799\n",
      "Epoch [721001/10000000], Training Loss: 0.0874, Validation Loss: 0.0783, Training RMSE: 0.2957, Validation RMSE: 0.2798\n",
      "Epoch [722001/10000000], Training Loss: 0.0870, Validation Loss: 0.0782, Training RMSE: 0.2950, Validation RMSE: 0.2797\n",
      "Epoch [723001/10000000], Training Loss: 0.0873, Validation Loss: 0.0782, Training RMSE: 0.2954, Validation RMSE: 0.2796\n",
      "Epoch [724001/10000000], Training Loss: 0.0870, Validation Loss: 0.0781, Training RMSE: 0.2949, Validation RMSE: 0.2795\n",
      "Epoch [725001/10000000], Training Loss: 0.0865, Validation Loss: 0.0781, Training RMSE: 0.2942, Validation RMSE: 0.2794\n",
      "Epoch [726001/10000000], Training Loss: 0.0872, Validation Loss: 0.0780, Training RMSE: 0.2953, Validation RMSE: 0.2793\n",
      "Epoch [727001/10000000], Training Loss: 0.0862, Validation Loss: 0.0780, Training RMSE: 0.2936, Validation RMSE: 0.2792\n",
      "Epoch [728001/10000000], Training Loss: 0.0868, Validation Loss: 0.0779, Training RMSE: 0.2947, Validation RMSE: 0.2791\n",
      "Epoch [729001/10000000], Training Loss: 0.0858, Validation Loss: 0.0779, Training RMSE: 0.2930, Validation RMSE: 0.2790\n",
      "Epoch [730001/10000000], Training Loss: 0.0859, Validation Loss: 0.0778, Training RMSE: 0.2931, Validation RMSE: 0.2789\n",
      "Epoch [731001/10000000], Training Loss: 0.0876, Validation Loss: 0.0777, Training RMSE: 0.2959, Validation RMSE: 0.2788\n",
      "Epoch [732001/10000000], Training Loss: 0.0860, Validation Loss: 0.0777, Training RMSE: 0.2932, Validation RMSE: 0.2787\n",
      "Epoch [733001/10000000], Training Loss: 0.0867, Validation Loss: 0.0776, Training RMSE: 0.2944, Validation RMSE: 0.2786\n",
      "Epoch [734001/10000000], Training Loss: 0.0857, Validation Loss: 0.0776, Training RMSE: 0.2928, Validation RMSE: 0.2785\n",
      "Epoch [735001/10000000], Training Loss: 0.0860, Validation Loss: 0.0775, Training RMSE: 0.2932, Validation RMSE: 0.2784\n",
      "Epoch [736001/10000000], Training Loss: 0.0873, Validation Loss: 0.0775, Training RMSE: 0.2955, Validation RMSE: 0.2784\n",
      "Epoch [737001/10000000], Training Loss: 0.0857, Validation Loss: 0.0774, Training RMSE: 0.2928, Validation RMSE: 0.2783\n",
      "Epoch [738001/10000000], Training Loss: 0.0863, Validation Loss: 0.0774, Training RMSE: 0.2938, Validation RMSE: 0.2782\n",
      "Epoch [739001/10000000], Training Loss: 0.0861, Validation Loss: 0.0773, Training RMSE: 0.2934, Validation RMSE: 0.2781\n",
      "Epoch [740001/10000000], Training Loss: 0.0854, Validation Loss: 0.0773, Training RMSE: 0.2923, Validation RMSE: 0.2780\n",
      "Epoch [741001/10000000], Training Loss: 0.0861, Validation Loss: 0.0772, Training RMSE: 0.2935, Validation RMSE: 0.2779\n",
      "Epoch [742001/10000000], Training Loss: 0.0856, Validation Loss: 0.0772, Training RMSE: 0.2926, Validation RMSE: 0.2778\n",
      "Epoch [743001/10000000], Training Loss: 0.0867, Validation Loss: 0.0771, Training RMSE: 0.2944, Validation RMSE: 0.2777\n",
      "Epoch [744001/10000000], Training Loss: 0.0854, Validation Loss: 0.0771, Training RMSE: 0.2922, Validation RMSE: 0.2776\n",
      "Epoch [745001/10000000], Training Loss: 0.0863, Validation Loss: 0.0770, Training RMSE: 0.2938, Validation RMSE: 0.2775\n",
      "Epoch [746001/10000000], Training Loss: 0.0851, Validation Loss: 0.0770, Training RMSE: 0.2917, Validation RMSE: 0.2774\n",
      "Epoch [747001/10000000], Training Loss: 0.0854, Validation Loss: 0.0769, Training RMSE: 0.2922, Validation RMSE: 0.2773\n",
      "Epoch [748001/10000000], Training Loss: 0.0856, Validation Loss: 0.0769, Training RMSE: 0.2925, Validation RMSE: 0.2772\n",
      "Epoch [749001/10000000], Training Loss: 0.0858, Validation Loss: 0.0768, Training RMSE: 0.2929, Validation RMSE: 0.2772\n",
      "Epoch [750001/10000000], Training Loss: 0.0850, Validation Loss: 0.0768, Training RMSE: 0.2916, Validation RMSE: 0.2771\n",
      "Epoch [751001/10000000], Training Loss: 0.0854, Validation Loss: 0.0767, Training RMSE: 0.2922, Validation RMSE: 0.2770\n",
      "Epoch [752001/10000000], Training Loss: 0.0850, Validation Loss: 0.0767, Training RMSE: 0.2916, Validation RMSE: 0.2769\n",
      "Epoch [753001/10000000], Training Loss: 0.0859, Validation Loss: 0.0766, Training RMSE: 0.2931, Validation RMSE: 0.2768\n",
      "Epoch [754001/10000000], Training Loss: 0.0854, Validation Loss: 0.0766, Training RMSE: 0.2922, Validation RMSE: 0.2767\n",
      "Epoch [755001/10000000], Training Loss: 0.0852, Validation Loss: 0.0765, Training RMSE: 0.2919, Validation RMSE: 0.2766\n",
      "Epoch [756001/10000000], Training Loss: 0.0849, Validation Loss: 0.0765, Training RMSE: 0.2913, Validation RMSE: 0.2765\n",
      "Epoch [757001/10000000], Training Loss: 0.0858, Validation Loss: 0.0764, Training RMSE: 0.2930, Validation RMSE: 0.2764\n",
      "Epoch [758001/10000000], Training Loss: 0.0855, Validation Loss: 0.0764, Training RMSE: 0.2924, Validation RMSE: 0.2763\n",
      "Epoch [759001/10000000], Training Loss: 0.0846, Validation Loss: 0.0763, Training RMSE: 0.2909, Validation RMSE: 0.2762\n",
      "Epoch [760001/10000000], Training Loss: 0.0848, Validation Loss: 0.0763, Training RMSE: 0.2913, Validation RMSE: 0.2762\n",
      "Epoch [761001/10000000], Training Loss: 0.0854, Validation Loss: 0.0762, Training RMSE: 0.2923, Validation RMSE: 0.2761\n",
      "Epoch [762001/10000000], Training Loss: 0.0855, Validation Loss: 0.0762, Training RMSE: 0.2924, Validation RMSE: 0.2760\n",
      "Epoch [763001/10000000], Training Loss: 0.0852, Validation Loss: 0.0761, Training RMSE: 0.2920, Validation RMSE: 0.2759\n",
      "Epoch [764001/10000000], Training Loss: 0.0855, Validation Loss: 0.0761, Training RMSE: 0.2923, Validation RMSE: 0.2758\n",
      "Epoch [765001/10000000], Training Loss: 0.0846, Validation Loss: 0.0760, Training RMSE: 0.2908, Validation RMSE: 0.2757\n",
      "Epoch [766001/10000000], Training Loss: 0.0848, Validation Loss: 0.0760, Training RMSE: 0.2912, Validation RMSE: 0.2756\n",
      "Epoch [767001/10000000], Training Loss: 0.0844, Validation Loss: 0.0759, Training RMSE: 0.2905, Validation RMSE: 0.2755\n",
      "Epoch [768001/10000000], Training Loss: 0.0845, Validation Loss: 0.0759, Training RMSE: 0.2906, Validation RMSE: 0.2754\n",
      "Epoch [769001/10000000], Training Loss: 0.0848, Validation Loss: 0.0758, Training RMSE: 0.2912, Validation RMSE: 0.2754\n",
      "Epoch [770001/10000000], Training Loss: 0.0850, Validation Loss: 0.0758, Training RMSE: 0.2915, Validation RMSE: 0.2753\n",
      "Epoch [771001/10000000], Training Loss: 0.0836, Validation Loss: 0.0757, Training RMSE: 0.2891, Validation RMSE: 0.2752\n",
      "Epoch [772001/10000000], Training Loss: 0.0838, Validation Loss: 0.0757, Training RMSE: 0.2894, Validation RMSE: 0.2751\n",
      "Epoch [773001/10000000], Training Loss: 0.0841, Validation Loss: 0.0756, Training RMSE: 0.2900, Validation RMSE: 0.2750\n",
      "Epoch [774001/10000000], Training Loss: 0.0848, Validation Loss: 0.0756, Training RMSE: 0.2912, Validation RMSE: 0.2749\n",
      "Epoch [775001/10000000], Training Loss: 0.0842, Validation Loss: 0.0755, Training RMSE: 0.2902, Validation RMSE: 0.2748\n",
      "Epoch [776001/10000000], Training Loss: 0.0846, Validation Loss: 0.0755, Training RMSE: 0.2908, Validation RMSE: 0.2747\n",
      "Epoch [777001/10000000], Training Loss: 0.0853, Validation Loss: 0.0754, Training RMSE: 0.2920, Validation RMSE: 0.2747\n",
      "Epoch [778001/10000000], Training Loss: 0.0842, Validation Loss: 0.0754, Training RMSE: 0.2901, Validation RMSE: 0.2746\n",
      "Epoch [779001/10000000], Training Loss: 0.0842, Validation Loss: 0.0753, Training RMSE: 0.2901, Validation RMSE: 0.2745\n",
      "Epoch [780001/10000000], Training Loss: 0.0843, Validation Loss: 0.0753, Training RMSE: 0.2904, Validation RMSE: 0.2744\n",
      "Epoch [781001/10000000], Training Loss: 0.0839, Validation Loss: 0.0752, Training RMSE: 0.2896, Validation RMSE: 0.2743\n",
      "Epoch [782001/10000000], Training Loss: 0.0833, Validation Loss: 0.0752, Training RMSE: 0.2886, Validation RMSE: 0.2742\n",
      "Epoch [783001/10000000], Training Loss: 0.0838, Validation Loss: 0.0751, Training RMSE: 0.2894, Validation RMSE: 0.2741\n",
      "Epoch [784001/10000000], Training Loss: 0.0846, Validation Loss: 0.0751, Training RMSE: 0.2908, Validation RMSE: 0.2740\n",
      "Epoch [785001/10000000], Training Loss: 0.0832, Validation Loss: 0.0751, Training RMSE: 0.2884, Validation RMSE: 0.2740\n",
      "Epoch [786001/10000000], Training Loss: 0.0838, Validation Loss: 0.0750, Training RMSE: 0.2894, Validation RMSE: 0.2739\n",
      "Epoch [787001/10000000], Training Loss: 0.0836, Validation Loss: 0.0750, Training RMSE: 0.2892, Validation RMSE: 0.2738\n",
      "Epoch [788001/10000000], Training Loss: 0.0839, Validation Loss: 0.0749, Training RMSE: 0.2897, Validation RMSE: 0.2737\n",
      "Epoch [789001/10000000], Training Loss: 0.0838, Validation Loss: 0.0749, Training RMSE: 0.2895, Validation RMSE: 0.2736\n",
      "Epoch [790001/10000000], Training Loss: 0.0837, Validation Loss: 0.0748, Training RMSE: 0.2893, Validation RMSE: 0.2735\n",
      "Epoch [791001/10000000], Training Loss: 0.0839, Validation Loss: 0.0748, Training RMSE: 0.2896, Validation RMSE: 0.2735\n",
      "Epoch [792001/10000000], Training Loss: 0.0832, Validation Loss: 0.0747, Training RMSE: 0.2884, Validation RMSE: 0.2734\n",
      "Epoch [793001/10000000], Training Loss: 0.0832, Validation Loss: 0.0747, Training RMSE: 0.2885, Validation RMSE: 0.2733\n",
      "Epoch [794001/10000000], Training Loss: 0.0827, Validation Loss: 0.0746, Training RMSE: 0.2875, Validation RMSE: 0.2732\n",
      "Epoch [795001/10000000], Training Loss: 0.0833, Validation Loss: 0.0746, Training RMSE: 0.2885, Validation RMSE: 0.2731\n",
      "Epoch [796001/10000000], Training Loss: 0.0832, Validation Loss: 0.0745, Training RMSE: 0.2884, Validation RMSE: 0.2730\n",
      "Epoch [797001/10000000], Training Loss: 0.0829, Validation Loss: 0.0745, Training RMSE: 0.2878, Validation RMSE: 0.2729\n",
      "Epoch [798001/10000000], Training Loss: 0.0840, Validation Loss: 0.0745, Training RMSE: 0.2899, Validation RMSE: 0.2729\n",
      "Epoch [799001/10000000], Training Loss: 0.0836, Validation Loss: 0.0744, Training RMSE: 0.2892, Validation RMSE: 0.2728\n",
      "Epoch [800001/10000000], Training Loss: 0.0823, Validation Loss: 0.0744, Training RMSE: 0.2869, Validation RMSE: 0.2727\n",
      "Epoch [801001/10000000], Training Loss: 0.0830, Validation Loss: 0.0743, Training RMSE: 0.2880, Validation RMSE: 0.2726\n",
      "Epoch [802001/10000000], Training Loss: 0.0825, Validation Loss: 0.0743, Training RMSE: 0.2872, Validation RMSE: 0.2725\n",
      "Epoch [803001/10000000], Training Loss: 0.0834, Validation Loss: 0.0742, Training RMSE: 0.2887, Validation RMSE: 0.2725\n",
      "Epoch [804001/10000000], Training Loss: 0.0821, Validation Loss: 0.0742, Training RMSE: 0.2865, Validation RMSE: 0.2724\n",
      "Epoch [805001/10000000], Training Loss: 0.0831, Validation Loss: 0.0741, Training RMSE: 0.2882, Validation RMSE: 0.2723\n",
      "Epoch [806001/10000000], Training Loss: 0.0823, Validation Loss: 0.0741, Training RMSE: 0.2869, Validation RMSE: 0.2722\n",
      "Epoch [807001/10000000], Training Loss: 0.0830, Validation Loss: 0.0741, Training RMSE: 0.2881, Validation RMSE: 0.2721\n",
      "Epoch [808001/10000000], Training Loss: 0.0823, Validation Loss: 0.0740, Training RMSE: 0.2869, Validation RMSE: 0.2720\n",
      "Epoch [809001/10000000], Training Loss: 0.0837, Validation Loss: 0.0740, Training RMSE: 0.2893, Validation RMSE: 0.2720\n",
      "Epoch [810001/10000000], Training Loss: 0.0823, Validation Loss: 0.0739, Training RMSE: 0.2869, Validation RMSE: 0.2719\n",
      "Epoch [811001/10000000], Training Loss: 0.0835, Validation Loss: 0.0739, Training RMSE: 0.2889, Validation RMSE: 0.2718\n",
      "Epoch [812001/10000000], Training Loss: 0.0839, Validation Loss: 0.0738, Training RMSE: 0.2896, Validation RMSE: 0.2717\n",
      "Epoch [813001/10000000], Training Loss: 0.0820, Validation Loss: 0.0738, Training RMSE: 0.2864, Validation RMSE: 0.2716\n",
      "Epoch [814001/10000000], Training Loss: 0.0820, Validation Loss: 0.0737, Training RMSE: 0.2863, Validation RMSE: 0.2716\n",
      "Epoch [815001/10000000], Training Loss: 0.0827, Validation Loss: 0.0737, Training RMSE: 0.2875, Validation RMSE: 0.2715\n",
      "Epoch [816001/10000000], Training Loss: 0.0827, Validation Loss: 0.0737, Training RMSE: 0.2876, Validation RMSE: 0.2714\n",
      "Epoch [817001/10000000], Training Loss: 0.0827, Validation Loss: 0.0736, Training RMSE: 0.2876, Validation RMSE: 0.2713\n",
      "Epoch [818001/10000000], Training Loss: 0.0823, Validation Loss: 0.0736, Training RMSE: 0.2869, Validation RMSE: 0.2712\n",
      "Epoch [819001/10000000], Training Loss: 0.0818, Validation Loss: 0.0735, Training RMSE: 0.2860, Validation RMSE: 0.2712\n",
      "Epoch [820001/10000000], Training Loss: 0.0822, Validation Loss: 0.0735, Training RMSE: 0.2867, Validation RMSE: 0.2711\n",
      "Epoch [821001/10000000], Training Loss: 0.0825, Validation Loss: 0.0734, Training RMSE: 0.2873, Validation RMSE: 0.2710\n",
      "Epoch [822001/10000000], Training Loss: 0.0819, Validation Loss: 0.0734, Training RMSE: 0.2861, Validation RMSE: 0.2709\n",
      "Epoch [823001/10000000], Training Loss: 0.0822, Validation Loss: 0.0734, Training RMSE: 0.2868, Validation RMSE: 0.2708\n",
      "Epoch [824001/10000000], Training Loss: 0.0830, Validation Loss: 0.0733, Training RMSE: 0.2881, Validation RMSE: 0.2708\n",
      "Epoch [825001/10000000], Training Loss: 0.0823, Validation Loss: 0.0733, Training RMSE: 0.2869, Validation RMSE: 0.2707\n",
      "Epoch [826001/10000000], Training Loss: 0.0814, Validation Loss: 0.0732, Training RMSE: 0.2852, Validation RMSE: 0.2706\n",
      "Epoch [827001/10000000], Training Loss: 0.0825, Validation Loss: 0.0732, Training RMSE: 0.2872, Validation RMSE: 0.2705\n",
      "Epoch [828001/10000000], Training Loss: 0.0816, Validation Loss: 0.0731, Training RMSE: 0.2856, Validation RMSE: 0.2705\n",
      "Epoch [829001/10000000], Training Loss: 0.0816, Validation Loss: 0.0731, Training RMSE: 0.2856, Validation RMSE: 0.2704\n",
      "Epoch [830001/10000000], Training Loss: 0.0818, Validation Loss: 0.0731, Training RMSE: 0.2860, Validation RMSE: 0.2703\n",
      "Epoch [831001/10000000], Training Loss: 0.0822, Validation Loss: 0.0730, Training RMSE: 0.2868, Validation RMSE: 0.2702\n",
      "Epoch [832001/10000000], Training Loss: 0.0822, Validation Loss: 0.0730, Training RMSE: 0.2867, Validation RMSE: 0.2701\n",
      "Epoch [833001/10000000], Training Loss: 0.0821, Validation Loss: 0.0729, Training RMSE: 0.2865, Validation RMSE: 0.2701\n",
      "Epoch [834001/10000000], Training Loss: 0.0819, Validation Loss: 0.0729, Training RMSE: 0.2863, Validation RMSE: 0.2700\n",
      "Epoch [835001/10000000], Training Loss: 0.0823, Validation Loss: 0.0729, Training RMSE: 0.2870, Validation RMSE: 0.2699\n",
      "Epoch [836001/10000000], Training Loss: 0.0814, Validation Loss: 0.0728, Training RMSE: 0.2854, Validation RMSE: 0.2698\n",
      "Epoch [837001/10000000], Training Loss: 0.0824, Validation Loss: 0.0728, Training RMSE: 0.2871, Validation RMSE: 0.2698\n",
      "Epoch [838001/10000000], Training Loss: 0.0818, Validation Loss: 0.0727, Training RMSE: 0.2860, Validation RMSE: 0.2697\n",
      "Epoch [839001/10000000], Training Loss: 0.0813, Validation Loss: 0.0727, Training RMSE: 0.2852, Validation RMSE: 0.2696\n",
      "Epoch [840001/10000000], Training Loss: 0.0817, Validation Loss: 0.0727, Training RMSE: 0.2858, Validation RMSE: 0.2695\n",
      "Epoch [841001/10000000], Training Loss: 0.0807, Validation Loss: 0.0726, Training RMSE: 0.2841, Validation RMSE: 0.2695\n",
      "Epoch [842001/10000000], Training Loss: 0.0811, Validation Loss: 0.0726, Training RMSE: 0.2847, Validation RMSE: 0.2694\n",
      "Epoch [843001/10000000], Training Loss: 0.0807, Validation Loss: 0.0725, Training RMSE: 0.2840, Validation RMSE: 0.2693\n",
      "Epoch [844001/10000000], Training Loss: 0.0798, Validation Loss: 0.0725, Training RMSE: 0.2824, Validation RMSE: 0.2693\n",
      "Epoch [845001/10000000], Training Loss: 0.0810, Validation Loss: 0.0725, Training RMSE: 0.2847, Validation RMSE: 0.2692\n",
      "Epoch [846001/10000000], Training Loss: 0.0811, Validation Loss: 0.0724, Training RMSE: 0.2847, Validation RMSE: 0.2691\n",
      "Epoch [847001/10000000], Training Loss: 0.0808, Validation Loss: 0.0724, Training RMSE: 0.2842, Validation RMSE: 0.2690\n",
      "Epoch [848001/10000000], Training Loss: 0.0812, Validation Loss: 0.0723, Training RMSE: 0.2850, Validation RMSE: 0.2690\n",
      "Epoch [849001/10000000], Training Loss: 0.0803, Validation Loss: 0.0723, Training RMSE: 0.2834, Validation RMSE: 0.2689\n",
      "Epoch [850001/10000000], Training Loss: 0.0808, Validation Loss: 0.0723, Training RMSE: 0.2843, Validation RMSE: 0.2688\n",
      "Epoch [851001/10000000], Training Loss: 0.0803, Validation Loss: 0.0722, Training RMSE: 0.2833, Validation RMSE: 0.2687\n",
      "Epoch [852001/10000000], Training Loss: 0.0801, Validation Loss: 0.0722, Training RMSE: 0.2831, Validation RMSE: 0.2687\n",
      "Epoch [853001/10000000], Training Loss: 0.0795, Validation Loss: 0.0721, Training RMSE: 0.2820, Validation RMSE: 0.2686\n",
      "Epoch [854001/10000000], Training Loss: 0.0809, Validation Loss: 0.0721, Training RMSE: 0.2845, Validation RMSE: 0.2685\n",
      "Epoch [855001/10000000], Training Loss: 0.0808, Validation Loss: 0.0721, Training RMSE: 0.2843, Validation RMSE: 0.2685\n",
      "Epoch [856001/10000000], Training Loss: 0.0800, Validation Loss: 0.0720, Training RMSE: 0.2829, Validation RMSE: 0.2684\n",
      "Epoch [857001/10000000], Training Loss: 0.0816, Validation Loss: 0.0720, Training RMSE: 0.2856, Validation RMSE: 0.2683\n",
      "Epoch [858001/10000000], Training Loss: 0.0803, Validation Loss: 0.0720, Training RMSE: 0.2833, Validation RMSE: 0.2682\n",
      "Epoch [859001/10000000], Training Loss: 0.0799, Validation Loss: 0.0719, Training RMSE: 0.2827, Validation RMSE: 0.2682\n",
      "Epoch [860001/10000000], Training Loss: 0.0810, Validation Loss: 0.0719, Training RMSE: 0.2845, Validation RMSE: 0.2681\n",
      "Epoch [861001/10000000], Training Loss: 0.0812, Validation Loss: 0.0718, Training RMSE: 0.2850, Validation RMSE: 0.2680\n",
      "Epoch [862001/10000000], Training Loss: 0.0805, Validation Loss: 0.0718, Training RMSE: 0.2837, Validation RMSE: 0.2680\n",
      "Epoch [863001/10000000], Training Loss: 0.0807, Validation Loss: 0.0718, Training RMSE: 0.2841, Validation RMSE: 0.2679\n",
      "Epoch [864001/10000000], Training Loss: 0.0805, Validation Loss: 0.0717, Training RMSE: 0.2837, Validation RMSE: 0.2678\n",
      "Epoch [865001/10000000], Training Loss: 0.0797, Validation Loss: 0.0717, Training RMSE: 0.2823, Validation RMSE: 0.2677\n",
      "Epoch [866001/10000000], Training Loss: 0.0804, Validation Loss: 0.0717, Training RMSE: 0.2835, Validation RMSE: 0.2677\n",
      "Epoch [867001/10000000], Training Loss: 0.0799, Validation Loss: 0.0716, Training RMSE: 0.2828, Validation RMSE: 0.2676\n",
      "Epoch [868001/10000000], Training Loss: 0.0807, Validation Loss: 0.0716, Training RMSE: 0.2841, Validation RMSE: 0.2675\n",
      "Epoch [869001/10000000], Training Loss: 0.0810, Validation Loss: 0.0715, Training RMSE: 0.2846, Validation RMSE: 0.2675\n",
      "Epoch [870001/10000000], Training Loss: 0.0807, Validation Loss: 0.0715, Training RMSE: 0.2841, Validation RMSE: 0.2674\n",
      "Epoch [871001/10000000], Training Loss: 0.0798, Validation Loss: 0.0715, Training RMSE: 0.2824, Validation RMSE: 0.2673\n",
      "Epoch [872001/10000000], Training Loss: 0.0807, Validation Loss: 0.0714, Training RMSE: 0.2841, Validation RMSE: 0.2673\n",
      "Epoch [873001/10000000], Training Loss: 0.0798, Validation Loss: 0.0714, Training RMSE: 0.2825, Validation RMSE: 0.2672\n",
      "Epoch [874001/10000000], Training Loss: 0.0796, Validation Loss: 0.0714, Training RMSE: 0.2821, Validation RMSE: 0.2671\n",
      "Epoch [875001/10000000], Training Loss: 0.0806, Validation Loss: 0.0713, Training RMSE: 0.2839, Validation RMSE: 0.2671\n",
      "Epoch [876001/10000000], Training Loss: 0.0799, Validation Loss: 0.0713, Training RMSE: 0.2827, Validation RMSE: 0.2670\n",
      "Epoch [877001/10000000], Training Loss: 0.0809, Validation Loss: 0.0713, Training RMSE: 0.2844, Validation RMSE: 0.2669\n",
      "Epoch [878001/10000000], Training Loss: 0.0802, Validation Loss: 0.0712, Training RMSE: 0.2832, Validation RMSE: 0.2669\n",
      "Epoch [879001/10000000], Training Loss: 0.0794, Validation Loss: 0.0712, Training RMSE: 0.2818, Validation RMSE: 0.2668\n",
      "Epoch [880001/10000000], Training Loss: 0.0796, Validation Loss: 0.0711, Training RMSE: 0.2822, Validation RMSE: 0.2667\n",
      "Epoch [881001/10000000], Training Loss: 0.0796, Validation Loss: 0.0711, Training RMSE: 0.2822, Validation RMSE: 0.2667\n",
      "Epoch [882001/10000000], Training Loss: 0.0804, Validation Loss: 0.0711, Training RMSE: 0.2836, Validation RMSE: 0.2666\n",
      "Epoch [883001/10000000], Training Loss: 0.0798, Validation Loss: 0.0710, Training RMSE: 0.2825, Validation RMSE: 0.2665\n",
      "Epoch [884001/10000000], Training Loss: 0.0797, Validation Loss: 0.0710, Training RMSE: 0.2824, Validation RMSE: 0.2665\n",
      "Epoch [885001/10000000], Training Loss: 0.0794, Validation Loss: 0.0710, Training RMSE: 0.2818, Validation RMSE: 0.2664\n",
      "Epoch [886001/10000000], Training Loss: 0.0794, Validation Loss: 0.0709, Training RMSE: 0.2818, Validation RMSE: 0.2663\n",
      "Epoch [887001/10000000], Training Loss: 0.0792, Validation Loss: 0.0709, Training RMSE: 0.2814, Validation RMSE: 0.2663\n",
      "Epoch [888001/10000000], Training Loss: 0.0795, Validation Loss: 0.0709, Training RMSE: 0.2820, Validation RMSE: 0.2662\n",
      "Epoch [889001/10000000], Training Loss: 0.0799, Validation Loss: 0.0708, Training RMSE: 0.2827, Validation RMSE: 0.2661\n",
      "Epoch [890001/10000000], Training Loss: 0.0793, Validation Loss: 0.0708, Training RMSE: 0.2817, Validation RMSE: 0.2661\n",
      "Epoch [891001/10000000], Training Loss: 0.0799, Validation Loss: 0.0708, Training RMSE: 0.2827, Validation RMSE: 0.2660\n",
      "Epoch [892001/10000000], Training Loss: 0.0793, Validation Loss: 0.0707, Training RMSE: 0.2816, Validation RMSE: 0.2660\n",
      "Epoch [893001/10000000], Training Loss: 0.0796, Validation Loss: 0.0707, Training RMSE: 0.2822, Validation RMSE: 0.2659\n",
      "Epoch [894001/10000000], Training Loss: 0.0794, Validation Loss: 0.0707, Training RMSE: 0.2817, Validation RMSE: 0.2658\n",
      "Epoch [895001/10000000], Training Loss: 0.0794, Validation Loss: 0.0706, Training RMSE: 0.2819, Validation RMSE: 0.2658\n",
      "Epoch [896001/10000000], Training Loss: 0.0788, Validation Loss: 0.0706, Training RMSE: 0.2806, Validation RMSE: 0.2657\n",
      "Epoch [897001/10000000], Training Loss: 0.0800, Validation Loss: 0.0706, Training RMSE: 0.2828, Validation RMSE: 0.2656\n",
      "Epoch [898001/10000000], Training Loss: 0.0789, Validation Loss: 0.0705, Training RMSE: 0.2808, Validation RMSE: 0.2656\n",
      "Epoch [899001/10000000], Training Loss: 0.0791, Validation Loss: 0.0705, Training RMSE: 0.2813, Validation RMSE: 0.2655\n",
      "Epoch [900001/10000000], Training Loss: 0.0790, Validation Loss: 0.0705, Training RMSE: 0.2810, Validation RMSE: 0.2655\n",
      "Epoch [901001/10000000], Training Loss: 0.0802, Validation Loss: 0.0704, Training RMSE: 0.2832, Validation RMSE: 0.2654\n",
      "Epoch [902001/10000000], Training Loss: 0.0799, Validation Loss: 0.0704, Training RMSE: 0.2826, Validation RMSE: 0.2653\n",
      "Epoch [903001/10000000], Training Loss: 0.0794, Validation Loss: 0.0704, Training RMSE: 0.2817, Validation RMSE: 0.2653\n",
      "Epoch [904001/10000000], Training Loss: 0.0790, Validation Loss: 0.0703, Training RMSE: 0.2812, Validation RMSE: 0.2652\n",
      "Epoch [905001/10000000], Training Loss: 0.0784, Validation Loss: 0.0703, Training RMSE: 0.2800, Validation RMSE: 0.2652\n",
      "Epoch [906001/10000000], Training Loss: 0.0793, Validation Loss: 0.0703, Training RMSE: 0.2817, Validation RMSE: 0.2651\n",
      "Epoch [907001/10000000], Training Loss: 0.0791, Validation Loss: 0.0702, Training RMSE: 0.2813, Validation RMSE: 0.2650\n",
      "Epoch [908001/10000000], Training Loss: 0.0800, Validation Loss: 0.0702, Training RMSE: 0.2829, Validation RMSE: 0.2650\n",
      "Epoch [909001/10000000], Training Loss: 0.0788, Validation Loss: 0.0702, Training RMSE: 0.2806, Validation RMSE: 0.2649\n",
      "Epoch [910001/10000000], Training Loss: 0.0794, Validation Loss: 0.0701, Training RMSE: 0.2817, Validation RMSE: 0.2649\n",
      "Epoch [911001/10000000], Training Loss: 0.0792, Validation Loss: 0.0701, Training RMSE: 0.2814, Validation RMSE: 0.2648\n",
      "Epoch [912001/10000000], Training Loss: 0.0792, Validation Loss: 0.0701, Training RMSE: 0.2813, Validation RMSE: 0.2647\n",
      "Epoch [913001/10000000], Training Loss: 0.0792, Validation Loss: 0.0701, Training RMSE: 0.2814, Validation RMSE: 0.2647\n",
      "Epoch [914001/10000000], Training Loss: 0.0784, Validation Loss: 0.0700, Training RMSE: 0.2800, Validation RMSE: 0.2646\n",
      "Epoch [915001/10000000], Training Loss: 0.0786, Validation Loss: 0.0700, Training RMSE: 0.2803, Validation RMSE: 0.2646\n",
      "Epoch [916001/10000000], Training Loss: 0.0788, Validation Loss: 0.0700, Training RMSE: 0.2808, Validation RMSE: 0.2645\n",
      "Epoch [917001/10000000], Training Loss: 0.0784, Validation Loss: 0.0699, Training RMSE: 0.2800, Validation RMSE: 0.2644\n",
      "Epoch [918001/10000000], Training Loss: 0.0794, Validation Loss: 0.0699, Training RMSE: 0.2818, Validation RMSE: 0.2644\n",
      "Epoch [919001/10000000], Training Loss: 0.0790, Validation Loss: 0.0699, Training RMSE: 0.2810, Validation RMSE: 0.2643\n",
      "Epoch [920001/10000000], Training Loss: 0.0782, Validation Loss: 0.0698, Training RMSE: 0.2797, Validation RMSE: 0.2643\n",
      "Epoch [921001/10000000], Training Loss: 0.0788, Validation Loss: 0.0698, Training RMSE: 0.2807, Validation RMSE: 0.2642\n",
      "Epoch [922001/10000000], Training Loss: 0.0795, Validation Loss: 0.0698, Training RMSE: 0.2819, Validation RMSE: 0.2642\n",
      "Epoch [923001/10000000], Training Loss: 0.0786, Validation Loss: 0.0697, Training RMSE: 0.2804, Validation RMSE: 0.2641\n",
      "Epoch [924001/10000000], Training Loss: 0.0791, Validation Loss: 0.0697, Training RMSE: 0.2812, Validation RMSE: 0.2640\n",
      "Epoch [925001/10000000], Training Loss: 0.0787, Validation Loss: 0.0697, Training RMSE: 0.2805, Validation RMSE: 0.2640\n",
      "Epoch [926001/10000000], Training Loss: 0.0783, Validation Loss: 0.0697, Training RMSE: 0.2798, Validation RMSE: 0.2639\n",
      "Epoch [927001/10000000], Training Loss: 0.0779, Validation Loss: 0.0696, Training RMSE: 0.2791, Validation RMSE: 0.2639\n",
      "Epoch [928001/10000000], Training Loss: 0.0787, Validation Loss: 0.0696, Training RMSE: 0.2806, Validation RMSE: 0.2638\n",
      "Epoch [929001/10000000], Training Loss: 0.0789, Validation Loss: 0.0696, Training RMSE: 0.2810, Validation RMSE: 0.2638\n",
      "Epoch [930001/10000000], Training Loss: 0.0785, Validation Loss: 0.0695, Training RMSE: 0.2802, Validation RMSE: 0.2637\n",
      "Epoch [931001/10000000], Training Loss: 0.0786, Validation Loss: 0.0695, Training RMSE: 0.2804, Validation RMSE: 0.2636\n",
      "Epoch [932001/10000000], Training Loss: 0.0784, Validation Loss: 0.0695, Training RMSE: 0.2799, Validation RMSE: 0.2636\n",
      "Epoch [933001/10000000], Training Loss: 0.0779, Validation Loss: 0.0695, Training RMSE: 0.2791, Validation RMSE: 0.2635\n",
      "Epoch [934001/10000000], Training Loss: 0.0777, Validation Loss: 0.0694, Training RMSE: 0.2787, Validation RMSE: 0.2635\n",
      "Epoch [935001/10000000], Training Loss: 0.0782, Validation Loss: 0.0694, Training RMSE: 0.2797, Validation RMSE: 0.2634\n",
      "Epoch [936001/10000000], Training Loss: 0.0780, Validation Loss: 0.0694, Training RMSE: 0.2793, Validation RMSE: 0.2634\n",
      "Epoch [937001/10000000], Training Loss: 0.0779, Validation Loss: 0.0693, Training RMSE: 0.2792, Validation RMSE: 0.2633\n",
      "Epoch [938001/10000000], Training Loss: 0.0783, Validation Loss: 0.0693, Training RMSE: 0.2798, Validation RMSE: 0.2633\n",
      "Epoch [939001/10000000], Training Loss: 0.0788, Validation Loss: 0.0693, Training RMSE: 0.2807, Validation RMSE: 0.2632\n",
      "Epoch [940001/10000000], Training Loss: 0.0782, Validation Loss: 0.0693, Training RMSE: 0.2796, Validation RMSE: 0.2632\n",
      "Epoch [941001/10000000], Training Loss: 0.0770, Validation Loss: 0.0692, Training RMSE: 0.2775, Validation RMSE: 0.2631\n",
      "Epoch [942001/10000000], Training Loss: 0.0775, Validation Loss: 0.0692, Training RMSE: 0.2783, Validation RMSE: 0.2630\n",
      "Epoch [943001/10000000], Training Loss: 0.0782, Validation Loss: 0.0692, Training RMSE: 0.2797, Validation RMSE: 0.2630\n",
      "Epoch [944001/10000000], Training Loss: 0.0786, Validation Loss: 0.0691, Training RMSE: 0.2804, Validation RMSE: 0.2629\n",
      "Epoch [945001/10000000], Training Loss: 0.0780, Validation Loss: 0.0691, Training RMSE: 0.2792, Validation RMSE: 0.2629\n",
      "Epoch [946001/10000000], Training Loss: 0.0783, Validation Loss: 0.0691, Training RMSE: 0.2798, Validation RMSE: 0.2628\n",
      "Epoch [947001/10000000], Training Loss: 0.0785, Validation Loss: 0.0691, Training RMSE: 0.2802, Validation RMSE: 0.2628\n",
      "Epoch [948001/10000000], Training Loss: 0.0772, Validation Loss: 0.0690, Training RMSE: 0.2779, Validation RMSE: 0.2627\n",
      "Epoch [949001/10000000], Training Loss: 0.0781, Validation Loss: 0.0690, Training RMSE: 0.2795, Validation RMSE: 0.2627\n",
      "Epoch [950001/10000000], Training Loss: 0.0779, Validation Loss: 0.0690, Training RMSE: 0.2792, Validation RMSE: 0.2626\n",
      "Epoch [951001/10000000], Training Loss: 0.0772, Validation Loss: 0.0689, Training RMSE: 0.2779, Validation RMSE: 0.2626\n",
      "Epoch [952001/10000000], Training Loss: 0.0774, Validation Loss: 0.0689, Training RMSE: 0.2782, Validation RMSE: 0.2625\n",
      "Epoch [953001/10000000], Training Loss: 0.0770, Validation Loss: 0.0689, Training RMSE: 0.2776, Validation RMSE: 0.2625\n",
      "Epoch [954001/10000000], Training Loss: 0.0776, Validation Loss: 0.0689, Training RMSE: 0.2786, Validation RMSE: 0.2624\n",
      "Epoch [955001/10000000], Training Loss: 0.0774, Validation Loss: 0.0688, Training RMSE: 0.2783, Validation RMSE: 0.2624\n",
      "Epoch [956001/10000000], Training Loss: 0.0772, Validation Loss: 0.0688, Training RMSE: 0.2779, Validation RMSE: 0.2623\n",
      "Epoch [957001/10000000], Training Loss: 0.0779, Validation Loss: 0.0688, Training RMSE: 0.2791, Validation RMSE: 0.2623\n",
      "Epoch [958001/10000000], Training Loss: 0.0778, Validation Loss: 0.0688, Training RMSE: 0.2789, Validation RMSE: 0.2622\n",
      "Epoch [959001/10000000], Training Loss: 0.0773, Validation Loss: 0.0687, Training RMSE: 0.2779, Validation RMSE: 0.2622\n",
      "Epoch [960001/10000000], Training Loss: 0.0776, Validation Loss: 0.0687, Training RMSE: 0.2785, Validation RMSE: 0.2621\n",
      "Epoch [961001/10000000], Training Loss: 0.0773, Validation Loss: 0.0687, Training RMSE: 0.2781, Validation RMSE: 0.2621\n",
      "Epoch [962001/10000000], Training Loss: 0.0778, Validation Loss: 0.0687, Training RMSE: 0.2790, Validation RMSE: 0.2620\n",
      "Epoch [963001/10000000], Training Loss: 0.0767, Validation Loss: 0.0686, Training RMSE: 0.2769, Validation RMSE: 0.2620\n",
      "Epoch [964001/10000000], Training Loss: 0.0775, Validation Loss: 0.0686, Training RMSE: 0.2784, Validation RMSE: 0.2619\n",
      "Epoch [965001/10000000], Training Loss: 0.0764, Validation Loss: 0.0686, Training RMSE: 0.2764, Validation RMSE: 0.2619\n",
      "Epoch [966001/10000000], Training Loss: 0.0775, Validation Loss: 0.0685, Training RMSE: 0.2784, Validation RMSE: 0.2618\n",
      "Epoch [967001/10000000], Training Loss: 0.0764, Validation Loss: 0.0685, Training RMSE: 0.2763, Validation RMSE: 0.2618\n",
      "Epoch [968001/10000000], Training Loss: 0.0767, Validation Loss: 0.0685, Training RMSE: 0.2769, Validation RMSE: 0.2617\n",
      "Epoch [969001/10000000], Training Loss: 0.0773, Validation Loss: 0.0685, Training RMSE: 0.2779, Validation RMSE: 0.2617\n",
      "Epoch [970001/10000000], Training Loss: 0.0766, Validation Loss: 0.0684, Training RMSE: 0.2768, Validation RMSE: 0.2616\n",
      "Epoch [971001/10000000], Training Loss: 0.0766, Validation Loss: 0.0684, Training RMSE: 0.2768, Validation RMSE: 0.2616\n",
      "Epoch [972001/10000000], Training Loss: 0.0765, Validation Loss: 0.0684, Training RMSE: 0.2765, Validation RMSE: 0.2615\n",
      "Epoch [973001/10000000], Training Loss: 0.0771, Validation Loss: 0.0684, Training RMSE: 0.2777, Validation RMSE: 0.2615\n",
      "Epoch [974001/10000000], Training Loss: 0.0766, Validation Loss: 0.0683, Training RMSE: 0.2768, Validation RMSE: 0.2614\n",
      "Epoch [975001/10000000], Training Loss: 0.0771, Validation Loss: 0.0683, Training RMSE: 0.2777, Validation RMSE: 0.2614\n",
      "Epoch [976001/10000000], Training Loss: 0.0764, Validation Loss: 0.0683, Training RMSE: 0.2764, Validation RMSE: 0.2613\n",
      "Epoch [977001/10000000], Training Loss: 0.0768, Validation Loss: 0.0683, Training RMSE: 0.2771, Validation RMSE: 0.2613\n",
      "Epoch [978001/10000000], Training Loss: 0.0771, Validation Loss: 0.0682, Training RMSE: 0.2776, Validation RMSE: 0.2612\n",
      "Epoch [979001/10000000], Training Loss: 0.0767, Validation Loss: 0.0682, Training RMSE: 0.2770, Validation RMSE: 0.2612\n",
      "Epoch [980001/10000000], Training Loss: 0.0771, Validation Loss: 0.0682, Training RMSE: 0.2776, Validation RMSE: 0.2611\n",
      "Epoch [981001/10000000], Training Loss: 0.0770, Validation Loss: 0.0682, Training RMSE: 0.2775, Validation RMSE: 0.2611\n",
      "Epoch [982001/10000000], Training Loss: 0.0765, Validation Loss: 0.0681, Training RMSE: 0.2766, Validation RMSE: 0.2610\n",
      "Epoch [983001/10000000], Training Loss: 0.0773, Validation Loss: 0.0681, Training RMSE: 0.2779, Validation RMSE: 0.2610\n",
      "Epoch [984001/10000000], Training Loss: 0.0763, Validation Loss: 0.0681, Training RMSE: 0.2762, Validation RMSE: 0.2609\n",
      "Epoch [985001/10000000], Training Loss: 0.0767, Validation Loss: 0.0681, Training RMSE: 0.2770, Validation RMSE: 0.2609\n",
      "Epoch [986001/10000000], Training Loss: 0.0765, Validation Loss: 0.0680, Training RMSE: 0.2767, Validation RMSE: 0.2608\n",
      "Epoch [987001/10000000], Training Loss: 0.0766, Validation Loss: 0.0680, Training RMSE: 0.2768, Validation RMSE: 0.2608\n",
      "Epoch [988001/10000000], Training Loss: 0.0768, Validation Loss: 0.0680, Training RMSE: 0.2771, Validation RMSE: 0.2608\n",
      "Epoch [989001/10000000], Training Loss: 0.0771, Validation Loss: 0.0680, Training RMSE: 0.2777, Validation RMSE: 0.2607\n",
      "Epoch [990001/10000000], Training Loss: 0.0777, Validation Loss: 0.0679, Training RMSE: 0.2787, Validation RMSE: 0.2607\n",
      "Epoch [991001/10000000], Training Loss: 0.0759, Validation Loss: 0.0679, Training RMSE: 0.2755, Validation RMSE: 0.2606\n",
      "Epoch [992001/10000000], Training Loss: 0.0770, Validation Loss: 0.0679, Training RMSE: 0.2774, Validation RMSE: 0.2606\n",
      "Epoch [993001/10000000], Training Loss: 0.0764, Validation Loss: 0.0679, Training RMSE: 0.2764, Validation RMSE: 0.2605\n",
      "Epoch [994001/10000000], Training Loss: 0.0767, Validation Loss: 0.0678, Training RMSE: 0.2769, Validation RMSE: 0.2605\n",
      "Epoch [995001/10000000], Training Loss: 0.0766, Validation Loss: 0.0678, Training RMSE: 0.2767, Validation RMSE: 0.2604\n",
      "Epoch [996001/10000000], Training Loss: 0.0762, Validation Loss: 0.0678, Training RMSE: 0.2761, Validation RMSE: 0.2604\n",
      "Epoch [997001/10000000], Training Loss: 0.0761, Validation Loss: 0.0678, Training RMSE: 0.2759, Validation RMSE: 0.2603\n",
      "Epoch [998001/10000000], Training Loss: 0.0759, Validation Loss: 0.0678, Training RMSE: 0.2755, Validation RMSE: 0.2603\n",
      "Epoch [999001/10000000], Training Loss: 0.0769, Validation Loss: 0.0677, Training RMSE: 0.2773, Validation RMSE: 0.2602\n",
      "Epoch [1000001/10000000], Training Loss: 0.0770, Validation Loss: 0.0677, Training RMSE: 0.2775, Validation RMSE: 0.2602\n",
      "Epoch [1001001/10000000], Training Loss: 0.0774, Validation Loss: 0.0677, Training RMSE: 0.2782, Validation RMSE: 0.2602\n",
      "Epoch [1002001/10000000], Training Loss: 0.0765, Validation Loss: 0.0677, Training RMSE: 0.2767, Validation RMSE: 0.2601\n",
      "Epoch [1003001/10000000], Training Loss: 0.0769, Validation Loss: 0.0676, Training RMSE: 0.2774, Validation RMSE: 0.2601\n",
      "Epoch [1004001/10000000], Training Loss: 0.0760, Validation Loss: 0.0676, Training RMSE: 0.2756, Validation RMSE: 0.2600\n",
      "Epoch [1005001/10000000], Training Loss: 0.0760, Validation Loss: 0.0676, Training RMSE: 0.2757, Validation RMSE: 0.2600\n",
      "Epoch [1006001/10000000], Training Loss: 0.0756, Validation Loss: 0.0676, Training RMSE: 0.2749, Validation RMSE: 0.2599\n",
      "Epoch [1007001/10000000], Training Loss: 0.0760, Validation Loss: 0.0675, Training RMSE: 0.2757, Validation RMSE: 0.2599\n",
      "Epoch [1008001/10000000], Training Loss: 0.0762, Validation Loss: 0.0675, Training RMSE: 0.2760, Validation RMSE: 0.2598\n",
      "Epoch [1009001/10000000], Training Loss: 0.0761, Validation Loss: 0.0675, Training RMSE: 0.2759, Validation RMSE: 0.2598\n",
      "Epoch [1010001/10000000], Training Loss: 0.0759, Validation Loss: 0.0675, Training RMSE: 0.2756, Validation RMSE: 0.2598\n",
      "Epoch [1011001/10000000], Training Loss: 0.0764, Validation Loss: 0.0674, Training RMSE: 0.2765, Validation RMSE: 0.2597\n",
      "Epoch [1012001/10000000], Training Loss: 0.0757, Validation Loss: 0.0674, Training RMSE: 0.2752, Validation RMSE: 0.2597\n",
      "Epoch [1013001/10000000], Training Loss: 0.0763, Validation Loss: 0.0674, Training RMSE: 0.2763, Validation RMSE: 0.2596\n",
      "Epoch [1014001/10000000], Training Loss: 0.0768, Validation Loss: 0.0674, Training RMSE: 0.2772, Validation RMSE: 0.2596\n",
      "Epoch [1015001/10000000], Training Loss: 0.0755, Validation Loss: 0.0674, Training RMSE: 0.2748, Validation RMSE: 0.2595\n",
      "Epoch [1016001/10000000], Training Loss: 0.0760, Validation Loss: 0.0673, Training RMSE: 0.2757, Validation RMSE: 0.2595\n",
      "Epoch [1017001/10000000], Training Loss: 0.0768, Validation Loss: 0.0673, Training RMSE: 0.2771, Validation RMSE: 0.2594\n",
      "Epoch [1018001/10000000], Training Loss: 0.0754, Validation Loss: 0.0673, Training RMSE: 0.2747, Validation RMSE: 0.2594\n",
      "Epoch [1019001/10000000], Training Loss: 0.0752, Validation Loss: 0.0673, Training RMSE: 0.2742, Validation RMSE: 0.2594\n",
      "Epoch [1020001/10000000], Training Loss: 0.0759, Validation Loss: 0.0672, Training RMSE: 0.2754, Validation RMSE: 0.2593\n",
      "Epoch [1021001/10000000], Training Loss: 0.0761, Validation Loss: 0.0672, Training RMSE: 0.2759, Validation RMSE: 0.2593\n",
      "Epoch [1022001/10000000], Training Loss: 0.0765, Validation Loss: 0.0672, Training RMSE: 0.2765, Validation RMSE: 0.2592\n",
      "Epoch [1023001/10000000], Training Loss: 0.0764, Validation Loss: 0.0672, Training RMSE: 0.2765, Validation RMSE: 0.2592\n",
      "Epoch [1024001/10000000], Training Loss: 0.0761, Validation Loss: 0.0672, Training RMSE: 0.2758, Validation RMSE: 0.2591\n",
      "Epoch [1025001/10000000], Training Loss: 0.0759, Validation Loss: 0.0671, Training RMSE: 0.2755, Validation RMSE: 0.2591\n",
      "Epoch [1026001/10000000], Training Loss: 0.0760, Validation Loss: 0.0671, Training RMSE: 0.2757, Validation RMSE: 0.2591\n",
      "Epoch [1027001/10000000], Training Loss: 0.0758, Validation Loss: 0.0671, Training RMSE: 0.2753, Validation RMSE: 0.2590\n",
      "Epoch [1028001/10000000], Training Loss: 0.0757, Validation Loss: 0.0671, Training RMSE: 0.2751, Validation RMSE: 0.2590\n",
      "Epoch [1029001/10000000], Training Loss: 0.0750, Validation Loss: 0.0670, Training RMSE: 0.2739, Validation RMSE: 0.2589\n",
      "Epoch [1030001/10000000], Training Loss: 0.0753, Validation Loss: 0.0670, Training RMSE: 0.2744, Validation RMSE: 0.2589\n",
      "Epoch [1031001/10000000], Training Loss: 0.0753, Validation Loss: 0.0670, Training RMSE: 0.2743, Validation RMSE: 0.2588\n",
      "Epoch [1032001/10000000], Training Loss: 0.0760, Validation Loss: 0.0670, Training RMSE: 0.2756, Validation RMSE: 0.2588\n",
      "Epoch [1033001/10000000], Training Loss: 0.0752, Validation Loss: 0.0670, Training RMSE: 0.2743, Validation RMSE: 0.2588\n",
      "Epoch [1034001/10000000], Training Loss: 0.0760, Validation Loss: 0.0669, Training RMSE: 0.2757, Validation RMSE: 0.2587\n",
      "Epoch [1035001/10000000], Training Loss: 0.0750, Validation Loss: 0.0669, Training RMSE: 0.2738, Validation RMSE: 0.2587\n",
      "Epoch [1036001/10000000], Training Loss: 0.0755, Validation Loss: 0.0669, Training RMSE: 0.2748, Validation RMSE: 0.2586\n",
      "Epoch [1037001/10000000], Training Loss: 0.0753, Validation Loss: 0.0669, Training RMSE: 0.2744, Validation RMSE: 0.2586\n",
      "Epoch [1038001/10000000], Training Loss: 0.0759, Validation Loss: 0.0668, Training RMSE: 0.2755, Validation RMSE: 0.2586\n",
      "Epoch [1039001/10000000], Training Loss: 0.0750, Validation Loss: 0.0668, Training RMSE: 0.2738, Validation RMSE: 0.2585\n",
      "Epoch [1040001/10000000], Training Loss: 0.0746, Validation Loss: 0.0668, Training RMSE: 0.2732, Validation RMSE: 0.2585\n",
      "Epoch [1041001/10000000], Training Loss: 0.0752, Validation Loss: 0.0668, Training RMSE: 0.2742, Validation RMSE: 0.2584\n",
      "Epoch [1042001/10000000], Training Loss: 0.0754, Validation Loss: 0.0668, Training RMSE: 0.2745, Validation RMSE: 0.2584\n",
      "Epoch [1043001/10000000], Training Loss: 0.0751, Validation Loss: 0.0667, Training RMSE: 0.2741, Validation RMSE: 0.2583\n",
      "Epoch [1044001/10000000], Training Loss: 0.0760, Validation Loss: 0.0667, Training RMSE: 0.2757, Validation RMSE: 0.2583\n",
      "Epoch [1045001/10000000], Training Loss: 0.0747, Validation Loss: 0.0667, Training RMSE: 0.2733, Validation RMSE: 0.2583\n",
      "Epoch [1046001/10000000], Training Loss: 0.0759, Validation Loss: 0.0667, Training RMSE: 0.2755, Validation RMSE: 0.2582\n",
      "Epoch [1047001/10000000], Training Loss: 0.0750, Validation Loss: 0.0667, Training RMSE: 0.2739, Validation RMSE: 0.2582\n",
      "Epoch [1048001/10000000], Training Loss: 0.0755, Validation Loss: 0.0666, Training RMSE: 0.2747, Validation RMSE: 0.2581\n",
      "Epoch [1049001/10000000], Training Loss: 0.0757, Validation Loss: 0.0666, Training RMSE: 0.2751, Validation RMSE: 0.2581\n",
      "Epoch [1050001/10000000], Training Loss: 0.0756, Validation Loss: 0.0666, Training RMSE: 0.2749, Validation RMSE: 0.2581\n",
      "Epoch [1051001/10000000], Training Loss: 0.0760, Validation Loss: 0.0666, Training RMSE: 0.2758, Validation RMSE: 0.2580\n",
      "Epoch [1052001/10000000], Training Loss: 0.0753, Validation Loss: 0.0666, Training RMSE: 0.2745, Validation RMSE: 0.2580\n",
      "Epoch [1053001/10000000], Training Loss: 0.0742, Validation Loss: 0.0665, Training RMSE: 0.2725, Validation RMSE: 0.2579\n",
      "Epoch [1054001/10000000], Training Loss: 0.0755, Validation Loss: 0.0665, Training RMSE: 0.2748, Validation RMSE: 0.2579\n",
      "Epoch [1055001/10000000], Training Loss: 0.0746, Validation Loss: 0.0665, Training RMSE: 0.2731, Validation RMSE: 0.2579\n",
      "Epoch [1056001/10000000], Training Loss: 0.0749, Validation Loss: 0.0665, Training RMSE: 0.2737, Validation RMSE: 0.2578\n",
      "Epoch [1057001/10000000], Training Loss: 0.0749, Validation Loss: 0.0664, Training RMSE: 0.2737, Validation RMSE: 0.2578\n",
      "Epoch [1058001/10000000], Training Loss: 0.0746, Validation Loss: 0.0664, Training RMSE: 0.2731, Validation RMSE: 0.2577\n",
      "Epoch [1059001/10000000], Training Loss: 0.0743, Validation Loss: 0.0664, Training RMSE: 0.2726, Validation RMSE: 0.2577\n",
      "Epoch [1060001/10000000], Training Loss: 0.0752, Validation Loss: 0.0664, Training RMSE: 0.2742, Validation RMSE: 0.2577\n",
      "Epoch [1061001/10000000], Training Loss: 0.0749, Validation Loss: 0.0664, Training RMSE: 0.2736, Validation RMSE: 0.2576\n",
      "Epoch [1062001/10000000], Training Loss: 0.0747, Validation Loss: 0.0663, Training RMSE: 0.2734, Validation RMSE: 0.2576\n",
      "Epoch [1063001/10000000], Training Loss: 0.0748, Validation Loss: 0.0663, Training RMSE: 0.2735, Validation RMSE: 0.2575\n",
      "Epoch [1064001/10000000], Training Loss: 0.0745, Validation Loss: 0.0663, Training RMSE: 0.2730, Validation RMSE: 0.2575\n",
      "Epoch [1065001/10000000], Training Loss: 0.0748, Validation Loss: 0.0663, Training RMSE: 0.2736, Validation RMSE: 0.2575\n",
      "Epoch [1066001/10000000], Training Loss: 0.0740, Validation Loss: 0.0663, Training RMSE: 0.2720, Validation RMSE: 0.2574\n",
      "Epoch [1067001/10000000], Training Loss: 0.0745, Validation Loss: 0.0662, Training RMSE: 0.2730, Validation RMSE: 0.2574\n",
      "Epoch [1068001/10000000], Training Loss: 0.0744, Validation Loss: 0.0662, Training RMSE: 0.2729, Validation RMSE: 0.2573\n",
      "Epoch [1069001/10000000], Training Loss: 0.0738, Validation Loss: 0.0662, Training RMSE: 0.2716, Validation RMSE: 0.2573\n",
      "Epoch [1070001/10000000], Training Loss: 0.0741, Validation Loss: 0.0662, Training RMSE: 0.2722, Validation RMSE: 0.2573\n",
      "Epoch [1071001/10000000], Training Loss: 0.0751, Validation Loss: 0.0662, Training RMSE: 0.2741, Validation RMSE: 0.2572\n",
      "Epoch [1072001/10000000], Training Loss: 0.0739, Validation Loss: 0.0661, Training RMSE: 0.2719, Validation RMSE: 0.2572\n",
      "Epoch [1073001/10000000], Training Loss: 0.0753, Validation Loss: 0.0661, Training RMSE: 0.2743, Validation RMSE: 0.2571\n",
      "Epoch [1074001/10000000], Training Loss: 0.0746, Validation Loss: 0.0661, Training RMSE: 0.2732, Validation RMSE: 0.2571\n",
      "Epoch [1075001/10000000], Training Loss: 0.0748, Validation Loss: 0.0661, Training RMSE: 0.2736, Validation RMSE: 0.2571\n",
      "Epoch [1076001/10000000], Training Loss: 0.0747, Validation Loss: 0.0661, Training RMSE: 0.2732, Validation RMSE: 0.2570\n",
      "Epoch [1077001/10000000], Training Loss: 0.0752, Validation Loss: 0.0660, Training RMSE: 0.2743, Validation RMSE: 0.2570\n",
      "Epoch [1078001/10000000], Training Loss: 0.0734, Validation Loss: 0.0660, Training RMSE: 0.2709, Validation RMSE: 0.2570\n",
      "Epoch [1079001/10000000], Training Loss: 0.0740, Validation Loss: 0.0660, Training RMSE: 0.2721, Validation RMSE: 0.2569\n",
      "Epoch [1080001/10000000], Training Loss: 0.0751, Validation Loss: 0.0660, Training RMSE: 0.2740, Validation RMSE: 0.2569\n",
      "Epoch [1081001/10000000], Training Loss: 0.0751, Validation Loss: 0.0660, Training RMSE: 0.2741, Validation RMSE: 0.2568\n",
      "Epoch [1082001/10000000], Training Loss: 0.0742, Validation Loss: 0.0659, Training RMSE: 0.2723, Validation RMSE: 0.2568\n",
      "Epoch [1083001/10000000], Training Loss: 0.0744, Validation Loss: 0.0659, Training RMSE: 0.2728, Validation RMSE: 0.2568\n",
      "Epoch [1084001/10000000], Training Loss: 0.0737, Validation Loss: 0.0659, Training RMSE: 0.2715, Validation RMSE: 0.2567\n",
      "Epoch [1085001/10000000], Training Loss: 0.0738, Validation Loss: 0.0659, Training RMSE: 0.2716, Validation RMSE: 0.2567\n",
      "Epoch [1086001/10000000], Training Loss: 0.0752, Validation Loss: 0.0659, Training RMSE: 0.2742, Validation RMSE: 0.2566\n",
      "Epoch [1087001/10000000], Training Loss: 0.0740, Validation Loss: 0.0658, Training RMSE: 0.2720, Validation RMSE: 0.2566\n",
      "Epoch [1088001/10000000], Training Loss: 0.0739, Validation Loss: 0.0658, Training RMSE: 0.2718, Validation RMSE: 0.2566\n",
      "Epoch [1089001/10000000], Training Loss: 0.0747, Validation Loss: 0.0658, Training RMSE: 0.2734, Validation RMSE: 0.2565\n",
      "Epoch [1090001/10000000], Training Loss: 0.0751, Validation Loss: 0.0658, Training RMSE: 0.2741, Validation RMSE: 0.2565\n",
      "Epoch [1091001/10000000], Training Loss: 0.0746, Validation Loss: 0.0658, Training RMSE: 0.2731, Validation RMSE: 0.2565\n",
      "Epoch [1092001/10000000], Training Loss: 0.0735, Validation Loss: 0.0658, Training RMSE: 0.2712, Validation RMSE: 0.2564\n",
      "Epoch [1093001/10000000], Training Loss: 0.0738, Validation Loss: 0.0657, Training RMSE: 0.2716, Validation RMSE: 0.2564\n",
      "Epoch [1094001/10000000], Training Loss: 0.0748, Validation Loss: 0.0657, Training RMSE: 0.2734, Validation RMSE: 0.2563\n",
      "Epoch [1095001/10000000], Training Loss: 0.0737, Validation Loss: 0.0657, Training RMSE: 0.2714, Validation RMSE: 0.2563\n",
      "Epoch [1096001/10000000], Training Loss: 0.0743, Validation Loss: 0.0657, Training RMSE: 0.2725, Validation RMSE: 0.2563\n",
      "Epoch [1097001/10000000], Training Loss: 0.0744, Validation Loss: 0.0657, Training RMSE: 0.2728, Validation RMSE: 0.2562\n",
      "Epoch [1098001/10000000], Training Loss: 0.0742, Validation Loss: 0.0656, Training RMSE: 0.2724, Validation RMSE: 0.2562\n",
      "Epoch [1099001/10000000], Training Loss: 0.0740, Validation Loss: 0.0656, Training RMSE: 0.2721, Validation RMSE: 0.2562\n",
      "Epoch [1100001/10000000], Training Loss: 0.0738, Validation Loss: 0.0656, Training RMSE: 0.2717, Validation RMSE: 0.2561\n",
      "Epoch [1101001/10000000], Training Loss: 0.0736, Validation Loss: 0.0656, Training RMSE: 0.2714, Validation RMSE: 0.2561\n",
      "Epoch [1102001/10000000], Training Loss: 0.0750, Validation Loss: 0.0656, Training RMSE: 0.2738, Validation RMSE: 0.2561\n",
      "Epoch [1103001/10000000], Training Loss: 0.0733, Validation Loss: 0.0655, Training RMSE: 0.2708, Validation RMSE: 0.2560\n",
      "Epoch [1104001/10000000], Training Loss: 0.0741, Validation Loss: 0.0655, Training RMSE: 0.2721, Validation RMSE: 0.2560\n",
      "Epoch [1105001/10000000], Training Loss: 0.0735, Validation Loss: 0.0655, Training RMSE: 0.2711, Validation RMSE: 0.2559\n",
      "Epoch [1106001/10000000], Training Loss: 0.0740, Validation Loss: 0.0655, Training RMSE: 0.2719, Validation RMSE: 0.2559\n",
      "Epoch [1107001/10000000], Training Loss: 0.0733, Validation Loss: 0.0655, Training RMSE: 0.2707, Validation RMSE: 0.2559\n",
      "Epoch [1108001/10000000], Training Loss: 0.0739, Validation Loss: 0.0654, Training RMSE: 0.2718, Validation RMSE: 0.2558\n",
      "Epoch [1109001/10000000], Training Loss: 0.0738, Validation Loss: 0.0654, Training RMSE: 0.2716, Validation RMSE: 0.2558\n",
      "Epoch [1110001/10000000], Training Loss: 0.0747, Validation Loss: 0.0654, Training RMSE: 0.2733, Validation RMSE: 0.2558\n",
      "Epoch [1111001/10000000], Training Loss: 0.0738, Validation Loss: 0.0654, Training RMSE: 0.2716, Validation RMSE: 0.2557\n",
      "Epoch [1112001/10000000], Training Loss: 0.0741, Validation Loss: 0.0654, Training RMSE: 0.2722, Validation RMSE: 0.2557\n",
      "Epoch [1113001/10000000], Training Loss: 0.0739, Validation Loss: 0.0654, Training RMSE: 0.2719, Validation RMSE: 0.2557\n",
      "Epoch [1114001/10000000], Training Loss: 0.0741, Validation Loss: 0.0653, Training RMSE: 0.2723, Validation RMSE: 0.2556\n",
      "Epoch [1115001/10000000], Training Loss: 0.0746, Validation Loss: 0.0653, Training RMSE: 0.2730, Validation RMSE: 0.2556\n",
      "Epoch [1116001/10000000], Training Loss: 0.0738, Validation Loss: 0.0653, Training RMSE: 0.2716, Validation RMSE: 0.2555\n",
      "Epoch [1117001/10000000], Training Loss: 0.0740, Validation Loss: 0.0653, Training RMSE: 0.2720, Validation RMSE: 0.2555\n",
      "Epoch [1118001/10000000], Training Loss: 0.0731, Validation Loss: 0.0653, Training RMSE: 0.2703, Validation RMSE: 0.2555\n",
      "Epoch [1119001/10000000], Training Loss: 0.0733, Validation Loss: 0.0652, Training RMSE: 0.2708, Validation RMSE: 0.2554\n",
      "Epoch [1120001/10000000], Training Loss: 0.0739, Validation Loss: 0.0652, Training RMSE: 0.2718, Validation RMSE: 0.2554\n",
      "Epoch [1121001/10000000], Training Loss: 0.0738, Validation Loss: 0.0652, Training RMSE: 0.2716, Validation RMSE: 0.2554\n",
      "Epoch [1122001/10000000], Training Loss: 0.0733, Validation Loss: 0.0652, Training RMSE: 0.2707, Validation RMSE: 0.2553\n",
      "Epoch [1123001/10000000], Training Loss: 0.0733, Validation Loss: 0.0652, Training RMSE: 0.2707, Validation RMSE: 0.2553\n",
      "Epoch [1124001/10000000], Training Loss: 0.0736, Validation Loss: 0.0652, Training RMSE: 0.2713, Validation RMSE: 0.2553\n",
      "Epoch [1125001/10000000], Training Loss: 0.0736, Validation Loss: 0.0651, Training RMSE: 0.2713, Validation RMSE: 0.2552\n",
      "Epoch [1126001/10000000], Training Loss: 0.0737, Validation Loss: 0.0651, Training RMSE: 0.2714, Validation RMSE: 0.2552\n",
      "Epoch [1127001/10000000], Training Loss: 0.0735, Validation Loss: 0.0651, Training RMSE: 0.2711, Validation RMSE: 0.2552\n",
      "Epoch [1128001/10000000], Training Loss: 0.0736, Validation Loss: 0.0651, Training RMSE: 0.2712, Validation RMSE: 0.2551\n",
      "Epoch [1129001/10000000], Training Loss: 0.0736, Validation Loss: 0.0651, Training RMSE: 0.2713, Validation RMSE: 0.2551\n",
      "Epoch [1130001/10000000], Training Loss: 0.0738, Validation Loss: 0.0651, Training RMSE: 0.2716, Validation RMSE: 0.2551\n",
      "Epoch [1131001/10000000], Training Loss: 0.0733, Validation Loss: 0.0650, Training RMSE: 0.2707, Validation RMSE: 0.2550\n",
      "Epoch [1132001/10000000], Training Loss: 0.0735, Validation Loss: 0.0650, Training RMSE: 0.2711, Validation RMSE: 0.2550\n",
      "Epoch [1133001/10000000], Training Loss: 0.0735, Validation Loss: 0.0650, Training RMSE: 0.2711, Validation RMSE: 0.2549\n",
      "Epoch [1134001/10000000], Training Loss: 0.0722, Validation Loss: 0.0650, Training RMSE: 0.2687, Validation RMSE: 0.2549\n",
      "Epoch [1135001/10000000], Training Loss: 0.0731, Validation Loss: 0.0650, Training RMSE: 0.2704, Validation RMSE: 0.2549\n",
      "Epoch [1136001/10000000], Training Loss: 0.0744, Validation Loss: 0.0649, Training RMSE: 0.2728, Validation RMSE: 0.2548\n",
      "Epoch [1137001/10000000], Training Loss: 0.0726, Validation Loss: 0.0649, Training RMSE: 0.2695, Validation RMSE: 0.2548\n",
      "Epoch [1138001/10000000], Training Loss: 0.0734, Validation Loss: 0.0649, Training RMSE: 0.2709, Validation RMSE: 0.2548\n",
      "Epoch [1139001/10000000], Training Loss: 0.0726, Validation Loss: 0.0649, Training RMSE: 0.2694, Validation RMSE: 0.2547\n",
      "Epoch [1140001/10000000], Training Loss: 0.0740, Validation Loss: 0.0649, Training RMSE: 0.2719, Validation RMSE: 0.2547\n",
      "Epoch [1141001/10000000], Training Loss: 0.0729, Validation Loss: 0.0649, Training RMSE: 0.2701, Validation RMSE: 0.2547\n",
      "Epoch [1142001/10000000], Training Loss: 0.0728, Validation Loss: 0.0648, Training RMSE: 0.2698, Validation RMSE: 0.2546\n",
      "Epoch [1143001/10000000], Training Loss: 0.0738, Validation Loss: 0.0648, Training RMSE: 0.2716, Validation RMSE: 0.2546\n",
      "Epoch [1144001/10000000], Training Loss: 0.0730, Validation Loss: 0.0648, Training RMSE: 0.2702, Validation RMSE: 0.2546\n",
      "Epoch [1145001/10000000], Training Loss: 0.0733, Validation Loss: 0.0648, Training RMSE: 0.2707, Validation RMSE: 0.2545\n",
      "Epoch [1146001/10000000], Training Loss: 0.0732, Validation Loss: 0.0648, Training RMSE: 0.2706, Validation RMSE: 0.2545\n",
      "Epoch [1147001/10000000], Training Loss: 0.0737, Validation Loss: 0.0648, Training RMSE: 0.2715, Validation RMSE: 0.2545\n",
      "Epoch [1148001/10000000], Training Loss: 0.0727, Validation Loss: 0.0647, Training RMSE: 0.2695, Validation RMSE: 0.2544\n",
      "Epoch [1149001/10000000], Training Loss: 0.0728, Validation Loss: 0.0647, Training RMSE: 0.2699, Validation RMSE: 0.2544\n",
      "Epoch [1150001/10000000], Training Loss: 0.0724, Validation Loss: 0.0647, Training RMSE: 0.2690, Validation RMSE: 0.2544\n",
      "Epoch [1151001/10000000], Training Loss: 0.0733, Validation Loss: 0.0647, Training RMSE: 0.2707, Validation RMSE: 0.2543\n",
      "Epoch [1152001/10000000], Training Loss: 0.0741, Validation Loss: 0.0647, Training RMSE: 0.2722, Validation RMSE: 0.2543\n",
      "Epoch [1153001/10000000], Training Loss: 0.0729, Validation Loss: 0.0647, Training RMSE: 0.2701, Validation RMSE: 0.2543\n",
      "Epoch [1154001/10000000], Training Loss: 0.0721, Validation Loss: 0.0646, Training RMSE: 0.2686, Validation RMSE: 0.2542\n",
      "Epoch [1155001/10000000], Training Loss: 0.0725, Validation Loss: 0.0646, Training RMSE: 0.2692, Validation RMSE: 0.2542\n",
      "Epoch [1156001/10000000], Training Loss: 0.0732, Validation Loss: 0.0646, Training RMSE: 0.2705, Validation RMSE: 0.2542\n",
      "Epoch [1157001/10000000], Training Loss: 0.0733, Validation Loss: 0.0646, Training RMSE: 0.2708, Validation RMSE: 0.2541\n",
      "Epoch [1158001/10000000], Training Loss: 0.0728, Validation Loss: 0.0646, Training RMSE: 0.2699, Validation RMSE: 0.2541\n",
      "Epoch [1159001/10000000], Training Loss: 0.0728, Validation Loss: 0.0646, Training RMSE: 0.2699, Validation RMSE: 0.2541\n",
      "Epoch [1160001/10000000], Training Loss: 0.0729, Validation Loss: 0.0645, Training RMSE: 0.2700, Validation RMSE: 0.2540\n",
      "Epoch [1161001/10000000], Training Loss: 0.0726, Validation Loss: 0.0645, Training RMSE: 0.2694, Validation RMSE: 0.2540\n",
      "Epoch [1162001/10000000], Training Loss: 0.0731, Validation Loss: 0.0645, Training RMSE: 0.2703, Validation RMSE: 0.2540\n",
      "Epoch [1163001/10000000], Training Loss: 0.0733, Validation Loss: 0.0645, Training RMSE: 0.2708, Validation RMSE: 0.2539\n",
      "Epoch [1164001/10000000], Training Loss: 0.0730, Validation Loss: 0.0645, Training RMSE: 0.2703, Validation RMSE: 0.2539\n",
      "Epoch [1165001/10000000], Training Loss: 0.0725, Validation Loss: 0.0645, Training RMSE: 0.2693, Validation RMSE: 0.2539\n",
      "Epoch [1166001/10000000], Training Loss: 0.0722, Validation Loss: 0.0644, Training RMSE: 0.2688, Validation RMSE: 0.2538\n",
      "Epoch [1167001/10000000], Training Loss: 0.0733, Validation Loss: 0.0644, Training RMSE: 0.2708, Validation RMSE: 0.2538\n",
      "Epoch [1168001/10000000], Training Loss: 0.0723, Validation Loss: 0.0644, Training RMSE: 0.2688, Validation RMSE: 0.2538\n",
      "Epoch [1169001/10000000], Training Loss: 0.0727, Validation Loss: 0.0644, Training RMSE: 0.2697, Validation RMSE: 0.2537\n",
      "Epoch [1170001/10000000], Training Loss: 0.0728, Validation Loss: 0.0644, Training RMSE: 0.2699, Validation RMSE: 0.2537\n",
      "Epoch [1171001/10000000], Training Loss: 0.0723, Validation Loss: 0.0644, Training RMSE: 0.2690, Validation RMSE: 0.2537\n",
      "Epoch [1172001/10000000], Training Loss: 0.0727, Validation Loss: 0.0643, Training RMSE: 0.2696, Validation RMSE: 0.2537\n",
      "Epoch [1173001/10000000], Training Loss: 0.0725, Validation Loss: 0.0643, Training RMSE: 0.2692, Validation RMSE: 0.2536\n",
      "Epoch [1174001/10000000], Training Loss: 0.0731, Validation Loss: 0.0643, Training RMSE: 0.2703, Validation RMSE: 0.2536\n",
      "Epoch [1175001/10000000], Training Loss: 0.0733, Validation Loss: 0.0643, Training RMSE: 0.2708, Validation RMSE: 0.2536\n",
      "Epoch [1176001/10000000], Training Loss: 0.0723, Validation Loss: 0.0643, Training RMSE: 0.2688, Validation RMSE: 0.2535\n",
      "Epoch [1177001/10000000], Training Loss: 0.0727, Validation Loss: 0.0643, Training RMSE: 0.2697, Validation RMSE: 0.2535\n",
      "Epoch [1178001/10000000], Training Loss: 0.0724, Validation Loss: 0.0642, Training RMSE: 0.2691, Validation RMSE: 0.2535\n",
      "Epoch [1179001/10000000], Training Loss: 0.0713, Validation Loss: 0.0642, Training RMSE: 0.2670, Validation RMSE: 0.2534\n",
      "Epoch [1180001/10000000], Training Loss: 0.0732, Validation Loss: 0.0642, Training RMSE: 0.2705, Validation RMSE: 0.2534\n",
      "Epoch [1181001/10000000], Training Loss: 0.0722, Validation Loss: 0.0642, Training RMSE: 0.2687, Validation RMSE: 0.2534\n",
      "Epoch [1182001/10000000], Training Loss: 0.0721, Validation Loss: 0.0642, Training RMSE: 0.2686, Validation RMSE: 0.2533\n",
      "Epoch [1183001/10000000], Training Loss: 0.0718, Validation Loss: 0.0642, Training RMSE: 0.2680, Validation RMSE: 0.2533\n",
      "Epoch [1184001/10000000], Training Loss: 0.0725, Validation Loss: 0.0641, Training RMSE: 0.2692, Validation RMSE: 0.2533\n",
      "Epoch [1185001/10000000], Training Loss: 0.0722, Validation Loss: 0.0641, Training RMSE: 0.2687, Validation RMSE: 0.2532\n",
      "Epoch [1186001/10000000], Training Loss: 0.0729, Validation Loss: 0.0641, Training RMSE: 0.2699, Validation RMSE: 0.2532\n",
      "Epoch [1187001/10000000], Training Loss: 0.0724, Validation Loss: 0.0641, Training RMSE: 0.2691, Validation RMSE: 0.2532\n",
      "Epoch [1188001/10000000], Training Loss: 0.0721, Validation Loss: 0.0641, Training RMSE: 0.2686, Validation RMSE: 0.2531\n",
      "Epoch [1189001/10000000], Training Loss: 0.0722, Validation Loss: 0.0641, Training RMSE: 0.2687, Validation RMSE: 0.2531\n",
      "Epoch [1190001/10000000], Training Loss: 0.0716, Validation Loss: 0.0641, Training RMSE: 0.2675, Validation RMSE: 0.2531\n",
      "Epoch [1191001/10000000], Training Loss: 0.0724, Validation Loss: 0.0640, Training RMSE: 0.2691, Validation RMSE: 0.2531\n",
      "Epoch [1192001/10000000], Training Loss: 0.0720, Validation Loss: 0.0640, Training RMSE: 0.2683, Validation RMSE: 0.2530\n",
      "Epoch [1193001/10000000], Training Loss: 0.0720, Validation Loss: 0.0640, Training RMSE: 0.2683, Validation RMSE: 0.2530\n",
      "Epoch [1194001/10000000], Training Loss: 0.0726, Validation Loss: 0.0640, Training RMSE: 0.2695, Validation RMSE: 0.2530\n",
      "Epoch [1195001/10000000], Training Loss: 0.0726, Validation Loss: 0.0640, Training RMSE: 0.2695, Validation RMSE: 0.2529\n",
      "Epoch [1196001/10000000], Training Loss: 0.0721, Validation Loss: 0.0640, Training RMSE: 0.2685, Validation RMSE: 0.2529\n",
      "Epoch [1197001/10000000], Training Loss: 0.0715, Validation Loss: 0.0639, Training RMSE: 0.2674, Validation RMSE: 0.2529\n",
      "Epoch [1198001/10000000], Training Loss: 0.0726, Validation Loss: 0.0639, Training RMSE: 0.2694, Validation RMSE: 0.2528\n",
      "Epoch [1199001/10000000], Training Loss: 0.0721, Validation Loss: 0.0639, Training RMSE: 0.2685, Validation RMSE: 0.2528\n",
      "Epoch [1200001/10000000], Training Loss: 0.0720, Validation Loss: 0.0639, Training RMSE: 0.2682, Validation RMSE: 0.2528\n",
      "Epoch [1201001/10000000], Training Loss: 0.0718, Validation Loss: 0.0639, Training RMSE: 0.2680, Validation RMSE: 0.2528\n",
      "Epoch [1202001/10000000], Training Loss: 0.0718, Validation Loss: 0.0639, Training RMSE: 0.2680, Validation RMSE: 0.2527\n",
      "Epoch [1203001/10000000], Training Loss: 0.0718, Validation Loss: 0.0639, Training RMSE: 0.2680, Validation RMSE: 0.2527\n",
      "Epoch [1204001/10000000], Training Loss: 0.0721, Validation Loss: 0.0638, Training RMSE: 0.2686, Validation RMSE: 0.2527\n",
      "Epoch [1205001/10000000], Training Loss: 0.0721, Validation Loss: 0.0638, Training RMSE: 0.2685, Validation RMSE: 0.2526\n",
      "Epoch [1206001/10000000], Training Loss: 0.0713, Validation Loss: 0.0638, Training RMSE: 0.2670, Validation RMSE: 0.2526\n",
      "Epoch [1207001/10000000], Training Loss: 0.0716, Validation Loss: 0.0638, Training RMSE: 0.2676, Validation RMSE: 0.2526\n",
      "Epoch [1208001/10000000], Training Loss: 0.0722, Validation Loss: 0.0638, Training RMSE: 0.2687, Validation RMSE: 0.2525\n",
      "Epoch [1209001/10000000], Training Loss: 0.0724, Validation Loss: 0.0638, Training RMSE: 0.2690, Validation RMSE: 0.2525\n",
      "Epoch [1210001/10000000], Training Loss: 0.0721, Validation Loss: 0.0637, Training RMSE: 0.2685, Validation RMSE: 0.2525\n",
      "Epoch [1211001/10000000], Training Loss: 0.0715, Validation Loss: 0.0637, Training RMSE: 0.2673, Validation RMSE: 0.2524\n",
      "Epoch [1212001/10000000], Training Loss: 0.0718, Validation Loss: 0.0637, Training RMSE: 0.2680, Validation RMSE: 0.2524\n",
      "Epoch [1213001/10000000], Training Loss: 0.0719, Validation Loss: 0.0637, Training RMSE: 0.2681, Validation RMSE: 0.2524\n",
      "Epoch [1214001/10000000], Training Loss: 0.0717, Validation Loss: 0.0637, Training RMSE: 0.2677, Validation RMSE: 0.2524\n",
      "Epoch [1215001/10000000], Training Loss: 0.0717, Validation Loss: 0.0637, Training RMSE: 0.2677, Validation RMSE: 0.2523\n",
      "Epoch [1216001/10000000], Training Loss: 0.0720, Validation Loss: 0.0637, Training RMSE: 0.2683, Validation RMSE: 0.2523\n",
      "Epoch [1217001/10000000], Training Loss: 0.0716, Validation Loss: 0.0636, Training RMSE: 0.2676, Validation RMSE: 0.2523\n",
      "Epoch [1218001/10000000], Training Loss: 0.0710, Validation Loss: 0.0636, Training RMSE: 0.2665, Validation RMSE: 0.2522\n",
      "Epoch [1219001/10000000], Training Loss: 0.0713, Validation Loss: 0.0636, Training RMSE: 0.2670, Validation RMSE: 0.2522\n",
      "Epoch [1220001/10000000], Training Loss: 0.0712, Validation Loss: 0.0636, Training RMSE: 0.2669, Validation RMSE: 0.2522\n",
      "Epoch [1221001/10000000], Training Loss: 0.0723, Validation Loss: 0.0636, Training RMSE: 0.2688, Validation RMSE: 0.2522\n",
      "Epoch [1222001/10000000], Training Loss: 0.0712, Validation Loss: 0.0636, Training RMSE: 0.2667, Validation RMSE: 0.2521\n",
      "Epoch [1223001/10000000], Training Loss: 0.0715, Validation Loss: 0.0636, Training RMSE: 0.2674, Validation RMSE: 0.2521\n",
      "Epoch [1224001/10000000], Training Loss: 0.0721, Validation Loss: 0.0635, Training RMSE: 0.2685, Validation RMSE: 0.2521\n",
      "Epoch [1225001/10000000], Training Loss: 0.0713, Validation Loss: 0.0635, Training RMSE: 0.2671, Validation RMSE: 0.2520\n",
      "Epoch [1226001/10000000], Training Loss: 0.0717, Validation Loss: 0.0635, Training RMSE: 0.2678, Validation RMSE: 0.2520\n",
      "Epoch [1227001/10000000], Training Loss: 0.0708, Validation Loss: 0.0635, Training RMSE: 0.2662, Validation RMSE: 0.2520\n",
      "Epoch [1228001/10000000], Training Loss: 0.0710, Validation Loss: 0.0635, Training RMSE: 0.2665, Validation RMSE: 0.2520\n",
      "Epoch [1229001/10000000], Training Loss: 0.0709, Validation Loss: 0.0635, Training RMSE: 0.2663, Validation RMSE: 0.2519\n",
      "Epoch [1230001/10000000], Training Loss: 0.0710, Validation Loss: 0.0635, Training RMSE: 0.2664, Validation RMSE: 0.2519\n",
      "Epoch [1231001/10000000], Training Loss: 0.0713, Validation Loss: 0.0634, Training RMSE: 0.2671, Validation RMSE: 0.2519\n",
      "Epoch [1232001/10000000], Training Loss: 0.0720, Validation Loss: 0.0634, Training RMSE: 0.2683, Validation RMSE: 0.2518\n",
      "Epoch [1233001/10000000], Training Loss: 0.0709, Validation Loss: 0.0634, Training RMSE: 0.2662, Validation RMSE: 0.2518\n",
      "Epoch [1234001/10000000], Training Loss: 0.0716, Validation Loss: 0.0634, Training RMSE: 0.2675, Validation RMSE: 0.2518\n",
      "Epoch [1235001/10000000], Training Loss: 0.0712, Validation Loss: 0.0634, Training RMSE: 0.2668, Validation RMSE: 0.2518\n",
      "Epoch [1236001/10000000], Training Loss: 0.0712, Validation Loss: 0.0634, Training RMSE: 0.2668, Validation RMSE: 0.2517\n",
      "Epoch [1237001/10000000], Training Loss: 0.0715, Validation Loss: 0.0634, Training RMSE: 0.2674, Validation RMSE: 0.2517\n",
      "Epoch [1238001/10000000], Training Loss: 0.0719, Validation Loss: 0.0633, Training RMSE: 0.2681, Validation RMSE: 0.2517\n",
      "Epoch [1239001/10000000], Training Loss: 0.0712, Validation Loss: 0.0633, Training RMSE: 0.2667, Validation RMSE: 0.2516\n",
      "Epoch [1240001/10000000], Training Loss: 0.0713, Validation Loss: 0.0633, Training RMSE: 0.2671, Validation RMSE: 0.2516\n",
      "Epoch [1241001/10000000], Training Loss: 0.0714, Validation Loss: 0.0633, Training RMSE: 0.2673, Validation RMSE: 0.2516\n",
      "Epoch [1242001/10000000], Training Loss: 0.0712, Validation Loss: 0.0633, Training RMSE: 0.2669, Validation RMSE: 0.2516\n",
      "Epoch [1243001/10000000], Training Loss: 0.0712, Validation Loss: 0.0633, Training RMSE: 0.2669, Validation RMSE: 0.2515\n",
      "Epoch [1244001/10000000], Training Loss: 0.0710, Validation Loss: 0.0633, Training RMSE: 0.2664, Validation RMSE: 0.2515\n",
      "Epoch [1245001/10000000], Training Loss: 0.0712, Validation Loss: 0.0632, Training RMSE: 0.2668, Validation RMSE: 0.2515\n",
      "Epoch [1246001/10000000], Training Loss: 0.0707, Validation Loss: 0.0632, Training RMSE: 0.2660, Validation RMSE: 0.2515\n",
      "Epoch [1247001/10000000], Training Loss: 0.0707, Validation Loss: 0.0632, Training RMSE: 0.2658, Validation RMSE: 0.2514\n",
      "Epoch [1248001/10000000], Training Loss: 0.0719, Validation Loss: 0.0632, Training RMSE: 0.2682, Validation RMSE: 0.2514\n",
      "Epoch [1249001/10000000], Training Loss: 0.0706, Validation Loss: 0.0632, Training RMSE: 0.2657, Validation RMSE: 0.2514\n",
      "Epoch [1250001/10000000], Training Loss: 0.0720, Validation Loss: 0.0632, Training RMSE: 0.2683, Validation RMSE: 0.2513\n",
      "Epoch [1251001/10000000], Training Loss: 0.0710, Validation Loss: 0.0632, Training RMSE: 0.2664, Validation RMSE: 0.2513\n",
      "Epoch [1252001/10000000], Training Loss: 0.0711, Validation Loss: 0.0631, Training RMSE: 0.2667, Validation RMSE: 0.2513\n",
      "Epoch [1253001/10000000], Training Loss: 0.0706, Validation Loss: 0.0631, Training RMSE: 0.2657, Validation RMSE: 0.2513\n",
      "Epoch [1254001/10000000], Training Loss: 0.0718, Validation Loss: 0.0631, Training RMSE: 0.2680, Validation RMSE: 0.2512\n",
      "Epoch [1255001/10000000], Training Loss: 0.0713, Validation Loss: 0.0631, Training RMSE: 0.2671, Validation RMSE: 0.2512\n",
      "Epoch [1256001/10000000], Training Loss: 0.0703, Validation Loss: 0.0631, Training RMSE: 0.2651, Validation RMSE: 0.2512\n",
      "Epoch [1257001/10000000], Training Loss: 0.0714, Validation Loss: 0.0631, Training RMSE: 0.2672, Validation RMSE: 0.2512\n",
      "Epoch [1258001/10000000], Training Loss: 0.0713, Validation Loss: 0.0631, Training RMSE: 0.2671, Validation RMSE: 0.2511\n",
      "Epoch [1259001/10000000], Training Loss: 0.0716, Validation Loss: 0.0631, Training RMSE: 0.2676, Validation RMSE: 0.2511\n",
      "Epoch [1260001/10000000], Training Loss: 0.0704, Validation Loss: 0.0630, Training RMSE: 0.2654, Validation RMSE: 0.2511\n",
      "Epoch [1261001/10000000], Training Loss: 0.0706, Validation Loss: 0.0630, Training RMSE: 0.2657, Validation RMSE: 0.2510\n",
      "Epoch [1262001/10000000], Training Loss: 0.0712, Validation Loss: 0.0630, Training RMSE: 0.2668, Validation RMSE: 0.2510\n",
      "Epoch [1263001/10000000], Training Loss: 0.0707, Validation Loss: 0.0630, Training RMSE: 0.2659, Validation RMSE: 0.2510\n",
      "Epoch [1264001/10000000], Training Loss: 0.0713, Validation Loss: 0.0630, Training RMSE: 0.2670, Validation RMSE: 0.2510\n",
      "Epoch [1265001/10000000], Training Loss: 0.0707, Validation Loss: 0.0630, Training RMSE: 0.2659, Validation RMSE: 0.2509\n",
      "Epoch [1266001/10000000], Training Loss: 0.0704, Validation Loss: 0.0630, Training RMSE: 0.2654, Validation RMSE: 0.2509\n",
      "Epoch [1267001/10000000], Training Loss: 0.0703, Validation Loss: 0.0629, Training RMSE: 0.2652, Validation RMSE: 0.2509\n",
      "Epoch [1268001/10000000], Training Loss: 0.0707, Validation Loss: 0.0629, Training RMSE: 0.2659, Validation RMSE: 0.2509\n",
      "Epoch [1269001/10000000], Training Loss: 0.0704, Validation Loss: 0.0629, Training RMSE: 0.2654, Validation RMSE: 0.2508\n",
      "Epoch [1270001/10000000], Training Loss: 0.0707, Validation Loss: 0.0629, Training RMSE: 0.2660, Validation RMSE: 0.2508\n",
      "Epoch [1271001/10000000], Training Loss: 0.0703, Validation Loss: 0.0629, Training RMSE: 0.2652, Validation RMSE: 0.2508\n",
      "Epoch [1272001/10000000], Training Loss: 0.0706, Validation Loss: 0.0629, Training RMSE: 0.2657, Validation RMSE: 0.2508\n",
      "Epoch [1273001/10000000], Training Loss: 0.0709, Validation Loss: 0.0629, Training RMSE: 0.2662, Validation RMSE: 0.2507\n",
      "Epoch [1274001/10000000], Training Loss: 0.0701, Validation Loss: 0.0629, Training RMSE: 0.2647, Validation RMSE: 0.2507\n",
      "Epoch [1275001/10000000], Training Loss: 0.0709, Validation Loss: 0.0628, Training RMSE: 0.2664, Validation RMSE: 0.2507\n",
      "Epoch [1276001/10000000], Training Loss: 0.0709, Validation Loss: 0.0628, Training RMSE: 0.2663, Validation RMSE: 0.2507\n",
      "Epoch [1277001/10000000], Training Loss: 0.0708, Validation Loss: 0.0628, Training RMSE: 0.2661, Validation RMSE: 0.2506\n",
      "Epoch [1278001/10000000], Training Loss: 0.0697, Validation Loss: 0.0628, Training RMSE: 0.2640, Validation RMSE: 0.2506\n",
      "Epoch [1279001/10000000], Training Loss: 0.0711, Validation Loss: 0.0628, Training RMSE: 0.2667, Validation RMSE: 0.2506\n",
      "Epoch [1280001/10000000], Training Loss: 0.0711, Validation Loss: 0.0628, Training RMSE: 0.2666, Validation RMSE: 0.2506\n",
      "Epoch [1281001/10000000], Training Loss: 0.0710, Validation Loss: 0.0628, Training RMSE: 0.2665, Validation RMSE: 0.2505\n",
      "Epoch [1282001/10000000], Training Loss: 0.0705, Validation Loss: 0.0628, Training RMSE: 0.2655, Validation RMSE: 0.2505\n",
      "Epoch [1283001/10000000], Training Loss: 0.0705, Validation Loss: 0.0627, Training RMSE: 0.2655, Validation RMSE: 0.2505\n",
      "Epoch [1284001/10000000], Training Loss: 0.0701, Validation Loss: 0.0627, Training RMSE: 0.2648, Validation RMSE: 0.2505\n",
      "Epoch [1285001/10000000], Training Loss: 0.0705, Validation Loss: 0.0627, Training RMSE: 0.2655, Validation RMSE: 0.2504\n",
      "Epoch [1286001/10000000], Training Loss: 0.0707, Validation Loss: 0.0627, Training RMSE: 0.2660, Validation RMSE: 0.2504\n",
      "Epoch [1287001/10000000], Training Loss: 0.0705, Validation Loss: 0.0627, Training RMSE: 0.2655, Validation RMSE: 0.2504\n",
      "Epoch [1288001/10000000], Training Loss: 0.0711, Validation Loss: 0.0627, Training RMSE: 0.2666, Validation RMSE: 0.2504\n",
      "Epoch [1289001/10000000], Training Loss: 0.0702, Validation Loss: 0.0627, Training RMSE: 0.2650, Validation RMSE: 0.2503\n",
      "Epoch [1290001/10000000], Training Loss: 0.0707, Validation Loss: 0.0627, Training RMSE: 0.2660, Validation RMSE: 0.2503\n",
      "Epoch [1291001/10000000], Training Loss: 0.0705, Validation Loss: 0.0626, Training RMSE: 0.2656, Validation RMSE: 0.2503\n",
      "Epoch [1292001/10000000], Training Loss: 0.0699, Validation Loss: 0.0626, Training RMSE: 0.2644, Validation RMSE: 0.2503\n",
      "Epoch [1293001/10000000], Training Loss: 0.0702, Validation Loss: 0.0626, Training RMSE: 0.2650, Validation RMSE: 0.2502\n",
      "Epoch [1294001/10000000], Training Loss: 0.0693, Validation Loss: 0.0626, Training RMSE: 0.2633, Validation RMSE: 0.2502\n",
      "Epoch [1295001/10000000], Training Loss: 0.0710, Validation Loss: 0.0626, Training RMSE: 0.2664, Validation RMSE: 0.2502\n",
      "Epoch [1296001/10000000], Training Loss: 0.0701, Validation Loss: 0.0626, Training RMSE: 0.2648, Validation RMSE: 0.2502\n",
      "Epoch [1297001/10000000], Training Loss: 0.0701, Validation Loss: 0.0626, Training RMSE: 0.2648, Validation RMSE: 0.2501\n",
      "Epoch [1298001/10000000], Training Loss: 0.0702, Validation Loss: 0.0626, Training RMSE: 0.2650, Validation RMSE: 0.2501\n",
      "Epoch [1299001/10000000], Training Loss: 0.0709, Validation Loss: 0.0625, Training RMSE: 0.2663, Validation RMSE: 0.2501\n",
      "Epoch [1300001/10000000], Training Loss: 0.0708, Validation Loss: 0.0625, Training RMSE: 0.2661, Validation RMSE: 0.2501\n",
      "Epoch [1301001/10000000], Training Loss: 0.0702, Validation Loss: 0.0625, Training RMSE: 0.2650, Validation RMSE: 0.2500\n",
      "Epoch [1302001/10000000], Training Loss: 0.0700, Validation Loss: 0.0625, Training RMSE: 0.2645, Validation RMSE: 0.2500\n",
      "Epoch [1303001/10000000], Training Loss: 0.0701, Validation Loss: 0.0625, Training RMSE: 0.2647, Validation RMSE: 0.2500\n",
      "Epoch [1304001/10000000], Training Loss: 0.0699, Validation Loss: 0.0625, Training RMSE: 0.2644, Validation RMSE: 0.2500\n",
      "Epoch [1305001/10000000], Training Loss: 0.0699, Validation Loss: 0.0625, Training RMSE: 0.2644, Validation RMSE: 0.2499\n",
      "Epoch [1306001/10000000], Training Loss: 0.0711, Validation Loss: 0.0625, Training RMSE: 0.2666, Validation RMSE: 0.2499\n",
      "Epoch [1307001/10000000], Training Loss: 0.0704, Validation Loss: 0.0624, Training RMSE: 0.2653, Validation RMSE: 0.2499\n",
      "Epoch [1308001/10000000], Training Loss: 0.0694, Validation Loss: 0.0624, Training RMSE: 0.2635, Validation RMSE: 0.2499\n",
      "Epoch [1309001/10000000], Training Loss: 0.0706, Validation Loss: 0.0624, Training RMSE: 0.2658, Validation RMSE: 0.2498\n",
      "Epoch [1310001/10000000], Training Loss: 0.0707, Validation Loss: 0.0624, Training RMSE: 0.2658, Validation RMSE: 0.2498\n",
      "Epoch [1311001/10000000], Training Loss: 0.0703, Validation Loss: 0.0624, Training RMSE: 0.2652, Validation RMSE: 0.2498\n",
      "Epoch [1312001/10000000], Training Loss: 0.0697, Validation Loss: 0.0624, Training RMSE: 0.2640, Validation RMSE: 0.2498\n",
      "Epoch [1313001/10000000], Training Loss: 0.0693, Validation Loss: 0.0624, Training RMSE: 0.2632, Validation RMSE: 0.2497\n",
      "Epoch [1314001/10000000], Training Loss: 0.0714, Validation Loss: 0.0624, Training RMSE: 0.2672, Validation RMSE: 0.2497\n",
      "Epoch [1315001/10000000], Training Loss: 0.0699, Validation Loss: 0.0623, Training RMSE: 0.2645, Validation RMSE: 0.2497\n",
      "Epoch [1316001/10000000], Training Loss: 0.0697, Validation Loss: 0.0623, Training RMSE: 0.2641, Validation RMSE: 0.2497\n",
      "Epoch [1317001/10000000], Training Loss: 0.0705, Validation Loss: 0.0623, Training RMSE: 0.2655, Validation RMSE: 0.2496\n",
      "Epoch [1318001/10000000], Training Loss: 0.0701, Validation Loss: 0.0623, Training RMSE: 0.2647, Validation RMSE: 0.2496\n",
      "Epoch [1319001/10000000], Training Loss: 0.0696, Validation Loss: 0.0623, Training RMSE: 0.2639, Validation RMSE: 0.2496\n",
      "Epoch [1320001/10000000], Training Loss: 0.0704, Validation Loss: 0.0623, Training RMSE: 0.2653, Validation RMSE: 0.2496\n",
      "Epoch [1321001/10000000], Training Loss: 0.0691, Validation Loss: 0.0623, Training RMSE: 0.2629, Validation RMSE: 0.2496\n",
      "Epoch [1322001/10000000], Training Loss: 0.0695, Validation Loss: 0.0623, Training RMSE: 0.2636, Validation RMSE: 0.2495\n",
      "Epoch [1323001/10000000], Training Loss: 0.0696, Validation Loss: 0.0623, Training RMSE: 0.2638, Validation RMSE: 0.2495\n",
      "Epoch [1324001/10000000], Training Loss: 0.0702, Validation Loss: 0.0622, Training RMSE: 0.2650, Validation RMSE: 0.2495\n",
      "Epoch [1325001/10000000], Training Loss: 0.0699, Validation Loss: 0.0622, Training RMSE: 0.2645, Validation RMSE: 0.2495\n",
      "Epoch [1326001/10000000], Training Loss: 0.0701, Validation Loss: 0.0622, Training RMSE: 0.2648, Validation RMSE: 0.2494\n",
      "Epoch [1327001/10000000], Training Loss: 0.0705, Validation Loss: 0.0622, Training RMSE: 0.2655, Validation RMSE: 0.2494\n",
      "Epoch [1328001/10000000], Training Loss: 0.0696, Validation Loss: 0.0622, Training RMSE: 0.2638, Validation RMSE: 0.2494\n",
      "Epoch [1329001/10000000], Training Loss: 0.0695, Validation Loss: 0.0622, Training RMSE: 0.2636, Validation RMSE: 0.2494\n",
      "Epoch [1330001/10000000], Training Loss: 0.0702, Validation Loss: 0.0622, Training RMSE: 0.2649, Validation RMSE: 0.2493\n",
      "Epoch [1331001/10000000], Training Loss: 0.0698, Validation Loss: 0.0622, Training RMSE: 0.2641, Validation RMSE: 0.2493\n",
      "Epoch [1332001/10000000], Training Loss: 0.0710, Validation Loss: 0.0622, Training RMSE: 0.2664, Validation RMSE: 0.2493\n",
      "Epoch [1333001/10000000], Training Loss: 0.0695, Validation Loss: 0.0621, Training RMSE: 0.2635, Validation RMSE: 0.2493\n",
      "Epoch [1334001/10000000], Training Loss: 0.0693, Validation Loss: 0.0621, Training RMSE: 0.2632, Validation RMSE: 0.2493\n",
      "Epoch [1335001/10000000], Training Loss: 0.0690, Validation Loss: 0.0621, Training RMSE: 0.2627, Validation RMSE: 0.2492\n",
      "Epoch [1336001/10000000], Training Loss: 0.0692, Validation Loss: 0.0621, Training RMSE: 0.2631, Validation RMSE: 0.2492\n",
      "Epoch [1337001/10000000], Training Loss: 0.0697, Validation Loss: 0.0621, Training RMSE: 0.2640, Validation RMSE: 0.2492\n",
      "Epoch [1338001/10000000], Training Loss: 0.0699, Validation Loss: 0.0621, Training RMSE: 0.2644, Validation RMSE: 0.2492\n",
      "Epoch [1339001/10000000], Training Loss: 0.0698, Validation Loss: 0.0621, Training RMSE: 0.2641, Validation RMSE: 0.2491\n",
      "Epoch [1340001/10000000], Training Loss: 0.0694, Validation Loss: 0.0621, Training RMSE: 0.2635, Validation RMSE: 0.2491\n",
      "Epoch [1341001/10000000], Training Loss: 0.0690, Validation Loss: 0.0620, Training RMSE: 0.2626, Validation RMSE: 0.2491\n",
      "Epoch [1342001/10000000], Training Loss: 0.0699, Validation Loss: 0.0620, Training RMSE: 0.2644, Validation RMSE: 0.2491\n",
      "Epoch [1343001/10000000], Training Loss: 0.0702, Validation Loss: 0.0620, Training RMSE: 0.2650, Validation RMSE: 0.2491\n",
      "Epoch [1344001/10000000], Training Loss: 0.0698, Validation Loss: 0.0620, Training RMSE: 0.2642, Validation RMSE: 0.2490\n",
      "Epoch [1345001/10000000], Training Loss: 0.0696, Validation Loss: 0.0620, Training RMSE: 0.2639, Validation RMSE: 0.2490\n",
      "Epoch [1346001/10000000], Training Loss: 0.0692, Validation Loss: 0.0620, Training RMSE: 0.2631, Validation RMSE: 0.2490\n",
      "Epoch [1347001/10000000], Training Loss: 0.0694, Validation Loss: 0.0620, Training RMSE: 0.2634, Validation RMSE: 0.2490\n",
      "Epoch [1348001/10000000], Training Loss: 0.0700, Validation Loss: 0.0620, Training RMSE: 0.2646, Validation RMSE: 0.2489\n",
      "Epoch [1349001/10000000], Training Loss: 0.0700, Validation Loss: 0.0620, Training RMSE: 0.2645, Validation RMSE: 0.2489\n",
      "Epoch [1350001/10000000], Training Loss: 0.0696, Validation Loss: 0.0620, Training RMSE: 0.2639, Validation RMSE: 0.2489\n",
      "Epoch [1351001/10000000], Training Loss: 0.0698, Validation Loss: 0.0619, Training RMSE: 0.2642, Validation RMSE: 0.2489\n",
      "Epoch [1352001/10000000], Training Loss: 0.0694, Validation Loss: 0.0619, Training RMSE: 0.2635, Validation RMSE: 0.2489\n",
      "Epoch [1353001/10000000], Training Loss: 0.0683, Validation Loss: 0.0619, Training RMSE: 0.2614, Validation RMSE: 0.2488\n",
      "Epoch [1354001/10000000], Training Loss: 0.0687, Validation Loss: 0.0619, Training RMSE: 0.2620, Validation RMSE: 0.2488\n",
      "Epoch [1355001/10000000], Training Loss: 0.0699, Validation Loss: 0.0619, Training RMSE: 0.2643, Validation RMSE: 0.2488\n",
      "Epoch [1356001/10000000], Training Loss: 0.0695, Validation Loss: 0.0619, Training RMSE: 0.2636, Validation RMSE: 0.2488\n",
      "Epoch [1357001/10000000], Training Loss: 0.0693, Validation Loss: 0.0619, Training RMSE: 0.2633, Validation RMSE: 0.2487\n",
      "Epoch [1358001/10000000], Training Loss: 0.0693, Validation Loss: 0.0619, Training RMSE: 0.2632, Validation RMSE: 0.2487\n",
      "Epoch [1359001/10000000], Training Loss: 0.0696, Validation Loss: 0.0619, Training RMSE: 0.2637, Validation RMSE: 0.2487\n",
      "Epoch [1360001/10000000], Training Loss: 0.0691, Validation Loss: 0.0618, Training RMSE: 0.2628, Validation RMSE: 0.2487\n",
      "Epoch [1361001/10000000], Training Loss: 0.0694, Validation Loss: 0.0618, Training RMSE: 0.2634, Validation RMSE: 0.2487\n",
      "Epoch [1362001/10000000], Training Loss: 0.0699, Validation Loss: 0.0618, Training RMSE: 0.2644, Validation RMSE: 0.2486\n",
      "Epoch [1363001/10000000], Training Loss: 0.0696, Validation Loss: 0.0618, Training RMSE: 0.2638, Validation RMSE: 0.2486\n",
      "Epoch [1364001/10000000], Training Loss: 0.0692, Validation Loss: 0.0618, Training RMSE: 0.2631, Validation RMSE: 0.2486\n",
      "Epoch [1365001/10000000], Training Loss: 0.0698, Validation Loss: 0.0618, Training RMSE: 0.2641, Validation RMSE: 0.2486\n",
      "Epoch [1366001/10000000], Training Loss: 0.0696, Validation Loss: 0.0618, Training RMSE: 0.2637, Validation RMSE: 0.2486\n",
      "Epoch [1367001/10000000], Training Loss: 0.0688, Validation Loss: 0.0618, Training RMSE: 0.2624, Validation RMSE: 0.2485\n",
      "Epoch [1368001/10000000], Training Loss: 0.0692, Validation Loss: 0.0618, Training RMSE: 0.2631, Validation RMSE: 0.2485\n",
      "Epoch [1369001/10000000], Training Loss: 0.0692, Validation Loss: 0.0617, Training RMSE: 0.2630, Validation RMSE: 0.2485\n",
      "Epoch [1370001/10000000], Training Loss: 0.0696, Validation Loss: 0.0617, Training RMSE: 0.2639, Validation RMSE: 0.2485\n",
      "Epoch [1371001/10000000], Training Loss: 0.0694, Validation Loss: 0.0617, Training RMSE: 0.2634, Validation RMSE: 0.2484\n",
      "Epoch [1372001/10000000], Training Loss: 0.0698, Validation Loss: 0.0617, Training RMSE: 0.2642, Validation RMSE: 0.2484\n",
      "Epoch [1373001/10000000], Training Loss: 0.0689, Validation Loss: 0.0617, Training RMSE: 0.2626, Validation RMSE: 0.2484\n",
      "Epoch [1374001/10000000], Training Loss: 0.0697, Validation Loss: 0.0617, Training RMSE: 0.2640, Validation RMSE: 0.2484\n",
      "Epoch [1375001/10000000], Training Loss: 0.0694, Validation Loss: 0.0617, Training RMSE: 0.2634, Validation RMSE: 0.2484\n",
      "Epoch [1376001/10000000], Training Loss: 0.0696, Validation Loss: 0.0617, Training RMSE: 0.2638, Validation RMSE: 0.2483\n",
      "Epoch [1377001/10000000], Training Loss: 0.0689, Validation Loss: 0.0617, Training RMSE: 0.2625, Validation RMSE: 0.2483\n",
      "Epoch [1378001/10000000], Training Loss: 0.0689, Validation Loss: 0.0617, Training RMSE: 0.2626, Validation RMSE: 0.2483\n",
      "Epoch [1379001/10000000], Training Loss: 0.0692, Validation Loss: 0.0616, Training RMSE: 0.2630, Validation RMSE: 0.2483\n",
      "Epoch [1380001/10000000], Training Loss: 0.0697, Validation Loss: 0.0616, Training RMSE: 0.2641, Validation RMSE: 0.2483\n",
      "Epoch [1381001/10000000], Training Loss: 0.0701, Validation Loss: 0.0616, Training RMSE: 0.2647, Validation RMSE: 0.2482\n",
      "Epoch [1382001/10000000], Training Loss: 0.0694, Validation Loss: 0.0616, Training RMSE: 0.2635, Validation RMSE: 0.2482\n",
      "Epoch [1383001/10000000], Training Loss: 0.0690, Validation Loss: 0.0616, Training RMSE: 0.2626, Validation RMSE: 0.2482\n",
      "Epoch [1384001/10000000], Training Loss: 0.0683, Validation Loss: 0.0616, Training RMSE: 0.2613, Validation RMSE: 0.2482\n",
      "Epoch [1385001/10000000], Training Loss: 0.0689, Validation Loss: 0.0616, Training RMSE: 0.2624, Validation RMSE: 0.2482\n",
      "Epoch [1386001/10000000], Training Loss: 0.0690, Validation Loss: 0.0616, Training RMSE: 0.2627, Validation RMSE: 0.2481\n",
      "Epoch [1387001/10000000], Training Loss: 0.0687, Validation Loss: 0.0616, Training RMSE: 0.2621, Validation RMSE: 0.2481\n",
      "Epoch [1388001/10000000], Training Loss: 0.0695, Validation Loss: 0.0616, Training RMSE: 0.2637, Validation RMSE: 0.2481\n",
      "Epoch [1389001/10000000], Training Loss: 0.0688, Validation Loss: 0.0615, Training RMSE: 0.2622, Validation RMSE: 0.2481\n",
      "Epoch [1390001/10000000], Training Loss: 0.0695, Validation Loss: 0.0615, Training RMSE: 0.2637, Validation RMSE: 0.2481\n",
      "Epoch [1391001/10000000], Training Loss: 0.0693, Validation Loss: 0.0615, Training RMSE: 0.2632, Validation RMSE: 0.2480\n",
      "Epoch [1392001/10000000], Training Loss: 0.0683, Validation Loss: 0.0615, Training RMSE: 0.2614, Validation RMSE: 0.2480\n",
      "Epoch [1393001/10000000], Training Loss: 0.0691, Validation Loss: 0.0615, Training RMSE: 0.2629, Validation RMSE: 0.2480\n",
      "Epoch [1394001/10000000], Training Loss: 0.0678, Validation Loss: 0.0615, Training RMSE: 0.2603, Validation RMSE: 0.2480\n",
      "Epoch [1395001/10000000], Training Loss: 0.0695, Validation Loss: 0.0615, Training RMSE: 0.2637, Validation RMSE: 0.2480\n",
      "Epoch [1396001/10000000], Training Loss: 0.0691, Validation Loss: 0.0615, Training RMSE: 0.2629, Validation RMSE: 0.2479\n",
      "Epoch [1397001/10000000], Training Loss: 0.0690, Validation Loss: 0.0615, Training RMSE: 0.2626, Validation RMSE: 0.2479\n",
      "Epoch [1398001/10000000], Training Loss: 0.0686, Validation Loss: 0.0615, Training RMSE: 0.2620, Validation RMSE: 0.2479\n",
      "Epoch [1399001/10000000], Training Loss: 0.0692, Validation Loss: 0.0614, Training RMSE: 0.2631, Validation RMSE: 0.2479\n",
      "Epoch [1400001/10000000], Training Loss: 0.0680, Validation Loss: 0.0614, Training RMSE: 0.2607, Validation RMSE: 0.2479\n",
      "Epoch [1401001/10000000], Training Loss: 0.0688, Validation Loss: 0.0614, Training RMSE: 0.2624, Validation RMSE: 0.2478\n",
      "Epoch [1402001/10000000], Training Loss: 0.0694, Validation Loss: 0.0614, Training RMSE: 0.2635, Validation RMSE: 0.2478\n",
      "Epoch [1403001/10000000], Training Loss: 0.0690, Validation Loss: 0.0614, Training RMSE: 0.2626, Validation RMSE: 0.2478\n",
      "Epoch [1404001/10000000], Training Loss: 0.0676, Validation Loss: 0.0614, Training RMSE: 0.2601, Validation RMSE: 0.2478\n",
      "Epoch [1405001/10000000], Training Loss: 0.0673, Validation Loss: 0.0614, Training RMSE: 0.2595, Validation RMSE: 0.2478\n",
      "Epoch [1406001/10000000], Training Loss: 0.0689, Validation Loss: 0.0614, Training RMSE: 0.2624, Validation RMSE: 0.2477\n",
      "Epoch [1407001/10000000], Training Loss: 0.0682, Validation Loss: 0.0614, Training RMSE: 0.2612, Validation RMSE: 0.2477\n",
      "Epoch [1408001/10000000], Training Loss: 0.0685, Validation Loss: 0.0614, Training RMSE: 0.2617, Validation RMSE: 0.2477\n",
      "Epoch [1409001/10000000], Training Loss: 0.0688, Validation Loss: 0.0613, Training RMSE: 0.2623, Validation RMSE: 0.2477\n",
      "Epoch [1410001/10000000], Training Loss: 0.0681, Validation Loss: 0.0613, Training RMSE: 0.2609, Validation RMSE: 0.2477\n",
      "Epoch [1411001/10000000], Training Loss: 0.0691, Validation Loss: 0.0613, Training RMSE: 0.2629, Validation RMSE: 0.2476\n",
      "Epoch [1412001/10000000], Training Loss: 0.0680, Validation Loss: 0.0613, Training RMSE: 0.2607, Validation RMSE: 0.2476\n",
      "Epoch [1413001/10000000], Training Loss: 0.0686, Validation Loss: 0.0613, Training RMSE: 0.2619, Validation RMSE: 0.2476\n",
      "Epoch [1414001/10000000], Training Loss: 0.0687, Validation Loss: 0.0613, Training RMSE: 0.2621, Validation RMSE: 0.2476\n",
      "Epoch [1415001/10000000], Training Loss: 0.0680, Validation Loss: 0.0613, Training RMSE: 0.2608, Validation RMSE: 0.2476\n",
      "Epoch [1416001/10000000], Training Loss: 0.0678, Validation Loss: 0.0613, Training RMSE: 0.2605, Validation RMSE: 0.2475\n",
      "Epoch [1417001/10000000], Training Loss: 0.0686, Validation Loss: 0.0613, Training RMSE: 0.2620, Validation RMSE: 0.2475\n",
      "Epoch [1418001/10000000], Training Loss: 0.0683, Validation Loss: 0.0613, Training RMSE: 0.2613, Validation RMSE: 0.2475\n",
      "Epoch [1419001/10000000], Training Loss: 0.0681, Validation Loss: 0.0613, Training RMSE: 0.2610, Validation RMSE: 0.2475\n",
      "Epoch [1420001/10000000], Training Loss: 0.0690, Validation Loss: 0.0612, Training RMSE: 0.2627, Validation RMSE: 0.2475\n",
      "Epoch [1421001/10000000], Training Loss: 0.0682, Validation Loss: 0.0612, Training RMSE: 0.2612, Validation RMSE: 0.2475\n",
      "Epoch [1422001/10000000], Training Loss: 0.0688, Validation Loss: 0.0612, Training RMSE: 0.2623, Validation RMSE: 0.2474\n",
      "Epoch [1423001/10000000], Training Loss: 0.0687, Validation Loss: 0.0612, Training RMSE: 0.2621, Validation RMSE: 0.2474\n",
      "Epoch [1424001/10000000], Training Loss: 0.0689, Validation Loss: 0.0612, Training RMSE: 0.2624, Validation RMSE: 0.2474\n",
      "Epoch [1425001/10000000], Training Loss: 0.0686, Validation Loss: 0.0612, Training RMSE: 0.2618, Validation RMSE: 0.2474\n",
      "Epoch [1426001/10000000], Training Loss: 0.0674, Validation Loss: 0.0612, Training RMSE: 0.2595, Validation RMSE: 0.2474\n",
      "Epoch [1427001/10000000], Training Loss: 0.0678, Validation Loss: 0.0612, Training RMSE: 0.2604, Validation RMSE: 0.2473\n",
      "Epoch [1428001/10000000], Training Loss: 0.0681, Validation Loss: 0.0612, Training RMSE: 0.2610, Validation RMSE: 0.2473\n",
      "Epoch [1429001/10000000], Training Loss: 0.0675, Validation Loss: 0.0612, Training RMSE: 0.2598, Validation RMSE: 0.2473\n",
      "Epoch [1430001/10000000], Training Loss: 0.0680, Validation Loss: 0.0612, Training RMSE: 0.2608, Validation RMSE: 0.2473\n",
      "Epoch [1431001/10000000], Training Loss: 0.0682, Validation Loss: 0.0611, Training RMSE: 0.2611, Validation RMSE: 0.2473\n",
      "Epoch [1432001/10000000], Training Loss: 0.0691, Validation Loss: 0.0611, Training RMSE: 0.2629, Validation RMSE: 0.2473\n",
      "Epoch [1433001/10000000], Training Loss: 0.0678, Validation Loss: 0.0611, Training RMSE: 0.2604, Validation RMSE: 0.2472\n",
      "Epoch [1434001/10000000], Training Loss: 0.0681, Validation Loss: 0.0611, Training RMSE: 0.2610, Validation RMSE: 0.2472\n",
      "Epoch [1435001/10000000], Training Loss: 0.0684, Validation Loss: 0.0611, Training RMSE: 0.2615, Validation RMSE: 0.2472\n",
      "Epoch [1436001/10000000], Training Loss: 0.0682, Validation Loss: 0.0611, Training RMSE: 0.2611, Validation RMSE: 0.2472\n",
      "Epoch [1437001/10000000], Training Loss: 0.0683, Validation Loss: 0.0611, Training RMSE: 0.2614, Validation RMSE: 0.2472\n",
      "Epoch [1438001/10000000], Training Loss: 0.0683, Validation Loss: 0.0611, Training RMSE: 0.2614, Validation RMSE: 0.2471\n",
      "Epoch [1439001/10000000], Training Loss: 0.0677, Validation Loss: 0.0611, Training RMSE: 0.2601, Validation RMSE: 0.2471\n",
      "Epoch [1440001/10000000], Training Loss: 0.0682, Validation Loss: 0.0611, Training RMSE: 0.2611, Validation RMSE: 0.2471\n",
      "Epoch [1441001/10000000], Training Loss: 0.0686, Validation Loss: 0.0611, Training RMSE: 0.2618, Validation RMSE: 0.2471\n",
      "Epoch [1442001/10000000], Training Loss: 0.0677, Validation Loss: 0.0610, Training RMSE: 0.2602, Validation RMSE: 0.2471\n",
      "Epoch [1443001/10000000], Training Loss: 0.0682, Validation Loss: 0.0610, Training RMSE: 0.2611, Validation RMSE: 0.2471\n",
      "Epoch [1444001/10000000], Training Loss: 0.0675, Validation Loss: 0.0610, Training RMSE: 0.2598, Validation RMSE: 0.2470\n",
      "Epoch [1445001/10000000], Training Loss: 0.0688, Validation Loss: 0.0610, Training RMSE: 0.2624, Validation RMSE: 0.2470\n",
      "Epoch [1446001/10000000], Training Loss: 0.0680, Validation Loss: 0.0610, Training RMSE: 0.2608, Validation RMSE: 0.2470\n",
      "Epoch [1447001/10000000], Training Loss: 0.0685, Validation Loss: 0.0610, Training RMSE: 0.2617, Validation RMSE: 0.2470\n",
      "Epoch [1448001/10000000], Training Loss: 0.0681, Validation Loss: 0.0610, Training RMSE: 0.2610, Validation RMSE: 0.2470\n",
      "Epoch [1449001/10000000], Training Loss: 0.0676, Validation Loss: 0.0610, Training RMSE: 0.2600, Validation RMSE: 0.2469\n",
      "Epoch [1450001/10000000], Training Loss: 0.0683, Validation Loss: 0.0610, Training RMSE: 0.2613, Validation RMSE: 0.2469\n",
      "Epoch [1451001/10000000], Training Loss: 0.0678, Validation Loss: 0.0610, Training RMSE: 0.2604, Validation RMSE: 0.2469\n",
      "Epoch [1452001/10000000], Training Loss: 0.0679, Validation Loss: 0.0610, Training RMSE: 0.2606, Validation RMSE: 0.2469\n",
      "Epoch [1453001/10000000], Training Loss: 0.0684, Validation Loss: 0.0609, Training RMSE: 0.2615, Validation RMSE: 0.2469\n",
      "Epoch [1454001/10000000], Training Loss: 0.0679, Validation Loss: 0.0609, Training RMSE: 0.2606, Validation RMSE: 0.2469\n",
      "Epoch [1455001/10000000], Training Loss: 0.0681, Validation Loss: 0.0609, Training RMSE: 0.2609, Validation RMSE: 0.2468\n",
      "Epoch [1456001/10000000], Training Loss: 0.0683, Validation Loss: 0.0609, Training RMSE: 0.2614, Validation RMSE: 0.2468\n",
      "Epoch [1457001/10000000], Training Loss: 0.0678, Validation Loss: 0.0609, Training RMSE: 0.2605, Validation RMSE: 0.2468\n",
      "Epoch [1458001/10000000], Training Loss: 0.0680, Validation Loss: 0.0609, Training RMSE: 0.2608, Validation RMSE: 0.2468\n",
      "Epoch [1459001/10000000], Training Loss: 0.0685, Validation Loss: 0.0609, Training RMSE: 0.2617, Validation RMSE: 0.2468\n",
      "Epoch [1460001/10000000], Training Loss: 0.0672, Validation Loss: 0.0609, Training RMSE: 0.2592, Validation RMSE: 0.2468\n",
      "Epoch [1461001/10000000], Training Loss: 0.0684, Validation Loss: 0.0609, Training RMSE: 0.2615, Validation RMSE: 0.2467\n",
      "Epoch [1462001/10000000], Training Loss: 0.0683, Validation Loss: 0.0609, Training RMSE: 0.2614, Validation RMSE: 0.2467\n",
      "Epoch [1463001/10000000], Training Loss: 0.0682, Validation Loss: 0.0609, Training RMSE: 0.2612, Validation RMSE: 0.2467\n",
      "Epoch [1464001/10000000], Training Loss: 0.0683, Validation Loss: 0.0609, Training RMSE: 0.2614, Validation RMSE: 0.2467\n",
      "Epoch [1465001/10000000], Training Loss: 0.0682, Validation Loss: 0.0608, Training RMSE: 0.2611, Validation RMSE: 0.2467\n",
      "Epoch [1466001/10000000], Training Loss: 0.0673, Validation Loss: 0.0608, Training RMSE: 0.2595, Validation RMSE: 0.2466\n",
      "Epoch [1467001/10000000], Training Loss: 0.0675, Validation Loss: 0.0608, Training RMSE: 0.2598, Validation RMSE: 0.2466\n",
      "Epoch [1468001/10000000], Training Loss: 0.0677, Validation Loss: 0.0608, Training RMSE: 0.2603, Validation RMSE: 0.2466\n",
      "Epoch [1469001/10000000], Training Loss: 0.0675, Validation Loss: 0.0608, Training RMSE: 0.2597, Validation RMSE: 0.2466\n",
      "Epoch [1470001/10000000], Training Loss: 0.0672, Validation Loss: 0.0608, Training RMSE: 0.2592, Validation RMSE: 0.2466\n",
      "Epoch [1471001/10000000], Training Loss: 0.0670, Validation Loss: 0.0608, Training RMSE: 0.2589, Validation RMSE: 0.2466\n",
      "Epoch [1472001/10000000], Training Loss: 0.0679, Validation Loss: 0.0608, Training RMSE: 0.2605, Validation RMSE: 0.2465\n",
      "Epoch [1473001/10000000], Training Loss: 0.0680, Validation Loss: 0.0608, Training RMSE: 0.2609, Validation RMSE: 0.2465\n",
      "Epoch [1474001/10000000], Training Loss: 0.0670, Validation Loss: 0.0608, Training RMSE: 0.2588, Validation RMSE: 0.2465\n",
      "Epoch [1475001/10000000], Training Loss: 0.0679, Validation Loss: 0.0608, Training RMSE: 0.2606, Validation RMSE: 0.2465\n",
      "Epoch [1476001/10000000], Training Loss: 0.0678, Validation Loss: 0.0608, Training RMSE: 0.2605, Validation RMSE: 0.2465\n",
      "Epoch [1477001/10000000], Training Loss: 0.0679, Validation Loss: 0.0607, Training RMSE: 0.2605, Validation RMSE: 0.2465\n",
      "Epoch [1478001/10000000], Training Loss: 0.0675, Validation Loss: 0.0607, Training RMSE: 0.2598, Validation RMSE: 0.2464\n",
      "Epoch [1479001/10000000], Training Loss: 0.0683, Validation Loss: 0.0607, Training RMSE: 0.2614, Validation RMSE: 0.2464\n",
      "Epoch [1480001/10000000], Training Loss: 0.0681, Validation Loss: 0.0607, Training RMSE: 0.2610, Validation RMSE: 0.2464\n",
      "Epoch [1481001/10000000], Training Loss: 0.0682, Validation Loss: 0.0607, Training RMSE: 0.2611, Validation RMSE: 0.2464\n",
      "Epoch [1482001/10000000], Training Loss: 0.0675, Validation Loss: 0.0607, Training RMSE: 0.2597, Validation RMSE: 0.2464\n",
      "Epoch [1483001/10000000], Training Loss: 0.0673, Validation Loss: 0.0607, Training RMSE: 0.2594, Validation RMSE: 0.2464\n",
      "Epoch [1484001/10000000], Training Loss: 0.0673, Validation Loss: 0.0607, Training RMSE: 0.2595, Validation RMSE: 0.2463\n",
      "Epoch [1485001/10000000], Training Loss: 0.0677, Validation Loss: 0.0607, Training RMSE: 0.2601, Validation RMSE: 0.2463\n",
      "Epoch [1486001/10000000], Training Loss: 0.0677, Validation Loss: 0.0607, Training RMSE: 0.2601, Validation RMSE: 0.2463\n",
      "Epoch [1487001/10000000], Training Loss: 0.0676, Validation Loss: 0.0607, Training RMSE: 0.2600, Validation RMSE: 0.2463\n",
      "Epoch [1488001/10000000], Training Loss: 0.0678, Validation Loss: 0.0607, Training RMSE: 0.2605, Validation RMSE: 0.2463\n",
      "Epoch [1489001/10000000], Training Loss: 0.0672, Validation Loss: 0.0606, Training RMSE: 0.2592, Validation RMSE: 0.2463\n",
      "Epoch [1490001/10000000], Training Loss: 0.0676, Validation Loss: 0.0606, Training RMSE: 0.2600, Validation RMSE: 0.2462\n",
      "Epoch [1491001/10000000], Training Loss: 0.0678, Validation Loss: 0.0606, Training RMSE: 0.2603, Validation RMSE: 0.2462\n",
      "Epoch [1492001/10000000], Training Loss: 0.0675, Validation Loss: 0.0606, Training RMSE: 0.2599, Validation RMSE: 0.2462\n",
      "Epoch [1493001/10000000], Training Loss: 0.0673, Validation Loss: 0.0606, Training RMSE: 0.2593, Validation RMSE: 0.2462\n",
      "Epoch [1494001/10000000], Training Loss: 0.0677, Validation Loss: 0.0606, Training RMSE: 0.2602, Validation RMSE: 0.2462\n",
      "Epoch [1495001/10000000], Training Loss: 0.0670, Validation Loss: 0.0606, Training RMSE: 0.2588, Validation RMSE: 0.2462\n",
      "Epoch [1496001/10000000], Training Loss: 0.0675, Validation Loss: 0.0606, Training RMSE: 0.2598, Validation RMSE: 0.2461\n",
      "Epoch [1497001/10000000], Training Loss: 0.0679, Validation Loss: 0.0606, Training RMSE: 0.2606, Validation RMSE: 0.2461\n",
      "Epoch [1498001/10000000], Training Loss: 0.0675, Validation Loss: 0.0606, Training RMSE: 0.2599, Validation RMSE: 0.2461\n",
      "Epoch [1499001/10000000], Training Loss: 0.0678, Validation Loss: 0.0606, Training RMSE: 0.2605, Validation RMSE: 0.2461\n",
      "Epoch [1500001/10000000], Training Loss: 0.0669, Validation Loss: 0.0606, Training RMSE: 0.2586, Validation RMSE: 0.2461\n",
      "Epoch [1501001/10000000], Training Loss: 0.0674, Validation Loss: 0.0605, Training RMSE: 0.2596, Validation RMSE: 0.2461\n",
      "Epoch [1502001/10000000], Training Loss: 0.0671, Validation Loss: 0.0605, Training RMSE: 0.2591, Validation RMSE: 0.2460\n",
      "Epoch [1503001/10000000], Training Loss: 0.0666, Validation Loss: 0.0605, Training RMSE: 0.2581, Validation RMSE: 0.2460\n",
      "Epoch [1504001/10000000], Training Loss: 0.0674, Validation Loss: 0.0605, Training RMSE: 0.2596, Validation RMSE: 0.2460\n",
      "Epoch [1505001/10000000], Training Loss: 0.0678, Validation Loss: 0.0605, Training RMSE: 0.2604, Validation RMSE: 0.2460\n",
      "Epoch [1506001/10000000], Training Loss: 0.0669, Validation Loss: 0.0605, Training RMSE: 0.2586, Validation RMSE: 0.2460\n",
      "Epoch [1507001/10000000], Training Loss: 0.0673, Validation Loss: 0.0605, Training RMSE: 0.2594, Validation RMSE: 0.2460\n",
      "Epoch [1508001/10000000], Training Loss: 0.0672, Validation Loss: 0.0605, Training RMSE: 0.2591, Validation RMSE: 0.2459\n",
      "Epoch [1509001/10000000], Training Loss: 0.0676, Validation Loss: 0.0605, Training RMSE: 0.2601, Validation RMSE: 0.2459\n",
      "Epoch [1510001/10000000], Training Loss: 0.0665, Validation Loss: 0.0605, Training RMSE: 0.2579, Validation RMSE: 0.2459\n",
      "Epoch [1511001/10000000], Training Loss: 0.0673, Validation Loss: 0.0605, Training RMSE: 0.2595, Validation RMSE: 0.2459\n",
      "Epoch [1512001/10000000], Training Loss: 0.0678, Validation Loss: 0.0605, Training RMSE: 0.2604, Validation RMSE: 0.2459\n",
      "Epoch [1513001/10000000], Training Loss: 0.0674, Validation Loss: 0.0605, Training RMSE: 0.2596, Validation RMSE: 0.2459\n",
      "Epoch [1514001/10000000], Training Loss: 0.0668, Validation Loss: 0.0604, Training RMSE: 0.2584, Validation RMSE: 0.2459\n",
      "Epoch [1515001/10000000], Training Loss: 0.0668, Validation Loss: 0.0604, Training RMSE: 0.2585, Validation RMSE: 0.2458\n",
      "Epoch [1516001/10000000], Training Loss: 0.0671, Validation Loss: 0.0604, Training RMSE: 0.2591, Validation RMSE: 0.2458\n",
      "Epoch [1517001/10000000], Training Loss: 0.0667, Validation Loss: 0.0604, Training RMSE: 0.2583, Validation RMSE: 0.2458\n",
      "Epoch [1518001/10000000], Training Loss: 0.0671, Validation Loss: 0.0604, Training RMSE: 0.2590, Validation RMSE: 0.2458\n",
      "Epoch [1519001/10000000], Training Loss: 0.0668, Validation Loss: 0.0604, Training RMSE: 0.2584, Validation RMSE: 0.2458\n",
      "Epoch [1520001/10000000], Training Loss: 0.0676, Validation Loss: 0.0604, Training RMSE: 0.2600, Validation RMSE: 0.2458\n",
      "Epoch [1521001/10000000], Training Loss: 0.0668, Validation Loss: 0.0604, Training RMSE: 0.2585, Validation RMSE: 0.2457\n",
      "Epoch [1522001/10000000], Training Loss: 0.0674, Validation Loss: 0.0604, Training RMSE: 0.2596, Validation RMSE: 0.2457\n",
      "Epoch [1523001/10000000], Training Loss: 0.0670, Validation Loss: 0.0604, Training RMSE: 0.2588, Validation RMSE: 0.2457\n",
      "Epoch [1524001/10000000], Training Loss: 0.0661, Validation Loss: 0.0604, Training RMSE: 0.2571, Validation RMSE: 0.2457\n",
      "Epoch [1525001/10000000], Training Loss: 0.0670, Validation Loss: 0.0604, Training RMSE: 0.2588, Validation RMSE: 0.2457\n",
      "Epoch [1526001/10000000], Training Loss: 0.0681, Validation Loss: 0.0604, Training RMSE: 0.2609, Validation RMSE: 0.2457\n",
      "Epoch [1527001/10000000], Training Loss: 0.0668, Validation Loss: 0.0603, Training RMSE: 0.2584, Validation RMSE: 0.2456\n",
      "Epoch [1528001/10000000], Training Loss: 0.0677, Validation Loss: 0.0603, Training RMSE: 0.2602, Validation RMSE: 0.2456\n",
      "Epoch [1529001/10000000], Training Loss: 0.0671, Validation Loss: 0.0603, Training RMSE: 0.2591, Validation RMSE: 0.2456\n",
      "Epoch [1530001/10000000], Training Loss: 0.0669, Validation Loss: 0.0603, Training RMSE: 0.2586, Validation RMSE: 0.2456\n",
      "Epoch [1531001/10000000], Training Loss: 0.0664, Validation Loss: 0.0603, Training RMSE: 0.2577, Validation RMSE: 0.2456\n",
      "Epoch [1532001/10000000], Training Loss: 0.0668, Validation Loss: 0.0603, Training RMSE: 0.2585, Validation RMSE: 0.2456\n",
      "Epoch [1533001/10000000], Training Loss: 0.0671, Validation Loss: 0.0603, Training RMSE: 0.2590, Validation RMSE: 0.2456\n",
      "Epoch [1534001/10000000], Training Loss: 0.0673, Validation Loss: 0.0603, Training RMSE: 0.2594, Validation RMSE: 0.2455\n",
      "Epoch [1535001/10000000], Training Loss: 0.0666, Validation Loss: 0.0603, Training RMSE: 0.2580, Validation RMSE: 0.2455\n",
      "Epoch [1536001/10000000], Training Loss: 0.0667, Validation Loss: 0.0603, Training RMSE: 0.2582, Validation RMSE: 0.2455\n",
      "Epoch [1537001/10000000], Training Loss: 0.0673, Validation Loss: 0.0603, Training RMSE: 0.2595, Validation RMSE: 0.2455\n",
      "Epoch [1538001/10000000], Training Loss: 0.0668, Validation Loss: 0.0603, Training RMSE: 0.2585, Validation RMSE: 0.2455\n",
      "Epoch [1539001/10000000], Training Loss: 0.0668, Validation Loss: 0.0603, Training RMSE: 0.2585, Validation RMSE: 0.2455\n",
      "Epoch [1540001/10000000], Training Loss: 0.0671, Validation Loss: 0.0602, Training RMSE: 0.2590, Validation RMSE: 0.2454\n",
      "Epoch [1541001/10000000], Training Loss: 0.0664, Validation Loss: 0.0602, Training RMSE: 0.2577, Validation RMSE: 0.2454\n",
      "Epoch [1542001/10000000], Training Loss: 0.0672, Validation Loss: 0.0602, Training RMSE: 0.2591, Validation RMSE: 0.2454\n",
      "Epoch [1543001/10000000], Training Loss: 0.0675, Validation Loss: 0.0602, Training RMSE: 0.2597, Validation RMSE: 0.2454\n",
      "Epoch [1544001/10000000], Training Loss: 0.0669, Validation Loss: 0.0602, Training RMSE: 0.2587, Validation RMSE: 0.2454\n",
      "Epoch [1545001/10000000], Training Loss: 0.0670, Validation Loss: 0.0602, Training RMSE: 0.2588, Validation RMSE: 0.2454\n",
      "Epoch [1546001/10000000], Training Loss: 0.0665, Validation Loss: 0.0602, Training RMSE: 0.2578, Validation RMSE: 0.2454\n",
      "Epoch [1547001/10000000], Training Loss: 0.0671, Validation Loss: 0.0602, Training RMSE: 0.2590, Validation RMSE: 0.2453\n",
      "Epoch [1548001/10000000], Training Loss: 0.0664, Validation Loss: 0.0602, Training RMSE: 0.2577, Validation RMSE: 0.2453\n",
      "Epoch [1549001/10000000], Training Loss: 0.0663, Validation Loss: 0.0602, Training RMSE: 0.2575, Validation RMSE: 0.2453\n",
      "Epoch [1550001/10000000], Training Loss: 0.0665, Validation Loss: 0.0602, Training RMSE: 0.2580, Validation RMSE: 0.2453\n",
      "Epoch [1551001/10000000], Training Loss: 0.0668, Validation Loss: 0.0602, Training RMSE: 0.2585, Validation RMSE: 0.2453\n",
      "Epoch [1552001/10000000], Training Loss: 0.0675, Validation Loss: 0.0602, Training RMSE: 0.2598, Validation RMSE: 0.2453\n",
      "Epoch [1553001/10000000], Training Loss: 0.0668, Validation Loss: 0.0601, Training RMSE: 0.2584, Validation RMSE: 0.2453\n",
      "Epoch [1554001/10000000], Training Loss: 0.0668, Validation Loss: 0.0601, Training RMSE: 0.2585, Validation RMSE: 0.2452\n",
      "Epoch [1555001/10000000], Training Loss: 0.0671, Validation Loss: 0.0601, Training RMSE: 0.2591, Validation RMSE: 0.2452\n",
      "Epoch [1556001/10000000], Training Loss: 0.0664, Validation Loss: 0.0601, Training RMSE: 0.2577, Validation RMSE: 0.2452\n",
      "Epoch [1557001/10000000], Training Loss: 0.0666, Validation Loss: 0.0601, Training RMSE: 0.2582, Validation RMSE: 0.2452\n",
      "Epoch [1558001/10000000], Training Loss: 0.0661, Validation Loss: 0.0601, Training RMSE: 0.2571, Validation RMSE: 0.2452\n",
      "Epoch [1559001/10000000], Training Loss: 0.0657, Validation Loss: 0.0601, Training RMSE: 0.2563, Validation RMSE: 0.2452\n",
      "Epoch [1560001/10000000], Training Loss: 0.0667, Validation Loss: 0.0601, Training RMSE: 0.2582, Validation RMSE: 0.2452\n",
      "Epoch [1561001/10000000], Training Loss: 0.0665, Validation Loss: 0.0601, Training RMSE: 0.2578, Validation RMSE: 0.2451\n",
      "Epoch [1562001/10000000], Training Loss: 0.0675, Validation Loss: 0.0601, Training RMSE: 0.2599, Validation RMSE: 0.2451\n",
      "Epoch [1563001/10000000], Training Loss: 0.0675, Validation Loss: 0.0601, Training RMSE: 0.2599, Validation RMSE: 0.2451\n",
      "Epoch [1564001/10000000], Training Loss: 0.0658, Validation Loss: 0.0601, Training RMSE: 0.2565, Validation RMSE: 0.2451\n",
      "Epoch [1565001/10000000], Training Loss: 0.0674, Validation Loss: 0.0601, Training RMSE: 0.2596, Validation RMSE: 0.2451\n",
      "Epoch [1566001/10000000], Training Loss: 0.0665, Validation Loss: 0.0601, Training RMSE: 0.2578, Validation RMSE: 0.2451\n",
      "Epoch [1567001/10000000], Training Loss: 0.0667, Validation Loss: 0.0600, Training RMSE: 0.2583, Validation RMSE: 0.2450\n",
      "Epoch [1568001/10000000], Training Loss: 0.0662, Validation Loss: 0.0600, Training RMSE: 0.2573, Validation RMSE: 0.2450\n",
      "Epoch [1569001/10000000], Training Loss: 0.0670, Validation Loss: 0.0600, Training RMSE: 0.2589, Validation RMSE: 0.2450\n",
      "Epoch [1570001/10000000], Training Loss: 0.0658, Validation Loss: 0.0600, Training RMSE: 0.2565, Validation RMSE: 0.2450\n",
      "Epoch [1571001/10000000], Training Loss: 0.0659, Validation Loss: 0.0600, Training RMSE: 0.2567, Validation RMSE: 0.2450\n",
      "Epoch [1572001/10000000], Training Loss: 0.0664, Validation Loss: 0.0600, Training RMSE: 0.2577, Validation RMSE: 0.2450\n",
      "Epoch [1573001/10000000], Training Loss: 0.0661, Validation Loss: 0.0600, Training RMSE: 0.2571, Validation RMSE: 0.2450\n",
      "Epoch [1574001/10000000], Training Loss: 0.0662, Validation Loss: 0.0600, Training RMSE: 0.2573, Validation RMSE: 0.2449\n",
      "Epoch [1575001/10000000], Training Loss: 0.0659, Validation Loss: 0.0600, Training RMSE: 0.2567, Validation RMSE: 0.2449\n",
      "Epoch [1576001/10000000], Training Loss: 0.0673, Validation Loss: 0.0600, Training RMSE: 0.2595, Validation RMSE: 0.2449\n",
      "Epoch [1577001/10000000], Training Loss: 0.0663, Validation Loss: 0.0600, Training RMSE: 0.2575, Validation RMSE: 0.2449\n",
      "Epoch [1578001/10000000], Training Loss: 0.0666, Validation Loss: 0.0600, Training RMSE: 0.2581, Validation RMSE: 0.2449\n",
      "Epoch [1579001/10000000], Training Loss: 0.0664, Validation Loss: 0.0600, Training RMSE: 0.2576, Validation RMSE: 0.2449\n",
      "Epoch [1580001/10000000], Training Loss: 0.0664, Validation Loss: 0.0600, Training RMSE: 0.2577, Validation RMSE: 0.2449\n",
      "Epoch [1581001/10000000], Training Loss: 0.0665, Validation Loss: 0.0599, Training RMSE: 0.2579, Validation RMSE: 0.2448\n",
      "Epoch [1582001/10000000], Training Loss: 0.0662, Validation Loss: 0.0599, Training RMSE: 0.2574, Validation RMSE: 0.2448\n",
      "Epoch [1583001/10000000], Training Loss: 0.0668, Validation Loss: 0.0599, Training RMSE: 0.2584, Validation RMSE: 0.2448\n",
      "Epoch [1584001/10000000], Training Loss: 0.0655, Validation Loss: 0.0599, Training RMSE: 0.2559, Validation RMSE: 0.2448\n",
      "Epoch [1585001/10000000], Training Loss: 0.0669, Validation Loss: 0.0599, Training RMSE: 0.2587, Validation RMSE: 0.2448\n",
      "Epoch [1586001/10000000], Training Loss: 0.0665, Validation Loss: 0.0599, Training RMSE: 0.2578, Validation RMSE: 0.2448\n",
      "Epoch [1587001/10000000], Training Loss: 0.0656, Validation Loss: 0.0599, Training RMSE: 0.2561, Validation RMSE: 0.2448\n",
      "Epoch [1588001/10000000], Training Loss: 0.0662, Validation Loss: 0.0599, Training RMSE: 0.2573, Validation RMSE: 0.2447\n",
      "Epoch [1589001/10000000], Training Loss: 0.0657, Validation Loss: 0.0599, Training RMSE: 0.2563, Validation RMSE: 0.2447\n",
      "Epoch [1590001/10000000], Training Loss: 0.0662, Validation Loss: 0.0599, Training RMSE: 0.2573, Validation RMSE: 0.2447\n",
      "Epoch [1591001/10000000], Training Loss: 0.0670, Validation Loss: 0.0599, Training RMSE: 0.2588, Validation RMSE: 0.2447\n",
      "Epoch [1592001/10000000], Training Loss: 0.0666, Validation Loss: 0.0599, Training RMSE: 0.2581, Validation RMSE: 0.2447\n",
      "Epoch [1593001/10000000], Training Loss: 0.0655, Validation Loss: 0.0599, Training RMSE: 0.2560, Validation RMSE: 0.2447\n",
      "Epoch [1594001/10000000], Training Loss: 0.0669, Validation Loss: 0.0599, Training RMSE: 0.2586, Validation RMSE: 0.2447\n",
      "Epoch [1595001/10000000], Training Loss: 0.0661, Validation Loss: 0.0598, Training RMSE: 0.2571, Validation RMSE: 0.2446\n",
      "Epoch [1596001/10000000], Training Loss: 0.0658, Validation Loss: 0.0598, Training RMSE: 0.2564, Validation RMSE: 0.2446\n",
      "Epoch [1597001/10000000], Training Loss: 0.0659, Validation Loss: 0.0598, Training RMSE: 0.2567, Validation RMSE: 0.2446\n",
      "Epoch [1598001/10000000], Training Loss: 0.0659, Validation Loss: 0.0598, Training RMSE: 0.2567, Validation RMSE: 0.2446\n",
      "Epoch [1599001/10000000], Training Loss: 0.0661, Validation Loss: 0.0598, Training RMSE: 0.2572, Validation RMSE: 0.2446\n",
      "Epoch [1600001/10000000], Training Loss: 0.0657, Validation Loss: 0.0598, Training RMSE: 0.2562, Validation RMSE: 0.2446\n",
      "Epoch [1601001/10000000], Training Loss: 0.0659, Validation Loss: 0.0598, Training RMSE: 0.2567, Validation RMSE: 0.2446\n",
      "Epoch [1602001/10000000], Training Loss: 0.0657, Validation Loss: 0.0598, Training RMSE: 0.2562, Validation RMSE: 0.2445\n",
      "Epoch [1603001/10000000], Training Loss: 0.0661, Validation Loss: 0.0598, Training RMSE: 0.2571, Validation RMSE: 0.2445\n",
      "Epoch [1604001/10000000], Training Loss: 0.0657, Validation Loss: 0.0598, Training RMSE: 0.2563, Validation RMSE: 0.2445\n",
      "Epoch [1605001/10000000], Training Loss: 0.0659, Validation Loss: 0.0598, Training RMSE: 0.2567, Validation RMSE: 0.2445\n",
      "Epoch [1606001/10000000], Training Loss: 0.0654, Validation Loss: 0.0598, Training RMSE: 0.2558, Validation RMSE: 0.2445\n",
      "Epoch [1607001/10000000], Training Loss: 0.0658, Validation Loss: 0.0598, Training RMSE: 0.2565, Validation RMSE: 0.2445\n",
      "Epoch [1608001/10000000], Training Loss: 0.0662, Validation Loss: 0.0598, Training RMSE: 0.2572, Validation RMSE: 0.2445\n",
      "Epoch [1609001/10000000], Training Loss: 0.0661, Validation Loss: 0.0598, Training RMSE: 0.2570, Validation RMSE: 0.2444\n",
      "Epoch [1610001/10000000], Training Loss: 0.0657, Validation Loss: 0.0597, Training RMSE: 0.2563, Validation RMSE: 0.2444\n",
      "Epoch [1611001/10000000], Training Loss: 0.0649, Validation Loss: 0.0597, Training RMSE: 0.2547, Validation RMSE: 0.2444\n",
      "Epoch [1612001/10000000], Training Loss: 0.0659, Validation Loss: 0.0597, Training RMSE: 0.2566, Validation RMSE: 0.2444\n",
      "Epoch [1613001/10000000], Training Loss: 0.0656, Validation Loss: 0.0597, Training RMSE: 0.2561, Validation RMSE: 0.2444\n",
      "Epoch [1614001/10000000], Training Loss: 0.0664, Validation Loss: 0.0597, Training RMSE: 0.2577, Validation RMSE: 0.2444\n",
      "Epoch [1615001/10000000], Training Loss: 0.0659, Validation Loss: 0.0597, Training RMSE: 0.2567, Validation RMSE: 0.2444\n",
      "Epoch [1616001/10000000], Training Loss: 0.0659, Validation Loss: 0.0597, Training RMSE: 0.2566, Validation RMSE: 0.2444\n",
      "Epoch [1617001/10000000], Training Loss: 0.0662, Validation Loss: 0.0597, Training RMSE: 0.2574, Validation RMSE: 0.2443\n",
      "Epoch [1618001/10000000], Training Loss: 0.0652, Validation Loss: 0.0597, Training RMSE: 0.2554, Validation RMSE: 0.2443\n",
      "Epoch [1619001/10000000], Training Loss: 0.0659, Validation Loss: 0.0597, Training RMSE: 0.2566, Validation RMSE: 0.2443\n",
      "Epoch [1620001/10000000], Training Loss: 0.0664, Validation Loss: 0.0597, Training RMSE: 0.2576, Validation RMSE: 0.2443\n",
      "Epoch [1621001/10000000], Training Loss: 0.0661, Validation Loss: 0.0597, Training RMSE: 0.2572, Validation RMSE: 0.2443\n",
      "Epoch [1622001/10000000], Training Loss: 0.0644, Validation Loss: 0.0597, Training RMSE: 0.2537, Validation RMSE: 0.2443\n",
      "Epoch [1623001/10000000], Training Loss: 0.0664, Validation Loss: 0.0597, Training RMSE: 0.2578, Validation RMSE: 0.2443\n",
      "Epoch [1624001/10000000], Training Loss: 0.0655, Validation Loss: 0.0597, Training RMSE: 0.2560, Validation RMSE: 0.2442\n",
      "Epoch [1625001/10000000], Training Loss: 0.0666, Validation Loss: 0.0596, Training RMSE: 0.2581, Validation RMSE: 0.2442\n",
      "Epoch [1626001/10000000], Training Loss: 0.0658, Validation Loss: 0.0596, Training RMSE: 0.2564, Validation RMSE: 0.2442\n",
      "Epoch [1627001/10000000], Training Loss: 0.0660, Validation Loss: 0.0596, Training RMSE: 0.2569, Validation RMSE: 0.2442\n",
      "Epoch [1628001/10000000], Training Loss: 0.0659, Validation Loss: 0.0596, Training RMSE: 0.2567, Validation RMSE: 0.2442\n",
      "Epoch [1629001/10000000], Training Loss: 0.0657, Validation Loss: 0.0596, Training RMSE: 0.2564, Validation RMSE: 0.2442\n",
      "Epoch [1630001/10000000], Training Loss: 0.0650, Validation Loss: 0.0596, Training RMSE: 0.2550, Validation RMSE: 0.2442\n",
      "Epoch [1631001/10000000], Training Loss: 0.0661, Validation Loss: 0.0596, Training RMSE: 0.2571, Validation RMSE: 0.2441\n",
      "Epoch [1632001/10000000], Training Loss: 0.0659, Validation Loss: 0.0596, Training RMSE: 0.2566, Validation RMSE: 0.2441\n",
      "Epoch [1633001/10000000], Training Loss: 0.0657, Validation Loss: 0.0596, Training RMSE: 0.2563, Validation RMSE: 0.2441\n",
      "Epoch [1634001/10000000], Training Loss: 0.0654, Validation Loss: 0.0596, Training RMSE: 0.2557, Validation RMSE: 0.2441\n",
      "Epoch [1635001/10000000], Training Loss: 0.0652, Validation Loss: 0.0596, Training RMSE: 0.2554, Validation RMSE: 0.2441\n",
      "Epoch [1636001/10000000], Training Loss: 0.0653, Validation Loss: 0.0596, Training RMSE: 0.2555, Validation RMSE: 0.2441\n",
      "Epoch [1637001/10000000], Training Loss: 0.0654, Validation Loss: 0.0596, Training RMSE: 0.2558, Validation RMSE: 0.2441\n",
      "Epoch [1638001/10000000], Training Loss: 0.0655, Validation Loss: 0.0596, Training RMSE: 0.2559, Validation RMSE: 0.2441\n",
      "Epoch [1639001/10000000], Training Loss: 0.0649, Validation Loss: 0.0596, Training RMSE: 0.2548, Validation RMSE: 0.2440\n",
      "Epoch [1640001/10000000], Training Loss: 0.0661, Validation Loss: 0.0596, Training RMSE: 0.2571, Validation RMSE: 0.2440\n",
      "Epoch [1641001/10000000], Training Loss: 0.0656, Validation Loss: 0.0595, Training RMSE: 0.2561, Validation RMSE: 0.2440\n",
      "Epoch [1642001/10000000], Training Loss: 0.0658, Validation Loss: 0.0595, Training RMSE: 0.2565, Validation RMSE: 0.2440\n",
      "Epoch [1643001/10000000], Training Loss: 0.0656, Validation Loss: 0.0595, Training RMSE: 0.2561, Validation RMSE: 0.2440\n",
      "Epoch [1644001/10000000], Training Loss: 0.0652, Validation Loss: 0.0595, Training RMSE: 0.2554, Validation RMSE: 0.2440\n",
      "Epoch [1645001/10000000], Training Loss: 0.0652, Validation Loss: 0.0595, Training RMSE: 0.2554, Validation RMSE: 0.2440\n",
      "Epoch [1646001/10000000], Training Loss: 0.0649, Validation Loss: 0.0595, Training RMSE: 0.2547, Validation RMSE: 0.2439\n",
      "Epoch [1647001/10000000], Training Loss: 0.0660, Validation Loss: 0.0595, Training RMSE: 0.2569, Validation RMSE: 0.2439\n",
      "Epoch [1648001/10000000], Training Loss: 0.0648, Validation Loss: 0.0595, Training RMSE: 0.2545, Validation RMSE: 0.2439\n",
      "Epoch [1649001/10000000], Training Loss: 0.0653, Validation Loss: 0.0595, Training RMSE: 0.2555, Validation RMSE: 0.2439\n",
      "Epoch [1650001/10000000], Training Loss: 0.0647, Validation Loss: 0.0595, Training RMSE: 0.2544, Validation RMSE: 0.2439\n",
      "Epoch [1651001/10000000], Training Loss: 0.0656, Validation Loss: 0.0595, Training RMSE: 0.2561, Validation RMSE: 0.2439\n",
      "Epoch [1652001/10000000], Training Loss: 0.0653, Validation Loss: 0.0595, Training RMSE: 0.2555, Validation RMSE: 0.2439\n",
      "Epoch [1653001/10000000], Training Loss: 0.0661, Validation Loss: 0.0595, Training RMSE: 0.2571, Validation RMSE: 0.2439\n",
      "Epoch [1654001/10000000], Training Loss: 0.0648, Validation Loss: 0.0595, Training RMSE: 0.2546, Validation RMSE: 0.2438\n",
      "Epoch [1655001/10000000], Training Loss: 0.0655, Validation Loss: 0.0595, Training RMSE: 0.2560, Validation RMSE: 0.2438\n",
      "Epoch [1656001/10000000], Training Loss: 0.0648, Validation Loss: 0.0594, Training RMSE: 0.2546, Validation RMSE: 0.2438\n",
      "Epoch [1657001/10000000], Training Loss: 0.0658, Validation Loss: 0.0594, Training RMSE: 0.2565, Validation RMSE: 0.2438\n",
      "Epoch [1658001/10000000], Training Loss: 0.0659, Validation Loss: 0.0594, Training RMSE: 0.2568, Validation RMSE: 0.2438\n",
      "Epoch [1659001/10000000], Training Loss: 0.0659, Validation Loss: 0.0594, Training RMSE: 0.2567, Validation RMSE: 0.2438\n",
      "Epoch [1660001/10000000], Training Loss: 0.0655, Validation Loss: 0.0594, Training RMSE: 0.2560, Validation RMSE: 0.2438\n",
      "Epoch [1661001/10000000], Training Loss: 0.0645, Validation Loss: 0.0594, Training RMSE: 0.2540, Validation RMSE: 0.2438\n",
      "Epoch [1662001/10000000], Training Loss: 0.0651, Validation Loss: 0.0594, Training RMSE: 0.2552, Validation RMSE: 0.2437\n",
      "Epoch [1663001/10000000], Training Loss: 0.0649, Validation Loss: 0.0594, Training RMSE: 0.2547, Validation RMSE: 0.2437\n",
      "Epoch [1664001/10000000], Training Loss: 0.0653, Validation Loss: 0.0594, Training RMSE: 0.2554, Validation RMSE: 0.2437\n",
      "Epoch [1665001/10000000], Training Loss: 0.0659, Validation Loss: 0.0594, Training RMSE: 0.2567, Validation RMSE: 0.2437\n",
      "Epoch [1666001/10000000], Training Loss: 0.0654, Validation Loss: 0.0594, Training RMSE: 0.2558, Validation RMSE: 0.2437\n",
      "Epoch [1667001/10000000], Training Loss: 0.0651, Validation Loss: 0.0594, Training RMSE: 0.2551, Validation RMSE: 0.2437\n",
      "Epoch [1668001/10000000], Training Loss: 0.0649, Validation Loss: 0.0594, Training RMSE: 0.2548, Validation RMSE: 0.2437\n",
      "Epoch [1669001/10000000], Training Loss: 0.0649, Validation Loss: 0.0594, Training RMSE: 0.2547, Validation RMSE: 0.2437\n",
      "Epoch [1670001/10000000], Training Loss: 0.0656, Validation Loss: 0.0594, Training RMSE: 0.2562, Validation RMSE: 0.2436\n",
      "Epoch [1671001/10000000], Training Loss: 0.0645, Validation Loss: 0.0594, Training RMSE: 0.2539, Validation RMSE: 0.2436\n",
      "Epoch [1672001/10000000], Training Loss: 0.0648, Validation Loss: 0.0593, Training RMSE: 0.2546, Validation RMSE: 0.2436\n",
      "Epoch [1673001/10000000], Training Loss: 0.0657, Validation Loss: 0.0593, Training RMSE: 0.2563, Validation RMSE: 0.2436\n",
      "Epoch [1674001/10000000], Training Loss: 0.0647, Validation Loss: 0.0593, Training RMSE: 0.2543, Validation RMSE: 0.2436\n",
      "Epoch [1675001/10000000], Training Loss: 0.0654, Validation Loss: 0.0593, Training RMSE: 0.2558, Validation RMSE: 0.2436\n",
      "Epoch [1676001/10000000], Training Loss: 0.0652, Validation Loss: 0.0593, Training RMSE: 0.2553, Validation RMSE: 0.2436\n",
      "Epoch [1677001/10000000], Training Loss: 0.0648, Validation Loss: 0.0593, Training RMSE: 0.2545, Validation RMSE: 0.2436\n",
      "Epoch [1678001/10000000], Training Loss: 0.0654, Validation Loss: 0.0593, Training RMSE: 0.2557, Validation RMSE: 0.2435\n",
      "Epoch [1679001/10000000], Training Loss: 0.0649, Validation Loss: 0.0593, Training RMSE: 0.2547, Validation RMSE: 0.2435\n",
      "Epoch [1680001/10000000], Training Loss: 0.0652, Validation Loss: 0.0593, Training RMSE: 0.2554, Validation RMSE: 0.2435\n",
      "Epoch [1681001/10000000], Training Loss: 0.0654, Validation Loss: 0.0593, Training RMSE: 0.2557, Validation RMSE: 0.2435\n",
      "Epoch [1682001/10000000], Training Loss: 0.0652, Validation Loss: 0.0593, Training RMSE: 0.2553, Validation RMSE: 0.2435\n",
      "Epoch [1683001/10000000], Training Loss: 0.0655, Validation Loss: 0.0593, Training RMSE: 0.2560, Validation RMSE: 0.2435\n",
      "Epoch [1684001/10000000], Training Loss: 0.0650, Validation Loss: 0.0593, Training RMSE: 0.2549, Validation RMSE: 0.2435\n",
      "Epoch [1685001/10000000], Training Loss: 0.0649, Validation Loss: 0.0593, Training RMSE: 0.2548, Validation RMSE: 0.2435\n",
      "Epoch [1686001/10000000], Training Loss: 0.0643, Validation Loss: 0.0593, Training RMSE: 0.2535, Validation RMSE: 0.2434\n",
      "Epoch [1687001/10000000], Training Loss: 0.0650, Validation Loss: 0.0593, Training RMSE: 0.2549, Validation RMSE: 0.2434\n",
      "Epoch [1688001/10000000], Training Loss: 0.0654, Validation Loss: 0.0593, Training RMSE: 0.2556, Validation RMSE: 0.2434\n",
      "Epoch [1689001/10000000], Training Loss: 0.0639, Validation Loss: 0.0592, Training RMSE: 0.2527, Validation RMSE: 0.2434\n",
      "Epoch [1690001/10000000], Training Loss: 0.0649, Validation Loss: 0.0592, Training RMSE: 0.2548, Validation RMSE: 0.2434\n",
      "Epoch [1691001/10000000], Training Loss: 0.0658, Validation Loss: 0.0592, Training RMSE: 0.2565, Validation RMSE: 0.2434\n",
      "Epoch [1692001/10000000], Training Loss: 0.0655, Validation Loss: 0.0592, Training RMSE: 0.2559, Validation RMSE: 0.2434\n",
      "Epoch [1693001/10000000], Training Loss: 0.0648, Validation Loss: 0.0592, Training RMSE: 0.2545, Validation RMSE: 0.2434\n",
      "Epoch [1694001/10000000], Training Loss: 0.0646, Validation Loss: 0.0592, Training RMSE: 0.2542, Validation RMSE: 0.2433\n",
      "Epoch [1695001/10000000], Training Loss: 0.0648, Validation Loss: 0.0592, Training RMSE: 0.2545, Validation RMSE: 0.2433\n",
      "Epoch [1696001/10000000], Training Loss: 0.0652, Validation Loss: 0.0592, Training RMSE: 0.2553, Validation RMSE: 0.2433\n",
      "Epoch [1697001/10000000], Training Loss: 0.0644, Validation Loss: 0.0592, Training RMSE: 0.2537, Validation RMSE: 0.2433\n",
      "Epoch [1698001/10000000], Training Loss: 0.0660, Validation Loss: 0.0592, Training RMSE: 0.2569, Validation RMSE: 0.2433\n",
      "Epoch [1699001/10000000], Training Loss: 0.0650, Validation Loss: 0.0592, Training RMSE: 0.2550, Validation RMSE: 0.2433\n",
      "Epoch [1700001/10000000], Training Loss: 0.0650, Validation Loss: 0.0592, Training RMSE: 0.2549, Validation RMSE: 0.2433\n",
      "Epoch [1701001/10000000], Training Loss: 0.0647, Validation Loss: 0.0592, Training RMSE: 0.2545, Validation RMSE: 0.2433\n",
      "Epoch [1702001/10000000], Training Loss: 0.0647, Validation Loss: 0.0592, Training RMSE: 0.2543, Validation RMSE: 0.2432\n",
      "Epoch [1703001/10000000], Training Loss: 0.0653, Validation Loss: 0.0592, Training RMSE: 0.2555, Validation RMSE: 0.2432\n",
      "Epoch [1704001/10000000], Training Loss: 0.0645, Validation Loss: 0.0592, Training RMSE: 0.2540, Validation RMSE: 0.2432\n",
      "Epoch [1705001/10000000], Training Loss: 0.0649, Validation Loss: 0.0592, Training RMSE: 0.2547, Validation RMSE: 0.2432\n",
      "Epoch [1706001/10000000], Training Loss: 0.0652, Validation Loss: 0.0591, Training RMSE: 0.2553, Validation RMSE: 0.2432\n",
      "Epoch [1707001/10000000], Training Loss: 0.0643, Validation Loss: 0.0591, Training RMSE: 0.2537, Validation RMSE: 0.2432\n",
      "Epoch [1708001/10000000], Training Loss: 0.0650, Validation Loss: 0.0591, Training RMSE: 0.2550, Validation RMSE: 0.2432\n",
      "Epoch [1709001/10000000], Training Loss: 0.0649, Validation Loss: 0.0591, Training RMSE: 0.2548, Validation RMSE: 0.2432\n",
      "Epoch [1710001/10000000], Training Loss: 0.0648, Validation Loss: 0.0591, Training RMSE: 0.2546, Validation RMSE: 0.2431\n",
      "Epoch [1711001/10000000], Training Loss: 0.0650, Validation Loss: 0.0591, Training RMSE: 0.2549, Validation RMSE: 0.2431\n",
      "Epoch [1712001/10000000], Training Loss: 0.0651, Validation Loss: 0.0591, Training RMSE: 0.2551, Validation RMSE: 0.2431\n",
      "Epoch [1713001/10000000], Training Loss: 0.0646, Validation Loss: 0.0591, Training RMSE: 0.2542, Validation RMSE: 0.2431\n",
      "Epoch [1714001/10000000], Training Loss: 0.0649, Validation Loss: 0.0591, Training RMSE: 0.2547, Validation RMSE: 0.2431\n",
      "Epoch [1715001/10000000], Training Loss: 0.0637, Validation Loss: 0.0591, Training RMSE: 0.2523, Validation RMSE: 0.2431\n",
      "Epoch [1716001/10000000], Training Loss: 0.0639, Validation Loss: 0.0591, Training RMSE: 0.2527, Validation RMSE: 0.2431\n",
      "Epoch [1717001/10000000], Training Loss: 0.0644, Validation Loss: 0.0591, Training RMSE: 0.2539, Validation RMSE: 0.2431\n",
      "Epoch [1718001/10000000], Training Loss: 0.0648, Validation Loss: 0.0591, Training RMSE: 0.2545, Validation RMSE: 0.2431\n",
      "Epoch [1719001/10000000], Training Loss: 0.0642, Validation Loss: 0.0591, Training RMSE: 0.2533, Validation RMSE: 0.2430\n",
      "Epoch [1720001/10000000], Training Loss: 0.0650, Validation Loss: 0.0591, Training RMSE: 0.2549, Validation RMSE: 0.2430\n",
      "Epoch [1721001/10000000], Training Loss: 0.0646, Validation Loss: 0.0591, Training RMSE: 0.2542, Validation RMSE: 0.2430\n",
      "Epoch [1722001/10000000], Training Loss: 0.0644, Validation Loss: 0.0591, Training RMSE: 0.2538, Validation RMSE: 0.2430\n",
      "Epoch [1723001/10000000], Training Loss: 0.0655, Validation Loss: 0.0590, Training RMSE: 0.2558, Validation RMSE: 0.2430\n",
      "Epoch [1724001/10000000], Training Loss: 0.0645, Validation Loss: 0.0590, Training RMSE: 0.2539, Validation RMSE: 0.2430\n",
      "Epoch [1725001/10000000], Training Loss: 0.0642, Validation Loss: 0.0590, Training RMSE: 0.2534, Validation RMSE: 0.2430\n",
      "Epoch [1726001/10000000], Training Loss: 0.0647, Validation Loss: 0.0590, Training RMSE: 0.2543, Validation RMSE: 0.2430\n",
      "Epoch [1727001/10000000], Training Loss: 0.0656, Validation Loss: 0.0590, Training RMSE: 0.2561, Validation RMSE: 0.2429\n",
      "Epoch [1728001/10000000], Training Loss: 0.0645, Validation Loss: 0.0590, Training RMSE: 0.2540, Validation RMSE: 0.2429\n",
      "Epoch [1729001/10000000], Training Loss: 0.0647, Validation Loss: 0.0590, Training RMSE: 0.2543, Validation RMSE: 0.2429\n",
      "Epoch [1730001/10000000], Training Loss: 0.0647, Validation Loss: 0.0590, Training RMSE: 0.2543, Validation RMSE: 0.2429\n",
      "Epoch [1731001/10000000], Training Loss: 0.0646, Validation Loss: 0.0590, Training RMSE: 0.2542, Validation RMSE: 0.2429\n",
      "Epoch [1732001/10000000], Training Loss: 0.0644, Validation Loss: 0.0590, Training RMSE: 0.2538, Validation RMSE: 0.2429\n",
      "Epoch [1733001/10000000], Training Loss: 0.0651, Validation Loss: 0.0590, Training RMSE: 0.2551, Validation RMSE: 0.2429\n",
      "Epoch [1734001/10000000], Training Loss: 0.0647, Validation Loss: 0.0590, Training RMSE: 0.2544, Validation RMSE: 0.2429\n",
      "Epoch [1735001/10000000], Training Loss: 0.0643, Validation Loss: 0.0590, Training RMSE: 0.2535, Validation RMSE: 0.2429\n",
      "Epoch [1736001/10000000], Training Loss: 0.0643, Validation Loss: 0.0590, Training RMSE: 0.2536, Validation RMSE: 0.2428\n",
      "Epoch [1737001/10000000], Training Loss: 0.0644, Validation Loss: 0.0590, Training RMSE: 0.2538, Validation RMSE: 0.2428\n",
      "Epoch [1738001/10000000], Training Loss: 0.0641, Validation Loss: 0.0590, Training RMSE: 0.2531, Validation RMSE: 0.2428\n",
      "Epoch [1739001/10000000], Training Loss: 0.0653, Validation Loss: 0.0590, Training RMSE: 0.2556, Validation RMSE: 0.2428\n",
      "Epoch [1740001/10000000], Training Loss: 0.0645, Validation Loss: 0.0590, Training RMSE: 0.2539, Validation RMSE: 0.2428\n",
      "Epoch [1741001/10000000], Training Loss: 0.0643, Validation Loss: 0.0589, Training RMSE: 0.2535, Validation RMSE: 0.2428\n",
      "Epoch [1742001/10000000], Training Loss: 0.0634, Validation Loss: 0.0589, Training RMSE: 0.2517, Validation RMSE: 0.2428\n",
      "Epoch [1743001/10000000], Training Loss: 0.0639, Validation Loss: 0.0589, Training RMSE: 0.2528, Validation RMSE: 0.2428\n",
      "Epoch [1744001/10000000], Training Loss: 0.0646, Validation Loss: 0.0589, Training RMSE: 0.2541, Validation RMSE: 0.2428\n",
      "Epoch [1745001/10000000], Training Loss: 0.0643, Validation Loss: 0.0589, Training RMSE: 0.2537, Validation RMSE: 0.2427\n",
      "Epoch [1746001/10000000], Training Loss: 0.0640, Validation Loss: 0.0589, Training RMSE: 0.2530, Validation RMSE: 0.2427\n",
      "Epoch [1747001/10000000], Training Loss: 0.0646, Validation Loss: 0.0589, Training RMSE: 0.2541, Validation RMSE: 0.2427\n",
      "Epoch [1748001/10000000], Training Loss: 0.0643, Validation Loss: 0.0589, Training RMSE: 0.2535, Validation RMSE: 0.2427\n",
      "Epoch [1749001/10000000], Training Loss: 0.0640, Validation Loss: 0.0589, Training RMSE: 0.2530, Validation RMSE: 0.2427\n",
      "Epoch [1750001/10000000], Training Loss: 0.0645, Validation Loss: 0.0589, Training RMSE: 0.2540, Validation RMSE: 0.2427\n",
      "Epoch [1751001/10000000], Training Loss: 0.0644, Validation Loss: 0.0589, Training RMSE: 0.2537, Validation RMSE: 0.2427\n",
      "Epoch [1752001/10000000], Training Loss: 0.0638, Validation Loss: 0.0589, Training RMSE: 0.2527, Validation RMSE: 0.2427\n",
      "Epoch [1753001/10000000], Training Loss: 0.0635, Validation Loss: 0.0589, Training RMSE: 0.2519, Validation RMSE: 0.2427\n",
      "Epoch [1754001/10000000], Training Loss: 0.0648, Validation Loss: 0.0589, Training RMSE: 0.2546, Validation RMSE: 0.2426\n",
      "Epoch [1755001/10000000], Training Loss: 0.0638, Validation Loss: 0.0589, Training RMSE: 0.2525, Validation RMSE: 0.2426\n",
      "Epoch [1756001/10000000], Training Loss: 0.0645, Validation Loss: 0.0589, Training RMSE: 0.2540, Validation RMSE: 0.2426\n",
      "Epoch [1757001/10000000], Training Loss: 0.0639, Validation Loss: 0.0589, Training RMSE: 0.2528, Validation RMSE: 0.2426\n",
      "Epoch [1758001/10000000], Training Loss: 0.0640, Validation Loss: 0.0589, Training RMSE: 0.2530, Validation RMSE: 0.2426\n",
      "Epoch [1759001/10000000], Training Loss: 0.0634, Validation Loss: 0.0588, Training RMSE: 0.2518, Validation RMSE: 0.2426\n",
      "Epoch [1760001/10000000], Training Loss: 0.0634, Validation Loss: 0.0588, Training RMSE: 0.2518, Validation RMSE: 0.2426\n",
      "Epoch [1761001/10000000], Training Loss: 0.0647, Validation Loss: 0.0588, Training RMSE: 0.2544, Validation RMSE: 0.2426\n",
      "Epoch [1762001/10000000], Training Loss: 0.0641, Validation Loss: 0.0588, Training RMSE: 0.2532, Validation RMSE: 0.2426\n",
      "Epoch [1763001/10000000], Training Loss: 0.0640, Validation Loss: 0.0588, Training RMSE: 0.2529, Validation RMSE: 0.2425\n",
      "Epoch [1764001/10000000], Training Loss: 0.0642, Validation Loss: 0.0588, Training RMSE: 0.2534, Validation RMSE: 0.2425\n",
      "Epoch [1765001/10000000], Training Loss: 0.0639, Validation Loss: 0.0588, Training RMSE: 0.2529, Validation RMSE: 0.2425\n",
      "Epoch [1766001/10000000], Training Loss: 0.0633, Validation Loss: 0.0588, Training RMSE: 0.2515, Validation RMSE: 0.2425\n",
      "Epoch [1767001/10000000], Training Loss: 0.0644, Validation Loss: 0.0588, Training RMSE: 0.2538, Validation RMSE: 0.2425\n",
      "Epoch [1768001/10000000], Training Loss: 0.0640, Validation Loss: 0.0588, Training RMSE: 0.2529, Validation RMSE: 0.2425\n",
      "Epoch [1769001/10000000], Training Loss: 0.0642, Validation Loss: 0.0588, Training RMSE: 0.2535, Validation RMSE: 0.2425\n",
      "Epoch [1770001/10000000], Training Loss: 0.0643, Validation Loss: 0.0588, Training RMSE: 0.2536, Validation RMSE: 0.2425\n",
      "Epoch [1771001/10000000], Training Loss: 0.0648, Validation Loss: 0.0588, Training RMSE: 0.2546, Validation RMSE: 0.2425\n",
      "Epoch [1772001/10000000], Training Loss: 0.0632, Validation Loss: 0.0588, Training RMSE: 0.2514, Validation RMSE: 0.2424\n",
      "Epoch [1773001/10000000], Training Loss: 0.0634, Validation Loss: 0.0588, Training RMSE: 0.2518, Validation RMSE: 0.2424\n",
      "Epoch [1774001/10000000], Training Loss: 0.0635, Validation Loss: 0.0588, Training RMSE: 0.2520, Validation RMSE: 0.2424\n",
      "Epoch [1775001/10000000], Training Loss: 0.0639, Validation Loss: 0.0588, Training RMSE: 0.2529, Validation RMSE: 0.2424\n",
      "Epoch [1776001/10000000], Training Loss: 0.0635, Validation Loss: 0.0588, Training RMSE: 0.2519, Validation RMSE: 0.2424\n",
      "Epoch [1777001/10000000], Training Loss: 0.0640, Validation Loss: 0.0588, Training RMSE: 0.2530, Validation RMSE: 0.2424\n",
      "Epoch [1778001/10000000], Training Loss: 0.0637, Validation Loss: 0.0587, Training RMSE: 0.2525, Validation RMSE: 0.2424\n",
      "Epoch [1779001/10000000], Training Loss: 0.0639, Validation Loss: 0.0587, Training RMSE: 0.2528, Validation RMSE: 0.2424\n",
      "Epoch [1780001/10000000], Training Loss: 0.0643, Validation Loss: 0.0587, Training RMSE: 0.2535, Validation RMSE: 0.2424\n",
      "Epoch [1781001/10000000], Training Loss: 0.0638, Validation Loss: 0.0587, Training RMSE: 0.2526, Validation RMSE: 0.2424\n",
      "Epoch [1782001/10000000], Training Loss: 0.0636, Validation Loss: 0.0587, Training RMSE: 0.2522, Validation RMSE: 0.2423\n",
      "Epoch [1783001/10000000], Training Loss: 0.0634, Validation Loss: 0.0587, Training RMSE: 0.2519, Validation RMSE: 0.2423\n",
      "Epoch [1784001/10000000], Training Loss: 0.0631, Validation Loss: 0.0587, Training RMSE: 0.2513, Validation RMSE: 0.2423\n",
      "Epoch [1785001/10000000], Training Loss: 0.0637, Validation Loss: 0.0587, Training RMSE: 0.2524, Validation RMSE: 0.2423\n",
      "Epoch [1786001/10000000], Training Loss: 0.0645, Validation Loss: 0.0587, Training RMSE: 0.2539, Validation RMSE: 0.2423\n",
      "Epoch [1787001/10000000], Training Loss: 0.0643, Validation Loss: 0.0587, Training RMSE: 0.2536, Validation RMSE: 0.2423\n",
      "Epoch [1788001/10000000], Training Loss: 0.0636, Validation Loss: 0.0587, Training RMSE: 0.2521, Validation RMSE: 0.2423\n",
      "Epoch [1789001/10000000], Training Loss: 0.0646, Validation Loss: 0.0587, Training RMSE: 0.2541, Validation RMSE: 0.2423\n",
      "Epoch [1790001/10000000], Training Loss: 0.0639, Validation Loss: 0.0587, Training RMSE: 0.2527, Validation RMSE: 0.2423\n",
      "Epoch [1791001/10000000], Training Loss: 0.0635, Validation Loss: 0.0587, Training RMSE: 0.2519, Validation RMSE: 0.2422\n",
      "Epoch [1792001/10000000], Training Loss: 0.0636, Validation Loss: 0.0587, Training RMSE: 0.2523, Validation RMSE: 0.2422\n",
      "Epoch [1793001/10000000], Training Loss: 0.0636, Validation Loss: 0.0587, Training RMSE: 0.2523, Validation RMSE: 0.2422\n",
      "Epoch [1794001/10000000], Training Loss: 0.0631, Validation Loss: 0.0587, Training RMSE: 0.2512, Validation RMSE: 0.2422\n",
      "Epoch [1795001/10000000], Training Loss: 0.0637, Validation Loss: 0.0587, Training RMSE: 0.2524, Validation RMSE: 0.2422\n",
      "Epoch [1796001/10000000], Training Loss: 0.0639, Validation Loss: 0.0587, Training RMSE: 0.2528, Validation RMSE: 0.2422\n",
      "Epoch [1797001/10000000], Training Loss: 0.0630, Validation Loss: 0.0587, Training RMSE: 0.2511, Validation RMSE: 0.2422\n",
      "Epoch [1798001/10000000], Training Loss: 0.0637, Validation Loss: 0.0586, Training RMSE: 0.2524, Validation RMSE: 0.2422\n",
      "Epoch [1799001/10000000], Training Loss: 0.0636, Validation Loss: 0.0586, Training RMSE: 0.2521, Validation RMSE: 0.2422\n",
      "Epoch [1800001/10000000], Training Loss: 0.0634, Validation Loss: 0.0586, Training RMSE: 0.2518, Validation RMSE: 0.2422\n",
      "Epoch [1801001/10000000], Training Loss: 0.0640, Validation Loss: 0.0586, Training RMSE: 0.2529, Validation RMSE: 0.2421\n",
      "Epoch [1802001/10000000], Training Loss: 0.0630, Validation Loss: 0.0586, Training RMSE: 0.2511, Validation RMSE: 0.2421\n",
      "Epoch [1803001/10000000], Training Loss: 0.0631, Validation Loss: 0.0586, Training RMSE: 0.2512, Validation RMSE: 0.2421\n",
      "Epoch [1804001/10000000], Training Loss: 0.0638, Validation Loss: 0.0586, Training RMSE: 0.2526, Validation RMSE: 0.2421\n",
      "Epoch [1805001/10000000], Training Loss: 0.0631, Validation Loss: 0.0586, Training RMSE: 0.2512, Validation RMSE: 0.2421\n",
      "Epoch [1806001/10000000], Training Loss: 0.0641, Validation Loss: 0.0586, Training RMSE: 0.2531, Validation RMSE: 0.2421\n",
      "Epoch [1807001/10000000], Training Loss: 0.0638, Validation Loss: 0.0586, Training RMSE: 0.2526, Validation RMSE: 0.2421\n",
      "Epoch [1808001/10000000], Training Loss: 0.0631, Validation Loss: 0.0586, Training RMSE: 0.2512, Validation RMSE: 0.2421\n",
      "Epoch [1809001/10000000], Training Loss: 0.0632, Validation Loss: 0.0586, Training RMSE: 0.2514, Validation RMSE: 0.2421\n",
      "Epoch [1810001/10000000], Training Loss: 0.0632, Validation Loss: 0.0586, Training RMSE: 0.2514, Validation RMSE: 0.2420\n",
      "Epoch [1811001/10000000], Training Loss: 0.0631, Validation Loss: 0.0586, Training RMSE: 0.2512, Validation RMSE: 0.2420\n",
      "Epoch [1812001/10000000], Training Loss: 0.0639, Validation Loss: 0.0586, Training RMSE: 0.2528, Validation RMSE: 0.2420\n",
      "Epoch [1813001/10000000], Training Loss: 0.0633, Validation Loss: 0.0586, Training RMSE: 0.2516, Validation RMSE: 0.2420\n",
      "Epoch [1814001/10000000], Training Loss: 0.0633, Validation Loss: 0.0586, Training RMSE: 0.2516, Validation RMSE: 0.2420\n",
      "Epoch [1815001/10000000], Training Loss: 0.0633, Validation Loss: 0.0586, Training RMSE: 0.2516, Validation RMSE: 0.2420\n",
      "Epoch [1816001/10000000], Training Loss: 0.0634, Validation Loss: 0.0586, Training RMSE: 0.2518, Validation RMSE: 0.2420\n",
      "Epoch [1817001/10000000], Training Loss: 0.0636, Validation Loss: 0.0586, Training RMSE: 0.2522, Validation RMSE: 0.2420\n",
      "Epoch [1818001/10000000], Training Loss: 0.0637, Validation Loss: 0.0585, Training RMSE: 0.2524, Validation RMSE: 0.2420\n",
      "Epoch [1819001/10000000], Training Loss: 0.0640, Validation Loss: 0.0585, Training RMSE: 0.2530, Validation RMSE: 0.2420\n",
      "Epoch [1820001/10000000], Training Loss: 0.0631, Validation Loss: 0.0585, Training RMSE: 0.2512, Validation RMSE: 0.2419\n",
      "Epoch [1821001/10000000], Training Loss: 0.0629, Validation Loss: 0.0585, Training RMSE: 0.2508, Validation RMSE: 0.2419\n",
      "Epoch [1822001/10000000], Training Loss: 0.0636, Validation Loss: 0.0585, Training RMSE: 0.2521, Validation RMSE: 0.2419\n",
      "Epoch [1823001/10000000], Training Loss: 0.0639, Validation Loss: 0.0585, Training RMSE: 0.2529, Validation RMSE: 0.2419\n",
      "Epoch [1824001/10000000], Training Loss: 0.0640, Validation Loss: 0.0585, Training RMSE: 0.2529, Validation RMSE: 0.2419\n",
      "Epoch [1825001/10000000], Training Loss: 0.0627, Validation Loss: 0.0585, Training RMSE: 0.2503, Validation RMSE: 0.2419\n",
      "Epoch [1826001/10000000], Training Loss: 0.0632, Validation Loss: 0.0585, Training RMSE: 0.2514, Validation RMSE: 0.2419\n",
      "Epoch [1827001/10000000], Training Loss: 0.0629, Validation Loss: 0.0585, Training RMSE: 0.2507, Validation RMSE: 0.2419\n",
      "Epoch [1828001/10000000], Training Loss: 0.0624, Validation Loss: 0.0585, Training RMSE: 0.2498, Validation RMSE: 0.2419\n",
      "Epoch [1829001/10000000], Training Loss: 0.0631, Validation Loss: 0.0585, Training RMSE: 0.2513, Validation RMSE: 0.2419\n",
      "Epoch [1830001/10000000], Training Loss: 0.0636, Validation Loss: 0.0585, Training RMSE: 0.2521, Validation RMSE: 0.2419\n",
      "Epoch [1831001/10000000], Training Loss: 0.0627, Validation Loss: 0.0585, Training RMSE: 0.2504, Validation RMSE: 0.2418\n",
      "Epoch [1832001/10000000], Training Loss: 0.0631, Validation Loss: 0.0585, Training RMSE: 0.2513, Validation RMSE: 0.2418\n",
      "Epoch [1833001/10000000], Training Loss: 0.0630, Validation Loss: 0.0585, Training RMSE: 0.2511, Validation RMSE: 0.2418\n",
      "Epoch [1834001/10000000], Training Loss: 0.0633, Validation Loss: 0.0585, Training RMSE: 0.2516, Validation RMSE: 0.2418\n",
      "Epoch [1835001/10000000], Training Loss: 0.0636, Validation Loss: 0.0585, Training RMSE: 0.2521, Validation RMSE: 0.2418\n",
      "Epoch [1836001/10000000], Training Loss: 0.0631, Validation Loss: 0.0585, Training RMSE: 0.2511, Validation RMSE: 0.2418\n",
      "Epoch [1837001/10000000], Training Loss: 0.0624, Validation Loss: 0.0585, Training RMSE: 0.2499, Validation RMSE: 0.2418\n",
      "Epoch [1838001/10000000], Training Loss: 0.0631, Validation Loss: 0.0585, Training RMSE: 0.2513, Validation RMSE: 0.2418\n",
      "Epoch [1839001/10000000], Training Loss: 0.0632, Validation Loss: 0.0584, Training RMSE: 0.2513, Validation RMSE: 0.2418\n",
      "Epoch [1840001/10000000], Training Loss: 0.0633, Validation Loss: 0.0584, Training RMSE: 0.2517, Validation RMSE: 0.2418\n",
      "Epoch [1841001/10000000], Training Loss: 0.0632, Validation Loss: 0.0584, Training RMSE: 0.2514, Validation RMSE: 0.2417\n",
      "Epoch [1842001/10000000], Training Loss: 0.0628, Validation Loss: 0.0584, Training RMSE: 0.2507, Validation RMSE: 0.2417\n",
      "Epoch [1843001/10000000], Training Loss: 0.0626, Validation Loss: 0.0584, Training RMSE: 0.2501, Validation RMSE: 0.2417\n",
      "Epoch [1844001/10000000], Training Loss: 0.0632, Validation Loss: 0.0584, Training RMSE: 0.2515, Validation RMSE: 0.2417\n",
      "Epoch [1845001/10000000], Training Loss: 0.0627, Validation Loss: 0.0584, Training RMSE: 0.2505, Validation RMSE: 0.2417\n",
      "Epoch [1846001/10000000], Training Loss: 0.0630, Validation Loss: 0.0584, Training RMSE: 0.2510, Validation RMSE: 0.2417\n",
      "Epoch [1847001/10000000], Training Loss: 0.0628, Validation Loss: 0.0584, Training RMSE: 0.2507, Validation RMSE: 0.2417\n",
      "Epoch [1848001/10000000], Training Loss: 0.0629, Validation Loss: 0.0584, Training RMSE: 0.2508, Validation RMSE: 0.2417\n",
      "Epoch [1849001/10000000], Training Loss: 0.0636, Validation Loss: 0.0584, Training RMSE: 0.2523, Validation RMSE: 0.2417\n",
      "Epoch [1850001/10000000], Training Loss: 0.0626, Validation Loss: 0.0584, Training RMSE: 0.2502, Validation RMSE: 0.2417\n",
      "Epoch [1851001/10000000], Training Loss: 0.0620, Validation Loss: 0.0584, Training RMSE: 0.2489, Validation RMSE: 0.2416\n",
      "Epoch [1852001/10000000], Training Loss: 0.0626, Validation Loss: 0.0584, Training RMSE: 0.2502, Validation RMSE: 0.2416\n",
      "Epoch [1853001/10000000], Training Loss: 0.0618, Validation Loss: 0.0584, Training RMSE: 0.2486, Validation RMSE: 0.2416\n",
      "Epoch [1854001/10000000], Training Loss: 0.0633, Validation Loss: 0.0584, Training RMSE: 0.2515, Validation RMSE: 0.2416\n",
      "Epoch [1855001/10000000], Training Loss: 0.0631, Validation Loss: 0.0584, Training RMSE: 0.2513, Validation RMSE: 0.2416\n",
      "Epoch [1856001/10000000], Training Loss: 0.0625, Validation Loss: 0.0584, Training RMSE: 0.2499, Validation RMSE: 0.2416\n",
      "Epoch [1857001/10000000], Training Loss: 0.0634, Validation Loss: 0.0584, Training RMSE: 0.2518, Validation RMSE: 0.2416\n",
      "Epoch [1858001/10000000], Training Loss: 0.0630, Validation Loss: 0.0584, Training RMSE: 0.2510, Validation RMSE: 0.2416\n",
      "Epoch [1859001/10000000], Training Loss: 0.0632, Validation Loss: 0.0584, Training RMSE: 0.2514, Validation RMSE: 0.2416\n",
      "Epoch [1860001/10000000], Training Loss: 0.0633, Validation Loss: 0.0584, Training RMSE: 0.2515, Validation RMSE: 0.2416\n",
      "Epoch [1861001/10000000], Training Loss: 0.0627, Validation Loss: 0.0583, Training RMSE: 0.2504, Validation RMSE: 0.2416\n",
      "Epoch [1862001/10000000], Training Loss: 0.0628, Validation Loss: 0.0583, Training RMSE: 0.2507, Validation RMSE: 0.2415\n",
      "Epoch [1863001/10000000], Training Loss: 0.0638, Validation Loss: 0.0583, Training RMSE: 0.2526, Validation RMSE: 0.2415\n",
      "Epoch [1864001/10000000], Training Loss: 0.0623, Validation Loss: 0.0583, Training RMSE: 0.2497, Validation RMSE: 0.2415\n",
      "Epoch [1865001/10000000], Training Loss: 0.0635, Validation Loss: 0.0583, Training RMSE: 0.2520, Validation RMSE: 0.2415\n",
      "Epoch [1866001/10000000], Training Loss: 0.0630, Validation Loss: 0.0583, Training RMSE: 0.2511, Validation RMSE: 0.2415\n",
      "Epoch [1867001/10000000], Training Loss: 0.0625, Validation Loss: 0.0583, Training RMSE: 0.2501, Validation RMSE: 0.2415\n",
      "Epoch [1868001/10000000], Training Loss: 0.0632, Validation Loss: 0.0583, Training RMSE: 0.2514, Validation RMSE: 0.2415\n",
      "Epoch [1869001/10000000], Training Loss: 0.0628, Validation Loss: 0.0583, Training RMSE: 0.2506, Validation RMSE: 0.2415\n",
      "Epoch [1870001/10000000], Training Loss: 0.0634, Validation Loss: 0.0583, Training RMSE: 0.2519, Validation RMSE: 0.2415\n",
      "Epoch [1871001/10000000], Training Loss: 0.0625, Validation Loss: 0.0583, Training RMSE: 0.2501, Validation RMSE: 0.2415\n",
      "Epoch [1872001/10000000], Training Loss: 0.0629, Validation Loss: 0.0583, Training RMSE: 0.2509, Validation RMSE: 0.2415\n",
      "Epoch [1873001/10000000], Training Loss: 0.0620, Validation Loss: 0.0583, Training RMSE: 0.2490, Validation RMSE: 0.2414\n",
      "Epoch [1874001/10000000], Training Loss: 0.0628, Validation Loss: 0.0583, Training RMSE: 0.2506, Validation RMSE: 0.2414\n",
      "Epoch [1875001/10000000], Training Loss: 0.0612, Validation Loss: 0.0583, Training RMSE: 0.2475, Validation RMSE: 0.2414\n",
      "Epoch [1876001/10000000], Training Loss: 0.0629, Validation Loss: 0.0583, Training RMSE: 0.2508, Validation RMSE: 0.2414\n",
      "Epoch [1877001/10000000], Training Loss: 0.0628, Validation Loss: 0.0583, Training RMSE: 0.2505, Validation RMSE: 0.2414\n",
      "Epoch [1878001/10000000], Training Loss: 0.0626, Validation Loss: 0.0583, Training RMSE: 0.2502, Validation RMSE: 0.2414\n",
      "Epoch [1879001/10000000], Training Loss: 0.0626, Validation Loss: 0.0583, Training RMSE: 0.2503, Validation RMSE: 0.2414\n",
      "Epoch [1880001/10000000], Training Loss: 0.0630, Validation Loss: 0.0583, Training RMSE: 0.2510, Validation RMSE: 0.2414\n",
      "Epoch [1881001/10000000], Training Loss: 0.0626, Validation Loss: 0.0583, Training RMSE: 0.2501, Validation RMSE: 0.2414\n",
      "Epoch [1882001/10000000], Training Loss: 0.0623, Validation Loss: 0.0583, Training RMSE: 0.2496, Validation RMSE: 0.2414\n",
      "Epoch [1883001/10000000], Training Loss: 0.0622, Validation Loss: 0.0583, Training RMSE: 0.2495, Validation RMSE: 0.2414\n",
      "Epoch [1884001/10000000], Training Loss: 0.0631, Validation Loss: 0.0582, Training RMSE: 0.2513, Validation RMSE: 0.2413\n",
      "Epoch [1885001/10000000], Training Loss: 0.0629, Validation Loss: 0.0582, Training RMSE: 0.2509, Validation RMSE: 0.2413\n",
      "Epoch [1886001/10000000], Training Loss: 0.0626, Validation Loss: 0.0582, Training RMSE: 0.2502, Validation RMSE: 0.2413\n",
      "Epoch [1887001/10000000], Training Loss: 0.0627, Validation Loss: 0.0582, Training RMSE: 0.2504, Validation RMSE: 0.2413\n",
      "Epoch [1888001/10000000], Training Loss: 0.0629, Validation Loss: 0.0582, Training RMSE: 0.2509, Validation RMSE: 0.2413\n",
      "Epoch [1889001/10000000], Training Loss: 0.0627, Validation Loss: 0.0582, Training RMSE: 0.2505, Validation RMSE: 0.2413\n",
      "Epoch [1890001/10000000], Training Loss: 0.0631, Validation Loss: 0.0582, Training RMSE: 0.2511, Validation RMSE: 0.2413\n",
      "Epoch [1891001/10000000], Training Loss: 0.0632, Validation Loss: 0.0582, Training RMSE: 0.2513, Validation RMSE: 0.2413\n",
      "Epoch [1892001/10000000], Training Loss: 0.0625, Validation Loss: 0.0582, Training RMSE: 0.2499, Validation RMSE: 0.2413\n",
      "Epoch [1893001/10000000], Training Loss: 0.0622, Validation Loss: 0.0582, Training RMSE: 0.2495, Validation RMSE: 0.2413\n",
      "Epoch [1894001/10000000], Training Loss: 0.0621, Validation Loss: 0.0582, Training RMSE: 0.2492, Validation RMSE: 0.2413\n",
      "Epoch [1895001/10000000], Training Loss: 0.0623, Validation Loss: 0.0582, Training RMSE: 0.2497, Validation RMSE: 0.2412\n",
      "Epoch [1896001/10000000], Training Loss: 0.0624, Validation Loss: 0.0582, Training RMSE: 0.2498, Validation RMSE: 0.2412\n",
      "Epoch [1897001/10000000], Training Loss: 0.0623, Validation Loss: 0.0582, Training RMSE: 0.2496, Validation RMSE: 0.2412\n",
      "Epoch [1898001/10000000], Training Loss: 0.0621, Validation Loss: 0.0582, Training RMSE: 0.2492, Validation RMSE: 0.2412\n",
      "Epoch [1899001/10000000], Training Loss: 0.0622, Validation Loss: 0.0582, Training RMSE: 0.2495, Validation RMSE: 0.2412\n",
      "Epoch [1900001/10000000], Training Loss: 0.0628, Validation Loss: 0.0582, Training RMSE: 0.2506, Validation RMSE: 0.2412\n",
      "Epoch [1901001/10000000], Training Loss: 0.0618, Validation Loss: 0.0582, Training RMSE: 0.2485, Validation RMSE: 0.2412\n",
      "Epoch [1902001/10000000], Training Loss: 0.0627, Validation Loss: 0.0582, Training RMSE: 0.2504, Validation RMSE: 0.2412\n",
      "Epoch [1903001/10000000], Training Loss: 0.0622, Validation Loss: 0.0582, Training RMSE: 0.2495, Validation RMSE: 0.2412\n",
      "Epoch [1904001/10000000], Training Loss: 0.0619, Validation Loss: 0.0582, Training RMSE: 0.2489, Validation RMSE: 0.2412\n",
      "Epoch [1905001/10000000], Training Loss: 0.0619, Validation Loss: 0.0582, Training RMSE: 0.2488, Validation RMSE: 0.2412\n",
      "Epoch [1906001/10000000], Training Loss: 0.0622, Validation Loss: 0.0582, Training RMSE: 0.2494, Validation RMSE: 0.2412\n",
      "Epoch [1907001/10000000], Training Loss: 0.0621, Validation Loss: 0.0581, Training RMSE: 0.2491, Validation RMSE: 0.2411\n",
      "Epoch [1908001/10000000], Training Loss: 0.0623, Validation Loss: 0.0581, Training RMSE: 0.2496, Validation RMSE: 0.2411\n",
      "Epoch [1909001/10000000], Training Loss: 0.0623, Validation Loss: 0.0581, Training RMSE: 0.2496, Validation RMSE: 0.2411\n",
      "Epoch [1910001/10000000], Training Loss: 0.0620, Validation Loss: 0.0581, Training RMSE: 0.2491, Validation RMSE: 0.2411\n",
      "Epoch [1911001/10000000], Training Loss: 0.0628, Validation Loss: 0.0581, Training RMSE: 0.2505, Validation RMSE: 0.2411\n",
      "Epoch [1912001/10000000], Training Loss: 0.0627, Validation Loss: 0.0581, Training RMSE: 0.2504, Validation RMSE: 0.2411\n",
      "Epoch [1913001/10000000], Training Loss: 0.0625, Validation Loss: 0.0581, Training RMSE: 0.2501, Validation RMSE: 0.2411\n",
      "Epoch [1914001/10000000], Training Loss: 0.0622, Validation Loss: 0.0581, Training RMSE: 0.2494, Validation RMSE: 0.2411\n",
      "Epoch [1915001/10000000], Training Loss: 0.0622, Validation Loss: 0.0581, Training RMSE: 0.2494, Validation RMSE: 0.2411\n",
      "Epoch [1916001/10000000], Training Loss: 0.0616, Validation Loss: 0.0581, Training RMSE: 0.2483, Validation RMSE: 0.2411\n",
      "Epoch [1917001/10000000], Training Loss: 0.0626, Validation Loss: 0.0581, Training RMSE: 0.2503, Validation RMSE: 0.2411\n",
      "Epoch [1918001/10000000], Training Loss: 0.0623, Validation Loss: 0.0581, Training RMSE: 0.2495, Validation RMSE: 0.2410\n",
      "Epoch [1919001/10000000], Training Loss: 0.0614, Validation Loss: 0.0581, Training RMSE: 0.2478, Validation RMSE: 0.2410\n",
      "Epoch [1920001/10000000], Training Loss: 0.0625, Validation Loss: 0.0581, Training RMSE: 0.2500, Validation RMSE: 0.2410\n",
      "Epoch [1921001/10000000], Training Loss: 0.0624, Validation Loss: 0.0581, Training RMSE: 0.2497, Validation RMSE: 0.2410\n",
      "Epoch [1922001/10000000], Training Loss: 0.0618, Validation Loss: 0.0581, Training RMSE: 0.2486, Validation RMSE: 0.2410\n",
      "Epoch [1923001/10000000], Training Loss: 0.0624, Validation Loss: 0.0581, Training RMSE: 0.2499, Validation RMSE: 0.2410\n",
      "Epoch [1924001/10000000], Training Loss: 0.0618, Validation Loss: 0.0581, Training RMSE: 0.2485, Validation RMSE: 0.2410\n",
      "Epoch [1925001/10000000], Training Loss: 0.0624, Validation Loss: 0.0581, Training RMSE: 0.2498, Validation RMSE: 0.2410\n",
      "Epoch [1926001/10000000], Training Loss: 0.0616, Validation Loss: 0.0581, Training RMSE: 0.2482, Validation RMSE: 0.2410\n",
      "Epoch [1927001/10000000], Training Loss: 0.0617, Validation Loss: 0.0581, Training RMSE: 0.2484, Validation RMSE: 0.2410\n",
      "Epoch [1928001/10000000], Training Loss: 0.0628, Validation Loss: 0.0581, Training RMSE: 0.2505, Validation RMSE: 0.2410\n",
      "Epoch [1929001/10000000], Training Loss: 0.0616, Validation Loss: 0.0581, Training RMSE: 0.2482, Validation RMSE: 0.2410\n",
      "Epoch [1930001/10000000], Training Loss: 0.0623, Validation Loss: 0.0581, Training RMSE: 0.2497, Validation RMSE: 0.2409\n",
      "Epoch [1931001/10000000], Training Loss: 0.0633, Validation Loss: 0.0581, Training RMSE: 0.2516, Validation RMSE: 0.2409\n",
      "Epoch [1932001/10000000], Training Loss: 0.0625, Validation Loss: 0.0580, Training RMSE: 0.2500, Validation RMSE: 0.2409\n",
      "Epoch [1933001/10000000], Training Loss: 0.0620, Validation Loss: 0.0580, Training RMSE: 0.2489, Validation RMSE: 0.2409\n",
      "Epoch [1934001/10000000], Training Loss: 0.0624, Validation Loss: 0.0580, Training RMSE: 0.2498, Validation RMSE: 0.2409\n",
      "Epoch [1935001/10000000], Training Loss: 0.0619, Validation Loss: 0.0580, Training RMSE: 0.2488, Validation RMSE: 0.2409\n",
      "Epoch [1936001/10000000], Training Loss: 0.0623, Validation Loss: 0.0580, Training RMSE: 0.2496, Validation RMSE: 0.2409\n",
      "Epoch [1937001/10000000], Training Loss: 0.0615, Validation Loss: 0.0580, Training RMSE: 0.2480, Validation RMSE: 0.2409\n",
      "Epoch [1938001/10000000], Training Loss: 0.0620, Validation Loss: 0.0580, Training RMSE: 0.2490, Validation RMSE: 0.2409\n",
      "Epoch [1939001/10000000], Training Loss: 0.0625, Validation Loss: 0.0580, Training RMSE: 0.2500, Validation RMSE: 0.2409\n",
      "Epoch [1940001/10000000], Training Loss: 0.0621, Validation Loss: 0.0580, Training RMSE: 0.2493, Validation RMSE: 0.2409\n",
      "Epoch [1941001/10000000], Training Loss: 0.0624, Validation Loss: 0.0580, Training RMSE: 0.2498, Validation RMSE: 0.2409\n",
      "Epoch [1942001/10000000], Training Loss: 0.0624, Validation Loss: 0.0580, Training RMSE: 0.2497, Validation RMSE: 0.2408\n",
      "Epoch [1943001/10000000], Training Loss: 0.0628, Validation Loss: 0.0580, Training RMSE: 0.2506, Validation RMSE: 0.2408\n",
      "Epoch [1944001/10000000], Training Loss: 0.0622, Validation Loss: 0.0580, Training RMSE: 0.2493, Validation RMSE: 0.2408\n",
      "Epoch [1945001/10000000], Training Loss: 0.0621, Validation Loss: 0.0580, Training RMSE: 0.2492, Validation RMSE: 0.2408\n",
      "Epoch [1946001/10000000], Training Loss: 0.0621, Validation Loss: 0.0580, Training RMSE: 0.2492, Validation RMSE: 0.2408\n",
      "Epoch [1947001/10000000], Training Loss: 0.0617, Validation Loss: 0.0580, Training RMSE: 0.2485, Validation RMSE: 0.2408\n",
      "Epoch [1948001/10000000], Training Loss: 0.0620, Validation Loss: 0.0580, Training RMSE: 0.2491, Validation RMSE: 0.2408\n",
      "Epoch [1949001/10000000], Training Loss: 0.0620, Validation Loss: 0.0580, Training RMSE: 0.2491, Validation RMSE: 0.2408\n",
      "Epoch [1950001/10000000], Training Loss: 0.0612, Validation Loss: 0.0580, Training RMSE: 0.2475, Validation RMSE: 0.2408\n",
      "Epoch [1951001/10000000], Training Loss: 0.0613, Validation Loss: 0.0580, Training RMSE: 0.2477, Validation RMSE: 0.2408\n",
      "Epoch [1952001/10000000], Training Loss: 0.0616, Validation Loss: 0.0580, Training RMSE: 0.2481, Validation RMSE: 0.2408\n",
      "Epoch [1953001/10000000], Training Loss: 0.0605, Validation Loss: 0.0580, Training RMSE: 0.2460, Validation RMSE: 0.2408\n",
      "Epoch [1954001/10000000], Training Loss: 0.0617, Validation Loss: 0.0580, Training RMSE: 0.2484, Validation RMSE: 0.2408\n",
      "Epoch [1955001/10000000], Training Loss: 0.0627, Validation Loss: 0.0580, Training RMSE: 0.2505, Validation RMSE: 0.2407\n",
      "Epoch [1956001/10000000], Training Loss: 0.0615, Validation Loss: 0.0580, Training RMSE: 0.2481, Validation RMSE: 0.2407\n",
      "Epoch [1957001/10000000], Training Loss: 0.0618, Validation Loss: 0.0579, Training RMSE: 0.2486, Validation RMSE: 0.2407\n",
      "Epoch [1958001/10000000], Training Loss: 0.0623, Validation Loss: 0.0579, Training RMSE: 0.2496, Validation RMSE: 0.2407\n",
      "Epoch [1959001/10000000], Training Loss: 0.0613, Validation Loss: 0.0579, Training RMSE: 0.2475, Validation RMSE: 0.2407\n",
      "Epoch [1960001/10000000], Training Loss: 0.0616, Validation Loss: 0.0579, Training RMSE: 0.2481, Validation RMSE: 0.2407\n",
      "Epoch [1961001/10000000], Training Loss: 0.0615, Validation Loss: 0.0579, Training RMSE: 0.2479, Validation RMSE: 0.2407\n",
      "Epoch [1962001/10000000], Training Loss: 0.0613, Validation Loss: 0.0579, Training RMSE: 0.2476, Validation RMSE: 0.2407\n",
      "Epoch [1963001/10000000], Training Loss: 0.0616, Validation Loss: 0.0579, Training RMSE: 0.2481, Validation RMSE: 0.2407\n",
      "Epoch [1964001/10000000], Training Loss: 0.0614, Validation Loss: 0.0579, Training RMSE: 0.2479, Validation RMSE: 0.2407\n",
      "Epoch [1965001/10000000], Training Loss: 0.0614, Validation Loss: 0.0579, Training RMSE: 0.2478, Validation RMSE: 0.2407\n",
      "Epoch [1966001/10000000], Training Loss: 0.0619, Validation Loss: 0.0579, Training RMSE: 0.2487, Validation RMSE: 0.2407\n",
      "Epoch [1967001/10000000], Training Loss: 0.0618, Validation Loss: 0.0579, Training RMSE: 0.2486, Validation RMSE: 0.2406\n",
      "Epoch [1968001/10000000], Training Loss: 0.0617, Validation Loss: 0.0579, Training RMSE: 0.2484, Validation RMSE: 0.2406\n",
      "Epoch [1969001/10000000], Training Loss: 0.0615, Validation Loss: 0.0579, Training RMSE: 0.2481, Validation RMSE: 0.2406\n",
      "Epoch [1970001/10000000], Training Loss: 0.0620, Validation Loss: 0.0579, Training RMSE: 0.2490, Validation RMSE: 0.2406\n",
      "Epoch [1971001/10000000], Training Loss: 0.0621, Validation Loss: 0.0579, Training RMSE: 0.2493, Validation RMSE: 0.2406\n",
      "Epoch [1972001/10000000], Training Loss: 0.0617, Validation Loss: 0.0579, Training RMSE: 0.2483, Validation RMSE: 0.2406\n",
      "Epoch [1973001/10000000], Training Loss: 0.0606, Validation Loss: 0.0579, Training RMSE: 0.2463, Validation RMSE: 0.2406\n",
      "Epoch [1974001/10000000], Training Loss: 0.0618, Validation Loss: 0.0579, Training RMSE: 0.2486, Validation RMSE: 0.2406\n",
      "Epoch [1975001/10000000], Training Loss: 0.0614, Validation Loss: 0.0579, Training RMSE: 0.2478, Validation RMSE: 0.2406\n",
      "Epoch [1976001/10000000], Training Loss: 0.0611, Validation Loss: 0.0579, Training RMSE: 0.2472, Validation RMSE: 0.2406\n",
      "Epoch [1977001/10000000], Training Loss: 0.0615, Validation Loss: 0.0579, Training RMSE: 0.2480, Validation RMSE: 0.2406\n",
      "Epoch [1978001/10000000], Training Loss: 0.0618, Validation Loss: 0.0579, Training RMSE: 0.2487, Validation RMSE: 0.2406\n",
      "Epoch [1979001/10000000], Training Loss: 0.0615, Validation Loss: 0.0579, Training RMSE: 0.2481, Validation RMSE: 0.2406\n",
      "Epoch [1980001/10000000], Training Loss: 0.0607, Validation Loss: 0.0579, Training RMSE: 0.2463, Validation RMSE: 0.2406\n",
      "Epoch [1981001/10000000], Training Loss: 0.0615, Validation Loss: 0.0579, Training RMSE: 0.2481, Validation RMSE: 0.2405\n",
      "Epoch [1982001/10000000], Training Loss: 0.0612, Validation Loss: 0.0579, Training RMSE: 0.2473, Validation RMSE: 0.2405\n",
      "Epoch [1983001/10000000], Training Loss: 0.0622, Validation Loss: 0.0579, Training RMSE: 0.2493, Validation RMSE: 0.2405\n",
      "Epoch [1984001/10000000], Training Loss: 0.0614, Validation Loss: 0.0579, Training RMSE: 0.2479, Validation RMSE: 0.2405\n",
      "Epoch [1985001/10000000], Training Loss: 0.0616, Validation Loss: 0.0578, Training RMSE: 0.2482, Validation RMSE: 0.2405\n",
      "Epoch [1986001/10000000], Training Loss: 0.0613, Validation Loss: 0.0578, Training RMSE: 0.2475, Validation RMSE: 0.2405\n",
      "Epoch [1987001/10000000], Training Loss: 0.0613, Validation Loss: 0.0578, Training RMSE: 0.2475, Validation RMSE: 0.2405\n",
      "Epoch [1988001/10000000], Training Loss: 0.0617, Validation Loss: 0.0578, Training RMSE: 0.2483, Validation RMSE: 0.2405\n",
      "Epoch [1989001/10000000], Training Loss: 0.0618, Validation Loss: 0.0578, Training RMSE: 0.2485, Validation RMSE: 0.2405\n",
      "Epoch [1990001/10000000], Training Loss: 0.0616, Validation Loss: 0.0578, Training RMSE: 0.2482, Validation RMSE: 0.2405\n",
      "Epoch [1991001/10000000], Training Loss: 0.0616, Validation Loss: 0.0578, Training RMSE: 0.2482, Validation RMSE: 0.2405\n",
      "Epoch [1992001/10000000], Training Loss: 0.0627, Validation Loss: 0.0578, Training RMSE: 0.2504, Validation RMSE: 0.2405\n",
      "Epoch [1993001/10000000], Training Loss: 0.0610, Validation Loss: 0.0578, Training RMSE: 0.2470, Validation RMSE: 0.2405\n",
      "Epoch [1994001/10000000], Training Loss: 0.0616, Validation Loss: 0.0578, Training RMSE: 0.2482, Validation RMSE: 0.2404\n",
      "Epoch [1995001/10000000], Training Loss: 0.0617, Validation Loss: 0.0578, Training RMSE: 0.2484, Validation RMSE: 0.2404\n",
      "Epoch [1996001/10000000], Training Loss: 0.0609, Validation Loss: 0.0578, Training RMSE: 0.2468, Validation RMSE: 0.2404\n",
      "Epoch [1997001/10000000], Training Loss: 0.0610, Validation Loss: 0.0578, Training RMSE: 0.2470, Validation RMSE: 0.2404\n",
      "Epoch [1998001/10000000], Training Loss: 0.0610, Validation Loss: 0.0578, Training RMSE: 0.2470, Validation RMSE: 0.2404\n",
      "Epoch [1999001/10000000], Training Loss: 0.0608, Validation Loss: 0.0578, Training RMSE: 0.2465, Validation RMSE: 0.2404\n",
      "Epoch [2000001/10000000], Training Loss: 0.0617, Validation Loss: 0.0578, Training RMSE: 0.2485, Validation RMSE: 0.2404\n",
      "Epoch [2001001/10000000], Training Loss: 0.0613, Validation Loss: 0.0578, Training RMSE: 0.2475, Validation RMSE: 0.2404\n",
      "Epoch [2002001/10000000], Training Loss: 0.0603, Validation Loss: 0.0578, Training RMSE: 0.2456, Validation RMSE: 0.2404\n",
      "Epoch [2003001/10000000], Training Loss: 0.0611, Validation Loss: 0.0578, Training RMSE: 0.2472, Validation RMSE: 0.2404\n",
      "Epoch [2004001/10000000], Training Loss: 0.0617, Validation Loss: 0.0578, Training RMSE: 0.2483, Validation RMSE: 0.2404\n",
      "Epoch [2005001/10000000], Training Loss: 0.0614, Validation Loss: 0.0578, Training RMSE: 0.2479, Validation RMSE: 0.2404\n",
      "Epoch [2006001/10000000], Training Loss: 0.0613, Validation Loss: 0.0578, Training RMSE: 0.2476, Validation RMSE: 0.2404\n",
      "Epoch [2007001/10000000], Training Loss: 0.0612, Validation Loss: 0.0578, Training RMSE: 0.2474, Validation RMSE: 0.2404\n",
      "Epoch [2008001/10000000], Training Loss: 0.0608, Validation Loss: 0.0578, Training RMSE: 0.2466, Validation RMSE: 0.2403\n",
      "Epoch [2009001/10000000], Training Loss: 0.0608, Validation Loss: 0.0578, Training RMSE: 0.2466, Validation RMSE: 0.2403\n",
      "Epoch [2010001/10000000], Training Loss: 0.0617, Validation Loss: 0.0578, Training RMSE: 0.2485, Validation RMSE: 0.2403\n",
      "Epoch [2011001/10000000], Training Loss: 0.0613, Validation Loss: 0.0578, Training RMSE: 0.2476, Validation RMSE: 0.2403\n",
      "Epoch [2012001/10000000], Training Loss: 0.0611, Validation Loss: 0.0578, Training RMSE: 0.2472, Validation RMSE: 0.2403\n",
      "Epoch [2013001/10000000], Training Loss: 0.0613, Validation Loss: 0.0577, Training RMSE: 0.2475, Validation RMSE: 0.2403\n",
      "Epoch [2014001/10000000], Training Loss: 0.0605, Validation Loss: 0.0577, Training RMSE: 0.2459, Validation RMSE: 0.2403\n",
      "Epoch [2015001/10000000], Training Loss: 0.0610, Validation Loss: 0.0577, Training RMSE: 0.2469, Validation RMSE: 0.2403\n",
      "Epoch [2016001/10000000], Training Loss: 0.0615, Validation Loss: 0.0577, Training RMSE: 0.2481, Validation RMSE: 0.2403\n",
      "Epoch [2017001/10000000], Training Loss: 0.0611, Validation Loss: 0.0577, Training RMSE: 0.2472, Validation RMSE: 0.2403\n",
      "Epoch [2018001/10000000], Training Loss: 0.0604, Validation Loss: 0.0577, Training RMSE: 0.2458, Validation RMSE: 0.2403\n",
      "Epoch [2019001/10000000], Training Loss: 0.0605, Validation Loss: 0.0577, Training RMSE: 0.2459, Validation RMSE: 0.2403\n",
      "Epoch [2020001/10000000], Training Loss: 0.0614, Validation Loss: 0.0577, Training RMSE: 0.2478, Validation RMSE: 0.2403\n",
      "Epoch [2021001/10000000], Training Loss: 0.0614, Validation Loss: 0.0577, Training RMSE: 0.2479, Validation RMSE: 0.2403\n",
      "Epoch [2022001/10000000], Training Loss: 0.0607, Validation Loss: 0.0577, Training RMSE: 0.2464, Validation RMSE: 0.2402\n",
      "Epoch [2023001/10000000], Training Loss: 0.0609, Validation Loss: 0.0577, Training RMSE: 0.2468, Validation RMSE: 0.2402\n",
      "Epoch [2024001/10000000], Training Loss: 0.0608, Validation Loss: 0.0577, Training RMSE: 0.2465, Validation RMSE: 0.2402\n",
      "Epoch [2025001/10000000], Training Loss: 0.0600, Validation Loss: 0.0577, Training RMSE: 0.2450, Validation RMSE: 0.2402\n",
      "Epoch [2026001/10000000], Training Loss: 0.0608, Validation Loss: 0.0577, Training RMSE: 0.2466, Validation RMSE: 0.2402\n",
      "Epoch [2027001/10000000], Training Loss: 0.0608, Validation Loss: 0.0577, Training RMSE: 0.2466, Validation RMSE: 0.2402\n",
      "Epoch [2028001/10000000], Training Loss: 0.0609, Validation Loss: 0.0577, Training RMSE: 0.2469, Validation RMSE: 0.2402\n",
      "Epoch [2029001/10000000], Training Loss: 0.0610, Validation Loss: 0.0577, Training RMSE: 0.2469, Validation RMSE: 0.2402\n",
      "Epoch [2030001/10000000], Training Loss: 0.0612, Validation Loss: 0.0577, Training RMSE: 0.2474, Validation RMSE: 0.2402\n",
      "Epoch [2031001/10000000], Training Loss: 0.0612, Validation Loss: 0.0577, Training RMSE: 0.2474, Validation RMSE: 0.2402\n",
      "Epoch [2032001/10000000], Training Loss: 0.0612, Validation Loss: 0.0577, Training RMSE: 0.2473, Validation RMSE: 0.2402\n",
      "Epoch [2033001/10000000], Training Loss: 0.0610, Validation Loss: 0.0577, Training RMSE: 0.2471, Validation RMSE: 0.2402\n",
      "Epoch [2034001/10000000], Training Loss: 0.0603, Validation Loss: 0.0577, Training RMSE: 0.2455, Validation RMSE: 0.2402\n",
      "Epoch [2035001/10000000], Training Loss: 0.0606, Validation Loss: 0.0577, Training RMSE: 0.2462, Validation RMSE: 0.2402\n",
      "Epoch [2036001/10000000], Training Loss: 0.0609, Validation Loss: 0.0577, Training RMSE: 0.2467, Validation RMSE: 0.2402\n",
      "Epoch [2037001/10000000], Training Loss: 0.0600, Validation Loss: 0.0577, Training RMSE: 0.2450, Validation RMSE: 0.2401\n",
      "Epoch [2038001/10000000], Training Loss: 0.0603, Validation Loss: 0.0577, Training RMSE: 0.2456, Validation RMSE: 0.2401\n",
      "Epoch [2039001/10000000], Training Loss: 0.0608, Validation Loss: 0.0577, Training RMSE: 0.2466, Validation RMSE: 0.2401\n",
      "Epoch [2040001/10000000], Training Loss: 0.0606, Validation Loss: 0.0577, Training RMSE: 0.2463, Validation RMSE: 0.2401\n",
      "Epoch [2041001/10000000], Training Loss: 0.0605, Validation Loss: 0.0577, Training RMSE: 0.2459, Validation RMSE: 0.2401\n",
      "Epoch [2042001/10000000], Training Loss: 0.0613, Validation Loss: 0.0577, Training RMSE: 0.2476, Validation RMSE: 0.2401\n",
      "Epoch [2043001/10000000], Training Loss: 0.0604, Validation Loss: 0.0576, Training RMSE: 0.2457, Validation RMSE: 0.2401\n",
      "Epoch [2044001/10000000], Training Loss: 0.0608, Validation Loss: 0.0576, Training RMSE: 0.2467, Validation RMSE: 0.2401\n",
      "Epoch [2045001/10000000], Training Loss: 0.0601, Validation Loss: 0.0576, Training RMSE: 0.2451, Validation RMSE: 0.2401\n",
      "Epoch [2046001/10000000], Training Loss: 0.0605, Validation Loss: 0.0576, Training RMSE: 0.2459, Validation RMSE: 0.2401\n",
      "Epoch [2047001/10000000], Training Loss: 0.0597, Validation Loss: 0.0576, Training RMSE: 0.2444, Validation RMSE: 0.2401\n",
      "Epoch [2048001/10000000], Training Loss: 0.0605, Validation Loss: 0.0576, Training RMSE: 0.2459, Validation RMSE: 0.2401\n",
      "Epoch [2049001/10000000], Training Loss: 0.0610, Validation Loss: 0.0576, Training RMSE: 0.2470, Validation RMSE: 0.2401\n",
      "Epoch [2050001/10000000], Training Loss: 0.0605, Validation Loss: 0.0576, Training RMSE: 0.2460, Validation RMSE: 0.2401\n",
      "Epoch [2051001/10000000], Training Loss: 0.0606, Validation Loss: 0.0576, Training RMSE: 0.2462, Validation RMSE: 0.2401\n",
      "Epoch [2052001/10000000], Training Loss: 0.0608, Validation Loss: 0.0576, Training RMSE: 0.2465, Validation RMSE: 0.2400\n",
      "Epoch [2053001/10000000], Training Loss: 0.0604, Validation Loss: 0.0576, Training RMSE: 0.2457, Validation RMSE: 0.2400\n",
      "Epoch [2054001/10000000], Training Loss: 0.0610, Validation Loss: 0.0576, Training RMSE: 0.2469, Validation RMSE: 0.2400\n",
      "Epoch [2055001/10000000], Training Loss: 0.0613, Validation Loss: 0.0576, Training RMSE: 0.2476, Validation RMSE: 0.2400\n",
      "Epoch [2056001/10000000], Training Loss: 0.0612, Validation Loss: 0.0576, Training RMSE: 0.2475, Validation RMSE: 0.2400\n",
      "Epoch [2057001/10000000], Training Loss: 0.0612, Validation Loss: 0.0576, Training RMSE: 0.2475, Validation RMSE: 0.2400\n",
      "Epoch [2058001/10000000], Training Loss: 0.0614, Validation Loss: 0.0576, Training RMSE: 0.2478, Validation RMSE: 0.2400\n",
      "Epoch [2059001/10000000], Training Loss: 0.0606, Validation Loss: 0.0576, Training RMSE: 0.2461, Validation RMSE: 0.2400\n",
      "Epoch [2060001/10000000], Training Loss: 0.0599, Validation Loss: 0.0576, Training RMSE: 0.2447, Validation RMSE: 0.2400\n",
      "Epoch [2061001/10000000], Training Loss: 0.0610, Validation Loss: 0.0576, Training RMSE: 0.2469, Validation RMSE: 0.2400\n",
      "Epoch [2062001/10000000], Training Loss: 0.0606, Validation Loss: 0.0576, Training RMSE: 0.2463, Validation RMSE: 0.2400\n",
      "Epoch [2063001/10000000], Training Loss: 0.0607, Validation Loss: 0.0576, Training RMSE: 0.2463, Validation RMSE: 0.2400\n",
      "Epoch [2064001/10000000], Training Loss: 0.0615, Validation Loss: 0.0576, Training RMSE: 0.2479, Validation RMSE: 0.2400\n",
      "Epoch [2065001/10000000], Training Loss: 0.0606, Validation Loss: 0.0576, Training RMSE: 0.2462, Validation RMSE: 0.2400\n",
      "Epoch [2066001/10000000], Training Loss: 0.0606, Validation Loss: 0.0576, Training RMSE: 0.2462, Validation RMSE: 0.2400\n",
      "Epoch [2067001/10000000], Training Loss: 0.0601, Validation Loss: 0.0576, Training RMSE: 0.2452, Validation RMSE: 0.2399\n",
      "Epoch [2068001/10000000], Training Loss: 0.0609, Validation Loss: 0.0576, Training RMSE: 0.2468, Validation RMSE: 0.2399\n",
      "Epoch [2069001/10000000], Training Loss: 0.0609, Validation Loss: 0.0576, Training RMSE: 0.2467, Validation RMSE: 0.2399\n",
      "Epoch [2070001/10000000], Training Loss: 0.0605, Validation Loss: 0.0576, Training RMSE: 0.2459, Validation RMSE: 0.2399\n",
      "Epoch [2071001/10000000], Training Loss: 0.0605, Validation Loss: 0.0576, Training RMSE: 0.2460, Validation RMSE: 0.2399\n",
      "Epoch [2072001/10000000], Training Loss: 0.0612, Validation Loss: 0.0576, Training RMSE: 0.2475, Validation RMSE: 0.2399\n",
      "Epoch [2073001/10000000], Training Loss: 0.0607, Validation Loss: 0.0576, Training RMSE: 0.2464, Validation RMSE: 0.2399\n",
      "Epoch [2074001/10000000], Training Loss: 0.0607, Validation Loss: 0.0576, Training RMSE: 0.2464, Validation RMSE: 0.2399\n",
      "Epoch [2075001/10000000], Training Loss: 0.0600, Validation Loss: 0.0576, Training RMSE: 0.2449, Validation RMSE: 0.2399\n",
      "Epoch [2076001/10000000], Training Loss: 0.0600, Validation Loss: 0.0575, Training RMSE: 0.2450, Validation RMSE: 0.2399\n",
      "Epoch [2077001/10000000], Training Loss: 0.0615, Validation Loss: 0.0575, Training RMSE: 0.2480, Validation RMSE: 0.2399\n",
      "Epoch [2078001/10000000], Training Loss: 0.0607, Validation Loss: 0.0575, Training RMSE: 0.2463, Validation RMSE: 0.2399\n",
      "Epoch [2079001/10000000], Training Loss: 0.0601, Validation Loss: 0.0575, Training RMSE: 0.2452, Validation RMSE: 0.2399\n",
      "Epoch [2080001/10000000], Training Loss: 0.0600, Validation Loss: 0.0575, Training RMSE: 0.2450, Validation RMSE: 0.2399\n",
      "Epoch [2081001/10000000], Training Loss: 0.0601, Validation Loss: 0.0575, Training RMSE: 0.2451, Validation RMSE: 0.2399\n",
      "Epoch [2082001/10000000], Training Loss: 0.0600, Validation Loss: 0.0575, Training RMSE: 0.2450, Validation RMSE: 0.2399\n",
      "Epoch [2083001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2398\n",
      "Epoch [2084001/10000000], Training Loss: 0.0593, Validation Loss: 0.0575, Training RMSE: 0.2435, Validation RMSE: 0.2398\n",
      "Epoch [2085001/10000000], Training Loss: 0.0604, Validation Loss: 0.0575, Training RMSE: 0.2457, Validation RMSE: 0.2398\n",
      "Epoch [2086001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2398\n",
      "Epoch [2087001/10000000], Training Loss: 0.0599, Validation Loss: 0.0575, Training RMSE: 0.2448, Validation RMSE: 0.2398\n",
      "Epoch [2088001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2398\n",
      "Epoch [2089001/10000000], Training Loss: 0.0598, Validation Loss: 0.0575, Training RMSE: 0.2445, Validation RMSE: 0.2398\n",
      "Epoch [2090001/10000000], Training Loss: 0.0597, Validation Loss: 0.0575, Training RMSE: 0.2443, Validation RMSE: 0.2398\n",
      "Epoch [2091001/10000000], Training Loss: 0.0601, Validation Loss: 0.0575, Training RMSE: 0.2452, Validation RMSE: 0.2398\n",
      "Epoch [2092001/10000000], Training Loss: 0.0601, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2398\n",
      "Epoch [2093001/10000000], Training Loss: 0.0604, Validation Loss: 0.0575, Training RMSE: 0.2457, Validation RMSE: 0.2398\n",
      "Epoch [2094001/10000000], Training Loss: 0.0605, Validation Loss: 0.0575, Training RMSE: 0.2460, Validation RMSE: 0.2398\n",
      "Epoch [2095001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2398\n",
      "Epoch [2096001/10000000], Training Loss: 0.0598, Validation Loss: 0.0575, Training RMSE: 0.2446, Validation RMSE: 0.2398\n",
      "Epoch [2097001/10000000], Training Loss: 0.0605, Validation Loss: 0.0575, Training RMSE: 0.2460, Validation RMSE: 0.2398\n",
      "Epoch [2098001/10000000], Training Loss: 0.0607, Validation Loss: 0.0575, Training RMSE: 0.2463, Validation RMSE: 0.2398\n",
      "Epoch [2099001/10000000], Training Loss: 0.0607, Validation Loss: 0.0575, Training RMSE: 0.2463, Validation RMSE: 0.2397\n",
      "Epoch [2100001/10000000], Training Loss: 0.0597, Validation Loss: 0.0575, Training RMSE: 0.2444, Validation RMSE: 0.2397\n",
      "Epoch [2101001/10000000], Training Loss: 0.0604, Validation Loss: 0.0575, Training RMSE: 0.2457, Validation RMSE: 0.2397\n",
      "Epoch [2102001/10000000], Training Loss: 0.0596, Validation Loss: 0.0575, Training RMSE: 0.2440, Validation RMSE: 0.2397\n",
      "Epoch [2103001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2397\n",
      "Epoch [2104001/10000000], Training Loss: 0.0595, Validation Loss: 0.0575, Training RMSE: 0.2439, Validation RMSE: 0.2397\n",
      "Epoch [2105001/10000000], Training Loss: 0.0591, Validation Loss: 0.0575, Training RMSE: 0.2431, Validation RMSE: 0.2397\n",
      "Epoch [2106001/10000000], Training Loss: 0.0608, Validation Loss: 0.0575, Training RMSE: 0.2466, Validation RMSE: 0.2397\n",
      "Epoch [2107001/10000000], Training Loss: 0.0601, Validation Loss: 0.0575, Training RMSE: 0.2451, Validation RMSE: 0.2397\n",
      "Epoch [2108001/10000000], Training Loss: 0.0603, Validation Loss: 0.0575, Training RMSE: 0.2456, Validation RMSE: 0.2397\n",
      "Epoch [2109001/10000000], Training Loss: 0.0602, Validation Loss: 0.0575, Training RMSE: 0.2453, Validation RMSE: 0.2397\n",
      "Epoch [2110001/10000000], Training Loss: 0.0601, Validation Loss: 0.0574, Training RMSE: 0.2451, Validation RMSE: 0.2397\n",
      "Epoch [2111001/10000000], Training Loss: 0.0603, Validation Loss: 0.0574, Training RMSE: 0.2455, Validation RMSE: 0.2397\n",
      "Epoch [2112001/10000000], Training Loss: 0.0599, Validation Loss: 0.0574, Training RMSE: 0.2448, Validation RMSE: 0.2397\n",
      "Epoch [2113001/10000000], Training Loss: 0.0598, Validation Loss: 0.0574, Training RMSE: 0.2445, Validation RMSE: 0.2397\n",
      "Epoch [2114001/10000000], Training Loss: 0.0603, Validation Loss: 0.0574, Training RMSE: 0.2456, Validation RMSE: 0.2397\n",
      "Epoch [2115001/10000000], Training Loss: 0.0597, Validation Loss: 0.0574, Training RMSE: 0.2443, Validation RMSE: 0.2397\n",
      "Epoch [2116001/10000000], Training Loss: 0.0607, Validation Loss: 0.0574, Training RMSE: 0.2464, Validation RMSE: 0.2397\n",
      "Epoch [2117001/10000000], Training Loss: 0.0600, Validation Loss: 0.0574, Training RMSE: 0.2450, Validation RMSE: 0.2396\n",
      "Epoch [2118001/10000000], Training Loss: 0.0600, Validation Loss: 0.0574, Training RMSE: 0.2450, Validation RMSE: 0.2396\n",
      "Epoch [2119001/10000000], Training Loss: 0.0603, Validation Loss: 0.0574, Training RMSE: 0.2456, Validation RMSE: 0.2396\n",
      "Epoch [2120001/10000000], Training Loss: 0.0604, Validation Loss: 0.0574, Training RMSE: 0.2457, Validation RMSE: 0.2396\n",
      "Epoch [2121001/10000000], Training Loss: 0.0598, Validation Loss: 0.0574, Training RMSE: 0.2445, Validation RMSE: 0.2396\n",
      "Epoch [2122001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2440, Validation RMSE: 0.2396\n",
      "Epoch [2123001/10000000], Training Loss: 0.0602, Validation Loss: 0.0574, Training RMSE: 0.2454, Validation RMSE: 0.2396\n",
      "Epoch [2124001/10000000], Training Loss: 0.0595, Validation Loss: 0.0574, Training RMSE: 0.2439, Validation RMSE: 0.2396\n",
      "Epoch [2125001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2442, Validation RMSE: 0.2396\n",
      "Epoch [2126001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2442, Validation RMSE: 0.2396\n",
      "Epoch [2127001/10000000], Training Loss: 0.0593, Validation Loss: 0.0574, Training RMSE: 0.2436, Validation RMSE: 0.2396\n",
      "Epoch [2128001/10000000], Training Loss: 0.0606, Validation Loss: 0.0574, Training RMSE: 0.2462, Validation RMSE: 0.2396\n",
      "Epoch [2129001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2441, Validation RMSE: 0.2396\n",
      "Epoch [2130001/10000000], Training Loss: 0.0601, Validation Loss: 0.0574, Training RMSE: 0.2451, Validation RMSE: 0.2396\n",
      "Epoch [2131001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2441, Validation RMSE: 0.2396\n",
      "Epoch [2132001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2442, Validation RMSE: 0.2396\n",
      "Epoch [2133001/10000000], Training Loss: 0.0593, Validation Loss: 0.0574, Training RMSE: 0.2434, Validation RMSE: 0.2396\n",
      "Epoch [2134001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2441, Validation RMSE: 0.2395\n",
      "Epoch [2135001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2442, Validation RMSE: 0.2395\n",
      "Epoch [2136001/10000000], Training Loss: 0.0588, Validation Loss: 0.0574, Training RMSE: 0.2424, Validation RMSE: 0.2395\n",
      "Epoch [2137001/10000000], Training Loss: 0.0594, Validation Loss: 0.0574, Training RMSE: 0.2438, Validation RMSE: 0.2395\n",
      "Epoch [2138001/10000000], Training Loss: 0.0598, Validation Loss: 0.0574, Training RMSE: 0.2446, Validation RMSE: 0.2395\n",
      "Epoch [2139001/10000000], Training Loss: 0.0603, Validation Loss: 0.0574, Training RMSE: 0.2456, Validation RMSE: 0.2395\n",
      "Epoch [2140001/10000000], Training Loss: 0.0599, Validation Loss: 0.0574, Training RMSE: 0.2447, Validation RMSE: 0.2395\n",
      "Epoch [2141001/10000000], Training Loss: 0.0601, Validation Loss: 0.0574, Training RMSE: 0.2451, Validation RMSE: 0.2395\n",
      "Epoch [2142001/10000000], Training Loss: 0.0595, Validation Loss: 0.0574, Training RMSE: 0.2440, Validation RMSE: 0.2395\n",
      "Epoch [2143001/10000000], Training Loss: 0.0599, Validation Loss: 0.0574, Training RMSE: 0.2447, Validation RMSE: 0.2395\n",
      "Epoch [2144001/10000000], Training Loss: 0.0602, Validation Loss: 0.0574, Training RMSE: 0.2454, Validation RMSE: 0.2395\n",
      "Epoch [2145001/10000000], Training Loss: 0.0596, Validation Loss: 0.0574, Training RMSE: 0.2442, Validation RMSE: 0.2395\n",
      "Epoch [2146001/10000000], Training Loss: 0.0601, Validation Loss: 0.0574, Training RMSE: 0.2451, Validation RMSE: 0.2395\n",
      "Epoch [2147001/10000000], Training Loss: 0.0599, Validation Loss: 0.0573, Training RMSE: 0.2448, Validation RMSE: 0.2395\n",
      "Epoch [2148001/10000000], Training Loss: 0.0595, Validation Loss: 0.0573, Training RMSE: 0.2440, Validation RMSE: 0.2395\n",
      "Epoch [2149001/10000000], Training Loss: 0.0592, Validation Loss: 0.0573, Training RMSE: 0.2433, Validation RMSE: 0.2395\n",
      "Epoch [2150001/10000000], Training Loss: 0.0593, Validation Loss: 0.0573, Training RMSE: 0.2434, Validation RMSE: 0.2395\n",
      "Epoch [2151001/10000000], Training Loss: 0.0600, Validation Loss: 0.0573, Training RMSE: 0.2449, Validation RMSE: 0.2395\n",
      "Epoch [2152001/10000000], Training Loss: 0.0596, Validation Loss: 0.0573, Training RMSE: 0.2441, Validation RMSE: 0.2394\n",
      "Epoch [2153001/10000000], Training Loss: 0.0597, Validation Loss: 0.0573, Training RMSE: 0.2444, Validation RMSE: 0.2394\n",
      "Epoch [2154001/10000000], Training Loss: 0.0593, Validation Loss: 0.0573, Training RMSE: 0.2435, Validation RMSE: 0.2394\n",
      "Epoch [2155001/10000000], Training Loss: 0.0599, Validation Loss: 0.0573, Training RMSE: 0.2447, Validation RMSE: 0.2394\n",
      "Epoch [2156001/10000000], Training Loss: 0.0592, Validation Loss: 0.0573, Training RMSE: 0.2432, Validation RMSE: 0.2394\n",
      "Epoch [2157001/10000000], Training Loss: 0.0599, Validation Loss: 0.0573, Training RMSE: 0.2448, Validation RMSE: 0.2394\n",
      "Epoch [2158001/10000000], Training Loss: 0.0594, Validation Loss: 0.0573, Training RMSE: 0.2437, Validation RMSE: 0.2394\n",
      "Epoch [2159001/10000000], Training Loss: 0.0598, Validation Loss: 0.0573, Training RMSE: 0.2446, Validation RMSE: 0.2394\n",
      "Epoch [2160001/10000000], Training Loss: 0.0588, Validation Loss: 0.0573, Training RMSE: 0.2424, Validation RMSE: 0.2394\n",
      "Epoch [2161001/10000000], Training Loss: 0.0594, Validation Loss: 0.0573, Training RMSE: 0.2437, Validation RMSE: 0.2394\n",
      "Epoch [2162001/10000000], Training Loss: 0.0585, Validation Loss: 0.0573, Training RMSE: 0.2418, Validation RMSE: 0.2394\n",
      "Epoch [2163001/10000000], Training Loss: 0.0595, Validation Loss: 0.0573, Training RMSE: 0.2440, Validation RMSE: 0.2394\n",
      "Epoch [2164001/10000000], Training Loss: 0.0599, Validation Loss: 0.0573, Training RMSE: 0.2447, Validation RMSE: 0.2394\n",
      "Epoch [2165001/10000000], Training Loss: 0.0602, Validation Loss: 0.0573, Training RMSE: 0.2454, Validation RMSE: 0.2394\n",
      "Epoch [2166001/10000000], Training Loss: 0.0598, Validation Loss: 0.0573, Training RMSE: 0.2445, Validation RMSE: 0.2394\n",
      "Epoch [2167001/10000000], Training Loss: 0.0600, Validation Loss: 0.0573, Training RMSE: 0.2449, Validation RMSE: 0.2394\n",
      "Epoch [2168001/10000000], Training Loss: 0.0597, Validation Loss: 0.0573, Training RMSE: 0.2443, Validation RMSE: 0.2394\n",
      "Epoch [2169001/10000000], Training Loss: 0.0588, Validation Loss: 0.0573, Training RMSE: 0.2424, Validation RMSE: 0.2394\n",
      "Epoch [2170001/10000000], Training Loss: 0.0591, Validation Loss: 0.0573, Training RMSE: 0.2431, Validation RMSE: 0.2393\n",
      "Epoch [2171001/10000000], Training Loss: 0.0594, Validation Loss: 0.0573, Training RMSE: 0.2436, Validation RMSE: 0.2393\n",
      "Epoch [2172001/10000000], Training Loss: 0.0590, Validation Loss: 0.0573, Training RMSE: 0.2430, Validation RMSE: 0.2393\n",
      "Epoch [2173001/10000000], Training Loss: 0.0588, Validation Loss: 0.0573, Training RMSE: 0.2425, Validation RMSE: 0.2393\n",
      "Epoch [2174001/10000000], Training Loss: 0.0594, Validation Loss: 0.0573, Training RMSE: 0.2437, Validation RMSE: 0.2393\n",
      "Epoch [2175001/10000000], Training Loss: 0.0590, Validation Loss: 0.0573, Training RMSE: 0.2429, Validation RMSE: 0.2393\n",
      "Epoch [2176001/10000000], Training Loss: 0.0590, Validation Loss: 0.0573, Training RMSE: 0.2428, Validation RMSE: 0.2393\n",
      "Epoch [2177001/10000000], Training Loss: 0.0593, Validation Loss: 0.0573, Training RMSE: 0.2436, Validation RMSE: 0.2393\n",
      "Epoch [2178001/10000000], Training Loss: 0.0591, Validation Loss: 0.0573, Training RMSE: 0.2432, Validation RMSE: 0.2393\n",
      "Epoch [2179001/10000000], Training Loss: 0.0589, Validation Loss: 0.0573, Training RMSE: 0.2428, Validation RMSE: 0.2393\n",
      "Epoch [2180001/10000000], Training Loss: 0.0596, Validation Loss: 0.0573, Training RMSE: 0.2441, Validation RMSE: 0.2393\n",
      "Epoch [2181001/10000000], Training Loss: 0.0593, Validation Loss: 0.0573, Training RMSE: 0.2435, Validation RMSE: 0.2393\n",
      "Epoch [2182001/10000000], Training Loss: 0.0593, Validation Loss: 0.0573, Training RMSE: 0.2436, Validation RMSE: 0.2393\n",
      "Epoch [2183001/10000000], Training Loss: 0.0588, Validation Loss: 0.0573, Training RMSE: 0.2425, Validation RMSE: 0.2393\n",
      "Epoch [2184001/10000000], Training Loss: 0.0589, Validation Loss: 0.0573, Training RMSE: 0.2427, Validation RMSE: 0.2393\n",
      "Epoch [2185001/10000000], Training Loss: 0.0588, Validation Loss: 0.0572, Training RMSE: 0.2425, Validation RMSE: 0.2393\n",
      "Epoch [2186001/10000000], Training Loss: 0.0588, Validation Loss: 0.0572, Training RMSE: 0.2424, Validation RMSE: 0.2393\n",
      "Epoch [2187001/10000000], Training Loss: 0.0600, Validation Loss: 0.0572, Training RMSE: 0.2450, Validation RMSE: 0.2393\n",
      "Epoch [2188001/10000000], Training Loss: 0.0604, Validation Loss: 0.0572, Training RMSE: 0.2457, Validation RMSE: 0.2393\n",
      "Epoch [2189001/10000000], Training Loss: 0.0592, Validation Loss: 0.0572, Training RMSE: 0.2433, Validation RMSE: 0.2392\n",
      "Epoch [2190001/10000000], Training Loss: 0.0592, Validation Loss: 0.0572, Training RMSE: 0.2432, Validation RMSE: 0.2392\n",
      "Epoch [2191001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2426, Validation RMSE: 0.2392\n",
      "Epoch [2192001/10000000], Training Loss: 0.0594, Validation Loss: 0.0572, Training RMSE: 0.2436, Validation RMSE: 0.2392\n",
      "Epoch [2193001/10000000], Training Loss: 0.0586, Validation Loss: 0.0572, Training RMSE: 0.2421, Validation RMSE: 0.2392\n",
      "Epoch [2194001/10000000], Training Loss: 0.0582, Validation Loss: 0.0572, Training RMSE: 0.2413, Validation RMSE: 0.2392\n",
      "Epoch [2195001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2422, Validation RMSE: 0.2392\n",
      "Epoch [2196001/10000000], Training Loss: 0.0592, Validation Loss: 0.0572, Training RMSE: 0.2434, Validation RMSE: 0.2392\n",
      "Epoch [2197001/10000000], Training Loss: 0.0586, Validation Loss: 0.0572, Training RMSE: 0.2421, Validation RMSE: 0.2392\n",
      "Epoch [2198001/10000000], Training Loss: 0.0591, Validation Loss: 0.0572, Training RMSE: 0.2431, Validation RMSE: 0.2392\n",
      "Epoch [2199001/10000000], Training Loss: 0.0584, Validation Loss: 0.0572, Training RMSE: 0.2416, Validation RMSE: 0.2392\n",
      "Epoch [2200001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2427, Validation RMSE: 0.2392\n",
      "Epoch [2201001/10000000], Training Loss: 0.0588, Validation Loss: 0.0572, Training RMSE: 0.2425, Validation RMSE: 0.2392\n",
      "Epoch [2202001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2428, Validation RMSE: 0.2392\n",
      "Epoch [2203001/10000000], Training Loss: 0.0583, Validation Loss: 0.0572, Training RMSE: 0.2415, Validation RMSE: 0.2392\n",
      "Epoch [2204001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2427, Validation RMSE: 0.2392\n",
      "Epoch [2205001/10000000], Training Loss: 0.0595, Validation Loss: 0.0572, Training RMSE: 0.2439, Validation RMSE: 0.2392\n",
      "Epoch [2206001/10000000], Training Loss: 0.0592, Validation Loss: 0.0572, Training RMSE: 0.2433, Validation RMSE: 0.2392\n",
      "Epoch [2207001/10000000], Training Loss: 0.0586, Validation Loss: 0.0572, Training RMSE: 0.2420, Validation RMSE: 0.2392\n",
      "Epoch [2208001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2427, Validation RMSE: 0.2392\n",
      "Epoch [2209001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2423, Validation RMSE: 0.2391\n",
      "Epoch [2210001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2423, Validation RMSE: 0.2391\n",
      "Epoch [2211001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2423, Validation RMSE: 0.2391\n",
      "Epoch [2212001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2426, Validation RMSE: 0.2391\n",
      "Epoch [2213001/10000000], Training Loss: 0.0590, Validation Loss: 0.0572, Training RMSE: 0.2429, Validation RMSE: 0.2391\n",
      "Epoch [2214001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2422, Validation RMSE: 0.2391\n",
      "Epoch [2215001/10000000], Training Loss: 0.0590, Validation Loss: 0.0572, Training RMSE: 0.2429, Validation RMSE: 0.2391\n",
      "Epoch [2216001/10000000], Training Loss: 0.0591, Validation Loss: 0.0572, Training RMSE: 0.2432, Validation RMSE: 0.2391\n",
      "Epoch [2217001/10000000], Training Loss: 0.0586, Validation Loss: 0.0572, Training RMSE: 0.2421, Validation RMSE: 0.2391\n",
      "Epoch [2218001/10000000], Training Loss: 0.0582, Validation Loss: 0.0572, Training RMSE: 0.2413, Validation RMSE: 0.2391\n",
      "Epoch [2219001/10000000], Training Loss: 0.0585, Validation Loss: 0.0572, Training RMSE: 0.2419, Validation RMSE: 0.2391\n",
      "Epoch [2220001/10000000], Training Loss: 0.0586, Validation Loss: 0.0572, Training RMSE: 0.2421, Validation RMSE: 0.2391\n",
      "Epoch [2221001/10000000], Training Loss: 0.0588, Validation Loss: 0.0572, Training RMSE: 0.2425, Validation RMSE: 0.2391\n",
      "Epoch [2222001/10000000], Training Loss: 0.0594, Validation Loss: 0.0572, Training RMSE: 0.2437, Validation RMSE: 0.2391\n",
      "Epoch [2223001/10000000], Training Loss: 0.0590, Validation Loss: 0.0572, Training RMSE: 0.2428, Validation RMSE: 0.2391\n",
      "Epoch [2224001/10000000], Training Loss: 0.0584, Validation Loss: 0.0572, Training RMSE: 0.2417, Validation RMSE: 0.2391\n",
      "Epoch [2225001/10000000], Training Loss: 0.0584, Validation Loss: 0.0572, Training RMSE: 0.2416, Validation RMSE: 0.2391\n",
      "Epoch [2226001/10000000], Training Loss: 0.0589, Validation Loss: 0.0572, Training RMSE: 0.2428, Validation RMSE: 0.2391\n",
      "Epoch [2227001/10000000], Training Loss: 0.0587, Validation Loss: 0.0572, Training RMSE: 0.2422, Validation RMSE: 0.2391\n",
      "Epoch [2228001/10000000], Training Loss: 0.0586, Validation Loss: 0.0571, Training RMSE: 0.2420, Validation RMSE: 0.2391\n",
      "Epoch [2229001/10000000], Training Loss: 0.0588, Validation Loss: 0.0571, Training RMSE: 0.2424, Validation RMSE: 0.2391\n",
      "Epoch [2230001/10000000], Training Loss: 0.0590, Validation Loss: 0.0571, Training RMSE: 0.2429, Validation RMSE: 0.2390\n",
      "Epoch [2231001/10000000], Training Loss: 0.0586, Validation Loss: 0.0571, Training RMSE: 0.2421, Validation RMSE: 0.2390\n",
      "Epoch [2232001/10000000], Training Loss: 0.0594, Validation Loss: 0.0571, Training RMSE: 0.2438, Validation RMSE: 0.2390\n",
      "Epoch [2233001/10000000], Training Loss: 0.0584, Validation Loss: 0.0571, Training RMSE: 0.2417, Validation RMSE: 0.2390\n",
      "Epoch [2234001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2415, Validation RMSE: 0.2390\n",
      "Epoch [2235001/10000000], Training Loss: 0.0588, Validation Loss: 0.0571, Training RMSE: 0.2425, Validation RMSE: 0.2390\n",
      "Epoch [2236001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2415, Validation RMSE: 0.2390\n",
      "Epoch [2237001/10000000], Training Loss: 0.0594, Validation Loss: 0.0571, Training RMSE: 0.2438, Validation RMSE: 0.2390\n",
      "Epoch [2238001/10000000], Training Loss: 0.0590, Validation Loss: 0.0571, Training RMSE: 0.2430, Validation RMSE: 0.2390\n",
      "Epoch [2239001/10000000], Training Loss: 0.0590, Validation Loss: 0.0571, Training RMSE: 0.2429, Validation RMSE: 0.2390\n",
      "Epoch [2240001/10000000], Training Loss: 0.0585, Validation Loss: 0.0571, Training RMSE: 0.2418, Validation RMSE: 0.2390\n",
      "Epoch [2241001/10000000], Training Loss: 0.0588, Validation Loss: 0.0571, Training RMSE: 0.2425, Validation RMSE: 0.2390\n",
      "Epoch [2242001/10000000], Training Loss: 0.0587, Validation Loss: 0.0571, Training RMSE: 0.2424, Validation RMSE: 0.2390\n",
      "Epoch [2243001/10000000], Training Loss: 0.0579, Validation Loss: 0.0571, Training RMSE: 0.2407, Validation RMSE: 0.2390\n",
      "Epoch [2244001/10000000], Training Loss: 0.0581, Validation Loss: 0.0571, Training RMSE: 0.2410, Validation RMSE: 0.2390\n",
      "Epoch [2245001/10000000], Training Loss: 0.0582, Validation Loss: 0.0571, Training RMSE: 0.2413, Validation RMSE: 0.2390\n",
      "Epoch [2246001/10000000], Training Loss: 0.0582, Validation Loss: 0.0571, Training RMSE: 0.2413, Validation RMSE: 0.2390\n",
      "Epoch [2247001/10000000], Training Loss: 0.0581, Validation Loss: 0.0571, Training RMSE: 0.2411, Validation RMSE: 0.2390\n",
      "Epoch [2248001/10000000], Training Loss: 0.0586, Validation Loss: 0.0571, Training RMSE: 0.2421, Validation RMSE: 0.2390\n",
      "Epoch [2249001/10000000], Training Loss: 0.0589, Validation Loss: 0.0571, Training RMSE: 0.2426, Validation RMSE: 0.2390\n",
      "Epoch [2250001/10000000], Training Loss: 0.0575, Validation Loss: 0.0571, Training RMSE: 0.2397, Validation RMSE: 0.2390\n",
      "Epoch [2251001/10000000], Training Loss: 0.0588, Validation Loss: 0.0571, Training RMSE: 0.2425, Validation RMSE: 0.2390\n",
      "Epoch [2252001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2414, Validation RMSE: 0.2390\n",
      "Epoch [2253001/10000000], Training Loss: 0.0588, Validation Loss: 0.0571, Training RMSE: 0.2425, Validation RMSE: 0.2389\n",
      "Epoch [2254001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2415, Validation RMSE: 0.2389\n",
      "Epoch [2255001/10000000], Training Loss: 0.0574, Validation Loss: 0.0571, Training RMSE: 0.2396, Validation RMSE: 0.2389\n",
      "Epoch [2256001/10000000], Training Loss: 0.0581, Validation Loss: 0.0571, Training RMSE: 0.2411, Validation RMSE: 0.2389\n",
      "Epoch [2257001/10000000], Training Loss: 0.0582, Validation Loss: 0.0571, Training RMSE: 0.2413, Validation RMSE: 0.2389\n",
      "Epoch [2258001/10000000], Training Loss: 0.0581, Validation Loss: 0.0571, Training RMSE: 0.2411, Validation RMSE: 0.2389\n",
      "Epoch [2259001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2414, Validation RMSE: 0.2389\n",
      "Epoch [2260001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2414, Validation RMSE: 0.2389\n",
      "Epoch [2261001/10000000], Training Loss: 0.0598, Validation Loss: 0.0571, Training RMSE: 0.2446, Validation RMSE: 0.2389\n",
      "Epoch [2262001/10000000], Training Loss: 0.0578, Validation Loss: 0.0571, Training RMSE: 0.2405, Validation RMSE: 0.2389\n",
      "Epoch [2263001/10000000], Training Loss: 0.0576, Validation Loss: 0.0571, Training RMSE: 0.2401, Validation RMSE: 0.2389\n",
      "Epoch [2264001/10000000], Training Loss: 0.0582, Validation Loss: 0.0571, Training RMSE: 0.2412, Validation RMSE: 0.2389\n",
      "Epoch [2265001/10000000], Training Loss: 0.0585, Validation Loss: 0.0571, Training RMSE: 0.2418, Validation RMSE: 0.2389\n",
      "Epoch [2266001/10000000], Training Loss: 0.0581, Validation Loss: 0.0571, Training RMSE: 0.2411, Validation RMSE: 0.2389\n",
      "Epoch [2267001/10000000], Training Loss: 0.0580, Validation Loss: 0.0571, Training RMSE: 0.2408, Validation RMSE: 0.2389\n",
      "Epoch [2268001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2414, Validation RMSE: 0.2389\n",
      "Epoch [2269001/10000000], Training Loss: 0.0589, Validation Loss: 0.0571, Training RMSE: 0.2427, Validation RMSE: 0.2389\n",
      "Epoch [2270001/10000000], Training Loss: 0.0578, Validation Loss: 0.0571, Training RMSE: 0.2405, Validation RMSE: 0.2389\n",
      "Epoch [2271001/10000000], Training Loss: 0.0583, Validation Loss: 0.0571, Training RMSE: 0.2415, Validation RMSE: 0.2389\n",
      "Epoch [2272001/10000000], Training Loss: 0.0586, Validation Loss: 0.0571, Training RMSE: 0.2420, Validation RMSE: 0.2389\n",
      "Epoch [2273001/10000000], Training Loss: 0.0589, Validation Loss: 0.0571, Training RMSE: 0.2427, Validation RMSE: 0.2389\n",
      "Epoch [2274001/10000000], Training Loss: 0.0591, Validation Loss: 0.0571, Training RMSE: 0.2431, Validation RMSE: 0.2389\n",
      "Epoch [2275001/10000000], Training Loss: 0.0578, Validation Loss: 0.0571, Training RMSE: 0.2404, Validation RMSE: 0.2389\n",
      "Epoch [2276001/10000000], Training Loss: 0.0582, Validation Loss: 0.0570, Training RMSE: 0.2413, Validation RMSE: 0.2388\n",
      "Epoch [2277001/10000000], Training Loss: 0.0593, Validation Loss: 0.0570, Training RMSE: 0.2436, Validation RMSE: 0.2388\n",
      "Epoch [2278001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2401, Validation RMSE: 0.2388\n",
      "Epoch [2279001/10000000], Training Loss: 0.0569, Validation Loss: 0.0570, Training RMSE: 0.2386, Validation RMSE: 0.2388\n",
      "Epoch [2280001/10000000], Training Loss: 0.0582, Validation Loss: 0.0570, Training RMSE: 0.2412, Validation RMSE: 0.2388\n",
      "Epoch [2281001/10000000], Training Loss: 0.0588, Validation Loss: 0.0570, Training RMSE: 0.2424, Validation RMSE: 0.2388\n",
      "Epoch [2282001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2398, Validation RMSE: 0.2388\n",
      "Epoch [2283001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2403, Validation RMSE: 0.2388\n",
      "Epoch [2284001/10000000], Training Loss: 0.0583, Validation Loss: 0.0570, Training RMSE: 0.2414, Validation RMSE: 0.2388\n",
      "Epoch [2285001/10000000], Training Loss: 0.0588, Validation Loss: 0.0570, Training RMSE: 0.2426, Validation RMSE: 0.2388\n",
      "Epoch [2286001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2400, Validation RMSE: 0.2388\n",
      "Epoch [2287001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2397, Validation RMSE: 0.2388\n",
      "Epoch [2288001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2398, Validation RMSE: 0.2388\n",
      "Epoch [2289001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2401, Validation RMSE: 0.2388\n",
      "Epoch [2290001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2398, Validation RMSE: 0.2388\n",
      "Epoch [2291001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2388\n",
      "Epoch [2292001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2403, Validation RMSE: 0.2388\n",
      "Epoch [2293001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2400, Validation RMSE: 0.2388\n",
      "Epoch [2294001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2399, Validation RMSE: 0.2388\n",
      "Epoch [2295001/10000000], Training Loss: 0.0585, Validation Loss: 0.0570, Training RMSE: 0.2418, Validation RMSE: 0.2388\n",
      "Epoch [2296001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2388\n",
      "Epoch [2297001/10000000], Training Loss: 0.0580, Validation Loss: 0.0570, Training RMSE: 0.2409, Validation RMSE: 0.2388\n",
      "Epoch [2298001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2401, Validation RMSE: 0.2388\n",
      "Epoch [2299001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2399, Validation RMSE: 0.2388\n",
      "Epoch [2300001/10000000], Training Loss: 0.0583, Validation Loss: 0.0570, Training RMSE: 0.2414, Validation RMSE: 0.2388\n",
      "Epoch [2301001/10000000], Training Loss: 0.0579, Validation Loss: 0.0570, Training RMSE: 0.2406, Validation RMSE: 0.2387\n",
      "Epoch [2302001/10000000], Training Loss: 0.0580, Validation Loss: 0.0570, Training RMSE: 0.2409, Validation RMSE: 0.2387\n",
      "Epoch [2303001/10000000], Training Loss: 0.0581, Validation Loss: 0.0570, Training RMSE: 0.2411, Validation RMSE: 0.2387\n",
      "Epoch [2304001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2398, Validation RMSE: 0.2387\n",
      "Epoch [2305001/10000000], Training Loss: 0.0584, Validation Loss: 0.0570, Training RMSE: 0.2416, Validation RMSE: 0.2387\n",
      "Epoch [2306001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2400, Validation RMSE: 0.2387\n",
      "Epoch [2307001/10000000], Training Loss: 0.0573, Validation Loss: 0.0570, Training RMSE: 0.2395, Validation RMSE: 0.2387\n",
      "Epoch [2308001/10000000], Training Loss: 0.0580, Validation Loss: 0.0570, Training RMSE: 0.2409, Validation RMSE: 0.2387\n",
      "Epoch [2309001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2400, Validation RMSE: 0.2387\n",
      "Epoch [2310001/10000000], Training Loss: 0.0580, Validation Loss: 0.0570, Training RMSE: 0.2409, Validation RMSE: 0.2387\n",
      "Epoch [2311001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2387\n",
      "Epoch [2312001/10000000], Training Loss: 0.0579, Validation Loss: 0.0570, Training RMSE: 0.2407, Validation RMSE: 0.2387\n",
      "Epoch [2313001/10000000], Training Loss: 0.0573, Validation Loss: 0.0570, Training RMSE: 0.2394, Validation RMSE: 0.2387\n",
      "Epoch [2314001/10000000], Training Loss: 0.0576, Validation Loss: 0.0570, Training RMSE: 0.2401, Validation RMSE: 0.2387\n",
      "Epoch [2315001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2387\n",
      "Epoch [2316001/10000000], Training Loss: 0.0574, Validation Loss: 0.0570, Training RMSE: 0.2397, Validation RMSE: 0.2387\n",
      "Epoch [2317001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2387\n",
      "Epoch [2318001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2387\n",
      "Epoch [2319001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2397, Validation RMSE: 0.2387\n",
      "Epoch [2320001/10000000], Training Loss: 0.0571, Validation Loss: 0.0570, Training RMSE: 0.2389, Validation RMSE: 0.2387\n",
      "Epoch [2321001/10000000], Training Loss: 0.0575, Validation Loss: 0.0570, Training RMSE: 0.2397, Validation RMSE: 0.2387\n",
      "Epoch [2322001/10000000], Training Loss: 0.0574, Validation Loss: 0.0570, Training RMSE: 0.2396, Validation RMSE: 0.2387\n",
      "Epoch [2323001/10000000], Training Loss: 0.0577, Validation Loss: 0.0570, Training RMSE: 0.2402, Validation RMSE: 0.2387\n",
      "Epoch [2324001/10000000], Training Loss: 0.0568, Validation Loss: 0.0570, Training RMSE: 0.2383, Validation RMSE: 0.2387\n",
      "Epoch [2325001/10000000], Training Loss: 0.0587, Validation Loss: 0.0570, Training RMSE: 0.2424, Validation RMSE: 0.2387\n",
      "Epoch [2326001/10000000], Training Loss: 0.0581, Validation Loss: 0.0570, Training RMSE: 0.2410, Validation RMSE: 0.2387\n",
      "Epoch [2327001/10000000], Training Loss: 0.0585, Validation Loss: 0.0570, Training RMSE: 0.2419, Validation RMSE: 0.2386\n",
      "Epoch [2328001/10000000], Training Loss: 0.0578, Validation Loss: 0.0570, Training RMSE: 0.2404, Validation RMSE: 0.2386\n",
      "Epoch [2329001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2393, Validation RMSE: 0.2386\n",
      "Epoch [2330001/10000000], Training Loss: 0.0576, Validation Loss: 0.0569, Training RMSE: 0.2401, Validation RMSE: 0.2386\n",
      "Epoch [2331001/10000000], Training Loss: 0.0579, Validation Loss: 0.0569, Training RMSE: 0.2406, Validation RMSE: 0.2386\n",
      "Epoch [2332001/10000000], Training Loss: 0.0574, Validation Loss: 0.0569, Training RMSE: 0.2397, Validation RMSE: 0.2386\n",
      "Epoch [2333001/10000000], Training Loss: 0.0578, Validation Loss: 0.0569, Training RMSE: 0.2403, Validation RMSE: 0.2386\n",
      "Epoch [2334001/10000000], Training Loss: 0.0574, Validation Loss: 0.0569, Training RMSE: 0.2396, Validation RMSE: 0.2386\n",
      "Epoch [2335001/10000000], Training Loss: 0.0580, Validation Loss: 0.0569, Training RMSE: 0.2408, Validation RMSE: 0.2386\n",
      "Epoch [2336001/10000000], Training Loss: 0.0576, Validation Loss: 0.0569, Training RMSE: 0.2399, Validation RMSE: 0.2386\n",
      "Epoch [2337001/10000000], Training Loss: 0.0577, Validation Loss: 0.0569, Training RMSE: 0.2401, Validation RMSE: 0.2386\n",
      "Epoch [2338001/10000000], Training Loss: 0.0575, Validation Loss: 0.0569, Training RMSE: 0.2399, Validation RMSE: 0.2386\n",
      "Epoch [2339001/10000000], Training Loss: 0.0579, Validation Loss: 0.0569, Training RMSE: 0.2405, Validation RMSE: 0.2386\n",
      "Epoch [2340001/10000000], Training Loss: 0.0570, Validation Loss: 0.0569, Training RMSE: 0.2388, Validation RMSE: 0.2386\n",
      "Epoch [2341001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2393, Validation RMSE: 0.2386\n",
      "Epoch [2342001/10000000], Training Loss: 0.0568, Validation Loss: 0.0569, Training RMSE: 0.2383, Validation RMSE: 0.2386\n",
      "Epoch [2343001/10000000], Training Loss: 0.0577, Validation Loss: 0.0569, Training RMSE: 0.2403, Validation RMSE: 0.2386\n",
      "Epoch [2344001/10000000], Training Loss: 0.0578, Validation Loss: 0.0569, Training RMSE: 0.2405, Validation RMSE: 0.2386\n",
      "Epoch [2345001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2386, Validation RMSE: 0.2386\n",
      "Epoch [2346001/10000000], Training Loss: 0.0563, Validation Loss: 0.0569, Training RMSE: 0.2372, Validation RMSE: 0.2386\n",
      "Epoch [2347001/10000000], Training Loss: 0.0575, Validation Loss: 0.0569, Training RMSE: 0.2397, Validation RMSE: 0.2386\n",
      "Epoch [2348001/10000000], Training Loss: 0.0575, Validation Loss: 0.0569, Training RMSE: 0.2397, Validation RMSE: 0.2386\n",
      "Epoch [2349001/10000000], Training Loss: 0.0571, Validation Loss: 0.0569, Training RMSE: 0.2390, Validation RMSE: 0.2386\n",
      "Epoch [2350001/10000000], Training Loss: 0.0574, Validation Loss: 0.0569, Training RMSE: 0.2395, Validation RMSE: 0.2386\n",
      "Epoch [2351001/10000000], Training Loss: 0.0570, Validation Loss: 0.0569, Training RMSE: 0.2387, Validation RMSE: 0.2386\n",
      "Epoch [2352001/10000000], Training Loss: 0.0575, Validation Loss: 0.0569, Training RMSE: 0.2397, Validation RMSE: 0.2386\n",
      "Epoch [2353001/10000000], Training Loss: 0.0576, Validation Loss: 0.0569, Training RMSE: 0.2400, Validation RMSE: 0.2386\n",
      "Epoch [2354001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2386, Validation RMSE: 0.2386\n",
      "Epoch [2355001/10000000], Training Loss: 0.0566, Validation Loss: 0.0569, Training RMSE: 0.2378, Validation RMSE: 0.2385\n",
      "Epoch [2356001/10000000], Training Loss: 0.0565, Validation Loss: 0.0569, Training RMSE: 0.2376, Validation RMSE: 0.2385\n",
      "Epoch [2357001/10000000], Training Loss: 0.0568, Validation Loss: 0.0569, Training RMSE: 0.2383, Validation RMSE: 0.2385\n",
      "Epoch [2358001/10000000], Training Loss: 0.0572, Validation Loss: 0.0569, Training RMSE: 0.2393, Validation RMSE: 0.2385\n",
      "Epoch [2359001/10000000], Training Loss: 0.0564, Validation Loss: 0.0569, Training RMSE: 0.2374, Validation RMSE: 0.2385\n",
      "Epoch [2360001/10000000], Training Loss: 0.0572, Validation Loss: 0.0569, Training RMSE: 0.2392, Validation RMSE: 0.2385\n",
      "Epoch [2361001/10000000], Training Loss: 0.0568, Validation Loss: 0.0569, Training RMSE: 0.2384, Validation RMSE: 0.2385\n",
      "Epoch [2362001/10000000], Training Loss: 0.0575, Validation Loss: 0.0569, Training RMSE: 0.2398, Validation RMSE: 0.2385\n",
      "Epoch [2363001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2385, Validation RMSE: 0.2385\n",
      "Epoch [2364001/10000000], Training Loss: 0.0577, Validation Loss: 0.0569, Training RMSE: 0.2401, Validation RMSE: 0.2385\n",
      "Epoch [2365001/10000000], Training Loss: 0.0568, Validation Loss: 0.0569, Training RMSE: 0.2382, Validation RMSE: 0.2385\n",
      "Epoch [2366001/10000000], Training Loss: 0.0572, Validation Loss: 0.0569, Training RMSE: 0.2393, Validation RMSE: 0.2385\n",
      "Epoch [2367001/10000000], Training Loss: 0.0577, Validation Loss: 0.0569, Training RMSE: 0.2402, Validation RMSE: 0.2385\n",
      "Epoch [2368001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2385, Validation RMSE: 0.2385\n",
      "Epoch [2369001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2385, Validation RMSE: 0.2385\n",
      "Epoch [2370001/10000000], Training Loss: 0.0565, Validation Loss: 0.0569, Training RMSE: 0.2376, Validation RMSE: 0.2385\n",
      "Epoch [2371001/10000000], Training Loss: 0.0567, Validation Loss: 0.0569, Training RMSE: 0.2381, Validation RMSE: 0.2385\n",
      "Epoch [2372001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2394, Validation RMSE: 0.2385\n",
      "Epoch [2373001/10000000], Training Loss: 0.0567, Validation Loss: 0.0569, Training RMSE: 0.2382, Validation RMSE: 0.2385\n",
      "Epoch [2374001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2385, Validation RMSE: 0.2385\n",
      "Epoch [2375001/10000000], Training Loss: 0.0574, Validation Loss: 0.0569, Training RMSE: 0.2395, Validation RMSE: 0.2385\n",
      "Epoch [2376001/10000000], Training Loss: 0.0570, Validation Loss: 0.0569, Training RMSE: 0.2388, Validation RMSE: 0.2385\n",
      "Epoch [2377001/10000000], Training Loss: 0.0576, Validation Loss: 0.0569, Training RMSE: 0.2401, Validation RMSE: 0.2385\n",
      "Epoch [2378001/10000000], Training Loss: 0.0570, Validation Loss: 0.0569, Training RMSE: 0.2387, Validation RMSE: 0.2385\n",
      "Epoch [2379001/10000000], Training Loss: 0.0568, Validation Loss: 0.0569, Training RMSE: 0.2384, Validation RMSE: 0.2385\n",
      "Epoch [2380001/10000000], Training Loss: 0.0572, Validation Loss: 0.0569, Training RMSE: 0.2392, Validation RMSE: 0.2385\n",
      "Epoch [2381001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2393, Validation RMSE: 0.2385\n",
      "Epoch [2382001/10000000], Training Loss: 0.0565, Validation Loss: 0.0569, Training RMSE: 0.2378, Validation RMSE: 0.2385\n",
      "Epoch [2383001/10000000], Training Loss: 0.0569, Validation Loss: 0.0569, Training RMSE: 0.2386, Validation RMSE: 0.2385\n",
      "Epoch [2384001/10000000], Training Loss: 0.0563, Validation Loss: 0.0569, Training RMSE: 0.2372, Validation RMSE: 0.2385\n",
      "Epoch [2385001/10000000], Training Loss: 0.0570, Validation Loss: 0.0569, Training RMSE: 0.2388, Validation RMSE: 0.2385\n",
      "Epoch [2386001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2395, Validation RMSE: 0.2384\n",
      "Epoch [2387001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2394, Validation RMSE: 0.2384\n",
      "Epoch [2388001/10000000], Training Loss: 0.0560, Validation Loss: 0.0569, Training RMSE: 0.2366, Validation RMSE: 0.2384\n",
      "Epoch [2389001/10000000], Training Loss: 0.0573, Validation Loss: 0.0569, Training RMSE: 0.2394, Validation RMSE: 0.2384\n",
      "Epoch [2390001/10000000], Training Loss: 0.0567, Validation Loss: 0.0569, Training RMSE: 0.2382, Validation RMSE: 0.2384\n",
      "Epoch [2391001/10000000], Training Loss: 0.0576, Validation Loss: 0.0568, Training RMSE: 0.2400, Validation RMSE: 0.2384\n",
      "Epoch [2392001/10000000], Training Loss: 0.0570, Validation Loss: 0.0568, Training RMSE: 0.2387, Validation RMSE: 0.2384\n",
      "Epoch [2393001/10000000], Training Loss: 0.0573, Validation Loss: 0.0568, Training RMSE: 0.2393, Validation RMSE: 0.2384\n",
      "Epoch [2394001/10000000], Training Loss: 0.0569, Validation Loss: 0.0568, Training RMSE: 0.2385, Validation RMSE: 0.2384\n",
      "Epoch [2395001/10000000], Training Loss: 0.0572, Validation Loss: 0.0568, Training RMSE: 0.2391, Validation RMSE: 0.2384\n",
      "Epoch [2396001/10000000], Training Loss: 0.0572, Validation Loss: 0.0568, Training RMSE: 0.2393, Validation RMSE: 0.2384\n",
      "Epoch [2397001/10000000], Training Loss: 0.0567, Validation Loss: 0.0568, Training RMSE: 0.2382, Validation RMSE: 0.2384\n",
      "Epoch [2398001/10000000], Training Loss: 0.0576, Validation Loss: 0.0568, Training RMSE: 0.2401, Validation RMSE: 0.2384\n",
      "Epoch [2399001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2380, Validation RMSE: 0.2384\n",
      "Epoch [2400001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2379, Validation RMSE: 0.2384\n",
      "Epoch [2401001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2367, Validation RMSE: 0.2384\n",
      "Epoch [2402001/10000000], Training Loss: 0.0558, Validation Loss: 0.0568, Training RMSE: 0.2361, Validation RMSE: 0.2384\n",
      "Epoch [2403001/10000000], Training Loss: 0.0568, Validation Loss: 0.0568, Training RMSE: 0.2382, Validation RMSE: 0.2384\n",
      "Epoch [2404001/10000000], Training Loss: 0.0568, Validation Loss: 0.0568, Training RMSE: 0.2383, Validation RMSE: 0.2384\n",
      "Epoch [2405001/10000000], Training Loss: 0.0570, Validation Loss: 0.0568, Training RMSE: 0.2386, Validation RMSE: 0.2384\n",
      "Epoch [2406001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2379, Validation RMSE: 0.2384\n",
      "Epoch [2407001/10000000], Training Loss: 0.0573, Validation Loss: 0.0568, Training RMSE: 0.2394, Validation RMSE: 0.2384\n",
      "Epoch [2408001/10000000], Training Loss: 0.0564, Validation Loss: 0.0568, Training RMSE: 0.2374, Validation RMSE: 0.2384\n",
      "Epoch [2409001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2380, Validation RMSE: 0.2384\n",
      "Epoch [2410001/10000000], Training Loss: 0.0569, Validation Loss: 0.0568, Training RMSE: 0.2385, Validation RMSE: 0.2384\n",
      "Epoch [2411001/10000000], Training Loss: 0.0570, Validation Loss: 0.0568, Training RMSE: 0.2387, Validation RMSE: 0.2384\n",
      "Epoch [2412001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2378, Validation RMSE: 0.2384\n",
      "Epoch [2413001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2370, Validation RMSE: 0.2384\n",
      "Epoch [2414001/10000000], Training Loss: 0.0565, Validation Loss: 0.0568, Training RMSE: 0.2376, Validation RMSE: 0.2384\n",
      "Epoch [2415001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2371, Validation RMSE: 0.2384\n",
      "Epoch [2416001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2371, Validation RMSE: 0.2384\n",
      "Epoch [2417001/10000000], Training Loss: 0.0556, Validation Loss: 0.0568, Training RMSE: 0.2359, Validation RMSE: 0.2384\n",
      "Epoch [2418001/10000000], Training Loss: 0.0563, Validation Loss: 0.0568, Training RMSE: 0.2374, Validation RMSE: 0.2384\n",
      "Epoch [2419001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2368, Validation RMSE: 0.2383\n",
      "Epoch [2420001/10000000], Training Loss: 0.0579, Validation Loss: 0.0568, Training RMSE: 0.2405, Validation RMSE: 0.2383\n",
      "Epoch [2421001/10000000], Training Loss: 0.0563, Validation Loss: 0.0568, Training RMSE: 0.2373, Validation RMSE: 0.2383\n",
      "Epoch [2422001/10000000], Training Loss: 0.0570, Validation Loss: 0.0568, Training RMSE: 0.2387, Validation RMSE: 0.2383\n",
      "Epoch [2423001/10000000], Training Loss: 0.0557, Validation Loss: 0.0568, Training RMSE: 0.2361, Validation RMSE: 0.2383\n",
      "Epoch [2424001/10000000], Training Loss: 0.0554, Validation Loss: 0.0568, Training RMSE: 0.2353, Validation RMSE: 0.2383\n",
      "Epoch [2425001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2368, Validation RMSE: 0.2383\n",
      "Epoch [2426001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2368, Validation RMSE: 0.2383\n",
      "Epoch [2427001/10000000], Training Loss: 0.0568, Validation Loss: 0.0568, Training RMSE: 0.2384, Validation RMSE: 0.2383\n",
      "Epoch [2428001/10000000], Training Loss: 0.0565, Validation Loss: 0.0568, Training RMSE: 0.2378, Validation RMSE: 0.2383\n",
      "Epoch [2429001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2380, Validation RMSE: 0.2383\n",
      "Epoch [2430001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2371, Validation RMSE: 0.2383\n",
      "Epoch [2431001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2370, Validation RMSE: 0.2383\n",
      "Epoch [2432001/10000000], Training Loss: 0.0567, Validation Loss: 0.0568, Training RMSE: 0.2382, Validation RMSE: 0.2383\n",
      "Epoch [2433001/10000000], Training Loss: 0.0565, Validation Loss: 0.0568, Training RMSE: 0.2377, Validation RMSE: 0.2383\n",
      "Epoch [2434001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2366, Validation RMSE: 0.2383\n",
      "Epoch [2435001/10000000], Training Loss: 0.0567, Validation Loss: 0.0568, Training RMSE: 0.2382, Validation RMSE: 0.2383\n",
      "Epoch [2436001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2369, Validation RMSE: 0.2383\n",
      "Epoch [2437001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2367, Validation RMSE: 0.2383\n",
      "Epoch [2438001/10000000], Training Loss: 0.0563, Validation Loss: 0.0568, Training RMSE: 0.2372, Validation RMSE: 0.2383\n",
      "Epoch [2439001/10000000], Training Loss: 0.0555, Validation Loss: 0.0568, Training RMSE: 0.2355, Validation RMSE: 0.2383\n",
      "Epoch [2440001/10000000], Training Loss: 0.0567, Validation Loss: 0.0568, Training RMSE: 0.2381, Validation RMSE: 0.2383\n",
      "Epoch [2441001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2368, Validation RMSE: 0.2383\n",
      "Epoch [2442001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2370, Validation RMSE: 0.2383\n",
      "Epoch [2443001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2367, Validation RMSE: 0.2383\n",
      "Epoch [2444001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2367, Validation RMSE: 0.2383\n",
      "Epoch [2445001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2370, Validation RMSE: 0.2383\n",
      "Epoch [2446001/10000000], Training Loss: 0.0567, Validation Loss: 0.0568, Training RMSE: 0.2381, Validation RMSE: 0.2383\n",
      "Epoch [2447001/10000000], Training Loss: 0.0565, Validation Loss: 0.0568, Training RMSE: 0.2377, Validation RMSE: 0.2383\n",
      "Epoch [2448001/10000000], Training Loss: 0.0557, Validation Loss: 0.0568, Training RMSE: 0.2360, Validation RMSE: 0.2383\n",
      "Epoch [2449001/10000000], Training Loss: 0.0563, Validation Loss: 0.0568, Training RMSE: 0.2373, Validation RMSE: 0.2383\n",
      "Epoch [2450001/10000000], Training Loss: 0.0570, Validation Loss: 0.0568, Training RMSE: 0.2387, Validation RMSE: 0.2383\n",
      "Epoch [2451001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2378, Validation RMSE: 0.2383\n",
      "Epoch [2452001/10000000], Training Loss: 0.0558, Validation Loss: 0.0568, Training RMSE: 0.2363, Validation RMSE: 0.2383\n",
      "Epoch [2453001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2369, Validation RMSE: 0.2383\n",
      "Epoch [2454001/10000000], Training Loss: 0.0558, Validation Loss: 0.0568, Training RMSE: 0.2362, Validation RMSE: 0.2383\n",
      "Epoch [2455001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2371, Validation RMSE: 0.2383\n",
      "Epoch [2456001/10000000], Training Loss: 0.0559, Validation Loss: 0.0568, Training RMSE: 0.2364, Validation RMSE: 0.2382\n",
      "Epoch [2457001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2368, Validation RMSE: 0.2382\n",
      "Epoch [2458001/10000000], Training Loss: 0.0561, Validation Loss: 0.0568, Training RMSE: 0.2369, Validation RMSE: 0.2382\n",
      "Epoch [2459001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2367, Validation RMSE: 0.2382\n",
      "Epoch [2460001/10000000], Training Loss: 0.0566, Validation Loss: 0.0568, Training RMSE: 0.2378, Validation RMSE: 0.2382\n",
      "Epoch [2461001/10000000], Training Loss: 0.0554, Validation Loss: 0.0568, Training RMSE: 0.2353, Validation RMSE: 0.2382\n",
      "Epoch [2462001/10000000], Training Loss: 0.0557, Validation Loss: 0.0568, Training RMSE: 0.2361, Validation RMSE: 0.2382\n",
      "Epoch [2463001/10000000], Training Loss: 0.0560, Validation Loss: 0.0568, Training RMSE: 0.2366, Validation RMSE: 0.2382\n",
      "Epoch [2464001/10000000], Training Loss: 0.0557, Validation Loss: 0.0568, Training RMSE: 0.2359, Validation RMSE: 0.2382\n",
      "Epoch [2465001/10000000], Training Loss: 0.0564, Validation Loss: 0.0568, Training RMSE: 0.2375, Validation RMSE: 0.2382\n",
      "Epoch [2466001/10000000], Training Loss: 0.0562, Validation Loss: 0.0568, Training RMSE: 0.2372, Validation RMSE: 0.2382\n",
      "Epoch [2467001/10000000], Training Loss: 0.0557, Validation Loss: 0.0568, Training RMSE: 0.2361, Validation RMSE: 0.2382\n",
      "Epoch [2468001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2361, Validation RMSE: 0.2382\n",
      "Epoch [2469001/10000000], Training Loss: 0.0560, Validation Loss: 0.0567, Training RMSE: 0.2365, Validation RMSE: 0.2382\n",
      "Epoch [2470001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2359, Validation RMSE: 0.2382\n",
      "Epoch [2471001/10000000], Training Loss: 0.0564, Validation Loss: 0.0567, Training RMSE: 0.2375, Validation RMSE: 0.2382\n",
      "Epoch [2472001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2354, Validation RMSE: 0.2382\n",
      "Epoch [2473001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2382\n",
      "Epoch [2474001/10000000], Training Loss: 0.0559, Validation Loss: 0.0567, Training RMSE: 0.2364, Validation RMSE: 0.2382\n",
      "Epoch [2475001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2351, Validation RMSE: 0.2382\n",
      "Epoch [2476001/10000000], Training Loss: 0.0570, Validation Loss: 0.0567, Training RMSE: 0.2388, Validation RMSE: 0.2382\n",
      "Epoch [2477001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2356, Validation RMSE: 0.2382\n",
      "Epoch [2478001/10000000], Training Loss: 0.0562, Validation Loss: 0.0567, Training RMSE: 0.2370, Validation RMSE: 0.2382\n",
      "Epoch [2479001/10000000], Training Loss: 0.0564, Validation Loss: 0.0567, Training RMSE: 0.2374, Validation RMSE: 0.2382\n",
      "Epoch [2480001/10000000], Training Loss: 0.0560, Validation Loss: 0.0567, Training RMSE: 0.2367, Validation RMSE: 0.2382\n",
      "Epoch [2481001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2382\n",
      "Epoch [2482001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2382\n",
      "Epoch [2483001/10000000], Training Loss: 0.0556, Validation Loss: 0.0567, Training RMSE: 0.2358, Validation RMSE: 0.2382\n",
      "Epoch [2484001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2382\n",
      "Epoch [2485001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2351, Validation RMSE: 0.2382\n",
      "Epoch [2486001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2382\n",
      "Epoch [2487001/10000000], Training Loss: 0.0561, Validation Loss: 0.0567, Training RMSE: 0.2370, Validation RMSE: 0.2382\n",
      "Epoch [2488001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2356, Validation RMSE: 0.2382\n",
      "Epoch [2489001/10000000], Training Loss: 0.0562, Validation Loss: 0.0567, Training RMSE: 0.2370, Validation RMSE: 0.2382\n",
      "Epoch [2490001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2361, Validation RMSE: 0.2382\n",
      "Epoch [2491001/10000000], Training Loss: 0.0550, Validation Loss: 0.0567, Training RMSE: 0.2344, Validation RMSE: 0.2382\n",
      "Epoch [2492001/10000000], Training Loss: 0.0561, Validation Loss: 0.0567, Training RMSE: 0.2368, Validation RMSE: 0.2382\n",
      "Epoch [2493001/10000000], Training Loss: 0.0562, Validation Loss: 0.0567, Training RMSE: 0.2370, Validation RMSE: 0.2382\n",
      "Epoch [2494001/10000000], Training Loss: 0.0561, Validation Loss: 0.0567, Training RMSE: 0.2368, Validation RMSE: 0.2382\n",
      "Epoch [2495001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2351, Validation RMSE: 0.2382\n",
      "Epoch [2496001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2348, Validation RMSE: 0.2382\n",
      "Epoch [2497001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2361, Validation RMSE: 0.2382\n",
      "Epoch [2498001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2355, Validation RMSE: 0.2382\n",
      "Epoch [2499001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2382\n",
      "Epoch [2500001/10000000], Training Loss: 0.0559, Validation Loss: 0.0567, Training RMSE: 0.2365, Validation RMSE: 0.2382\n",
      "Epoch [2501001/10000000], Training Loss: 0.0559, Validation Loss: 0.0567, Training RMSE: 0.2364, Validation RMSE: 0.2381\n",
      "Epoch [2502001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2361, Validation RMSE: 0.2381\n",
      "Epoch [2503001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2381\n",
      "Epoch [2504001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2356, Validation RMSE: 0.2381\n",
      "Epoch [2505001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2381\n",
      "Epoch [2506001/10000000], Training Loss: 0.0560, Validation Loss: 0.0567, Training RMSE: 0.2366, Validation RMSE: 0.2381\n",
      "Epoch [2507001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2346, Validation RMSE: 0.2381\n",
      "Epoch [2508001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2359, Validation RMSE: 0.2381\n",
      "Epoch [2509001/10000000], Training Loss: 0.0556, Validation Loss: 0.0567, Training RMSE: 0.2357, Validation RMSE: 0.2381\n",
      "Epoch [2510001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2355, Validation RMSE: 0.2381\n",
      "Epoch [2511001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2351, Validation RMSE: 0.2381\n",
      "Epoch [2512001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2352, Validation RMSE: 0.2381\n",
      "Epoch [2513001/10000000], Training Loss: 0.0558, Validation Loss: 0.0567, Training RMSE: 0.2362, Validation RMSE: 0.2381\n",
      "Epoch [2514001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2356, Validation RMSE: 0.2381\n",
      "Epoch [2515001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2337, Validation RMSE: 0.2381\n",
      "Epoch [2516001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2381\n",
      "Epoch [2517001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2381\n",
      "Epoch [2518001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2381\n",
      "Epoch [2519001/10000000], Training Loss: 0.0556, Validation Loss: 0.0567, Training RMSE: 0.2357, Validation RMSE: 0.2381\n",
      "Epoch [2520001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2343, Validation RMSE: 0.2381\n",
      "Epoch [2521001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2381\n",
      "Epoch [2522001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2348, Validation RMSE: 0.2381\n",
      "Epoch [2523001/10000000], Training Loss: 0.0548, Validation Loss: 0.0567, Training RMSE: 0.2340, Validation RMSE: 0.2381\n",
      "Epoch [2524001/10000000], Training Loss: 0.0548, Validation Loss: 0.0567, Training RMSE: 0.2341, Validation RMSE: 0.2381\n",
      "Epoch [2525001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2349, Validation RMSE: 0.2381\n",
      "Epoch [2526001/10000000], Training Loss: 0.0559, Validation Loss: 0.0567, Training RMSE: 0.2364, Validation RMSE: 0.2381\n",
      "Epoch [2527001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2381\n",
      "Epoch [2528001/10000000], Training Loss: 0.0560, Validation Loss: 0.0567, Training RMSE: 0.2366, Validation RMSE: 0.2381\n",
      "Epoch [2529001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2352, Validation RMSE: 0.2381\n",
      "Epoch [2530001/10000000], Training Loss: 0.0544, Validation Loss: 0.0567, Training RMSE: 0.2332, Validation RMSE: 0.2381\n",
      "Epoch [2531001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2359, Validation RMSE: 0.2381\n",
      "Epoch [2532001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2352, Validation RMSE: 0.2381\n",
      "Epoch [2533001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2381\n",
      "Epoch [2534001/10000000], Training Loss: 0.0547, Validation Loss: 0.0567, Training RMSE: 0.2339, Validation RMSE: 0.2381\n",
      "Epoch [2535001/10000000], Training Loss: 0.0548, Validation Loss: 0.0567, Training RMSE: 0.2340, Validation RMSE: 0.2381\n",
      "Epoch [2536001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2381\n",
      "Epoch [2537001/10000000], Training Loss: 0.0541, Validation Loss: 0.0567, Training RMSE: 0.2325, Validation RMSE: 0.2381\n",
      "Epoch [2538001/10000000], Training Loss: 0.0557, Validation Loss: 0.0567, Training RMSE: 0.2360, Validation RMSE: 0.2381\n",
      "Epoch [2539001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2344, Validation RMSE: 0.2381\n",
      "Epoch [2540001/10000000], Training Loss: 0.0544, Validation Loss: 0.0567, Training RMSE: 0.2332, Validation RMSE: 0.2381\n",
      "Epoch [2541001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2355, Validation RMSE: 0.2381\n",
      "Epoch [2542001/10000000], Training Loss: 0.0548, Validation Loss: 0.0567, Training RMSE: 0.2341, Validation RMSE: 0.2381\n",
      "Epoch [2543001/10000000], Training Loss: 0.0545, Validation Loss: 0.0567, Training RMSE: 0.2335, Validation RMSE: 0.2381\n",
      "Epoch [2544001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2381\n",
      "Epoch [2545001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2381\n",
      "Epoch [2546001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2355, Validation RMSE: 0.2381\n",
      "Epoch [2547001/10000000], Training Loss: 0.0539, Validation Loss: 0.0567, Training RMSE: 0.2322, Validation RMSE: 0.2381\n",
      "Epoch [2548001/10000000], Training Loss: 0.0550, Validation Loss: 0.0567, Training RMSE: 0.2345, Validation RMSE: 0.2381\n",
      "Epoch [2549001/10000000], Training Loss: 0.0548, Validation Loss: 0.0567, Training RMSE: 0.2341, Validation RMSE: 0.2381\n",
      "Epoch [2550001/10000000], Training Loss: 0.0545, Validation Loss: 0.0567, Training RMSE: 0.2335, Validation RMSE: 0.2381\n",
      "Epoch [2551001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2349, Validation RMSE: 0.2381\n",
      "Epoch [2552001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2381\n",
      "Epoch [2553001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2381\n",
      "Epoch [2554001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2344, Validation RMSE: 0.2381\n",
      "Epoch [2555001/10000000], Training Loss: 0.0547, Validation Loss: 0.0567, Training RMSE: 0.2339, Validation RMSE: 0.2381\n",
      "Epoch [2556001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2354, Validation RMSE: 0.2381\n",
      "Epoch [2557001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2351, Validation RMSE: 0.2381\n",
      "Epoch [2558001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2337, Validation RMSE: 0.2380\n",
      "Epoch [2559001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2380\n",
      "Epoch [2560001/10000000], Training Loss: 0.0547, Validation Loss: 0.0567, Training RMSE: 0.2340, Validation RMSE: 0.2380\n",
      "Epoch [2561001/10000000], Training Loss: 0.0542, Validation Loss: 0.0567, Training RMSE: 0.2329, Validation RMSE: 0.2380\n",
      "Epoch [2562001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2347, Validation RMSE: 0.2380\n",
      "Epoch [2563001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2344, Validation RMSE: 0.2380\n",
      "Epoch [2564001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2337, Validation RMSE: 0.2380\n",
      "Epoch [2565001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2348, Validation RMSE: 0.2380\n",
      "Epoch [2566001/10000000], Training Loss: 0.0550, Validation Loss: 0.0567, Training RMSE: 0.2345, Validation RMSE: 0.2380\n",
      "Epoch [2567001/10000000], Training Loss: 0.0547, Validation Loss: 0.0567, Training RMSE: 0.2338, Validation RMSE: 0.2380\n",
      "Epoch [2568001/10000000], Training Loss: 0.0556, Validation Loss: 0.0567, Training RMSE: 0.2357, Validation RMSE: 0.2380\n",
      "Epoch [2569001/10000000], Training Loss: 0.0551, Validation Loss: 0.0567, Training RMSE: 0.2348, Validation RMSE: 0.2380\n",
      "Epoch [2570001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2355, Validation RMSE: 0.2380\n",
      "Epoch [2571001/10000000], Training Loss: 0.0555, Validation Loss: 0.0567, Training RMSE: 0.2356, Validation RMSE: 0.2380\n",
      "Epoch [2572001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2343, Validation RMSE: 0.2380\n",
      "Epoch [2573001/10000000], Training Loss: 0.0547, Validation Loss: 0.0567, Training RMSE: 0.2339, Validation RMSE: 0.2380\n",
      "Epoch [2574001/10000000], Training Loss: 0.0552, Validation Loss: 0.0567, Training RMSE: 0.2350, Validation RMSE: 0.2380\n",
      "Epoch [2575001/10000000], Training Loss: 0.0549, Validation Loss: 0.0567, Training RMSE: 0.2344, Validation RMSE: 0.2380\n",
      "Epoch [2576001/10000000], Training Loss: 0.0554, Validation Loss: 0.0567, Training RMSE: 0.2353, Validation RMSE: 0.2380\n",
      "Epoch [2577001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2337, Validation RMSE: 0.2380\n",
      "Epoch [2578001/10000000], Training Loss: 0.0544, Validation Loss: 0.0567, Training RMSE: 0.2332, Validation RMSE: 0.2380\n",
      "Epoch [2579001/10000000], Training Loss: 0.0544, Validation Loss: 0.0567, Training RMSE: 0.2333, Validation RMSE: 0.2380\n",
      "Epoch [2580001/10000000], Training Loss: 0.0537, Validation Loss: 0.0567, Training RMSE: 0.2318, Validation RMSE: 0.2380\n",
      "Epoch [2581001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2336, Validation RMSE: 0.2380\n",
      "Epoch [2582001/10000000], Training Loss: 0.0553, Validation Loss: 0.0567, Training RMSE: 0.2352, Validation RMSE: 0.2380\n",
      "Epoch [2583001/10000000], Training Loss: 0.0545, Validation Loss: 0.0567, Training RMSE: 0.2334, Validation RMSE: 0.2380\n",
      "Epoch [2584001/10000000], Training Loss: 0.0546, Validation Loss: 0.0567, Training RMSE: 0.2336, Validation RMSE: 0.2380\n",
      "Epoch [2585001/10000000], Training Loss: 0.0544, Validation Loss: 0.0567, Training RMSE: 0.2333, Validation RMSE: 0.2380\n",
      "Epoch [2586001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2380\n",
      "Epoch [2587001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2327, Validation RMSE: 0.2380\n",
      "Epoch [2588001/10000000], Training Loss: 0.0551, Validation Loss: 0.0566, Training RMSE: 0.2346, Validation RMSE: 0.2380\n",
      "Epoch [2589001/10000000], Training Loss: 0.0548, Validation Loss: 0.0566, Training RMSE: 0.2342, Validation RMSE: 0.2380\n",
      "Epoch [2590001/10000000], Training Loss: 0.0545, Validation Loss: 0.0566, Training RMSE: 0.2335, Validation RMSE: 0.2380\n",
      "Epoch [2591001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2332, Validation RMSE: 0.2380\n",
      "Epoch [2592001/10000000], Training Loss: 0.0551, Validation Loss: 0.0566, Training RMSE: 0.2348, Validation RMSE: 0.2380\n",
      "Epoch [2593001/10000000], Training Loss: 0.0546, Validation Loss: 0.0566, Training RMSE: 0.2337, Validation RMSE: 0.2380\n",
      "Epoch [2594001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2325, Validation RMSE: 0.2380\n",
      "Epoch [2595001/10000000], Training Loss: 0.0549, Validation Loss: 0.0566, Training RMSE: 0.2343, Validation RMSE: 0.2380\n",
      "Epoch [2596001/10000000], Training Loss: 0.0547, Validation Loss: 0.0566, Training RMSE: 0.2340, Validation RMSE: 0.2380\n",
      "Epoch [2597001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2328, Validation RMSE: 0.2380\n",
      "Epoch [2598001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2380\n",
      "Epoch [2599001/10000000], Training Loss: 0.0553, Validation Loss: 0.0566, Training RMSE: 0.2351, Validation RMSE: 0.2380\n",
      "Epoch [2600001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2331, Validation RMSE: 0.2380\n",
      "Epoch [2601001/10000000], Training Loss: 0.0548, Validation Loss: 0.0566, Training RMSE: 0.2341, Validation RMSE: 0.2380\n",
      "Epoch [2602001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2331, Validation RMSE: 0.2380\n",
      "Epoch [2603001/10000000], Training Loss: 0.0547, Validation Loss: 0.0566, Training RMSE: 0.2340, Validation RMSE: 0.2380\n",
      "Epoch [2604001/10000000], Training Loss: 0.0546, Validation Loss: 0.0566, Training RMSE: 0.2338, Validation RMSE: 0.2380\n",
      "Epoch [2605001/10000000], Training Loss: 0.0550, Validation Loss: 0.0566, Training RMSE: 0.2346, Validation RMSE: 0.2380\n",
      "Epoch [2606001/10000000], Training Loss: 0.0553, Validation Loss: 0.0566, Training RMSE: 0.2352, Validation RMSE: 0.2380\n",
      "Epoch [2607001/10000000], Training Loss: 0.0547, Validation Loss: 0.0566, Training RMSE: 0.2340, Validation RMSE: 0.2380\n",
      "Epoch [2608001/10000000], Training Loss: 0.0549, Validation Loss: 0.0566, Training RMSE: 0.2343, Validation RMSE: 0.2380\n",
      "Epoch [2609001/10000000], Training Loss: 0.0548, Validation Loss: 0.0566, Training RMSE: 0.2340, Validation RMSE: 0.2380\n",
      "Epoch [2610001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2321, Validation RMSE: 0.2380\n",
      "Epoch [2611001/10000000], Training Loss: 0.0545, Validation Loss: 0.0566, Training RMSE: 0.2333, Validation RMSE: 0.2380\n",
      "Epoch [2612001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2380\n",
      "Epoch [2613001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2380\n",
      "Epoch [2614001/10000000], Training Loss: 0.0552, Validation Loss: 0.0566, Training RMSE: 0.2348, Validation RMSE: 0.2380\n",
      "Epoch [2615001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2380\n",
      "Epoch [2616001/10000000], Training Loss: 0.0537, Validation Loss: 0.0566, Training RMSE: 0.2317, Validation RMSE: 0.2380\n",
      "Epoch [2617001/10000000], Training Loss: 0.0546, Validation Loss: 0.0566, Training RMSE: 0.2338, Validation RMSE: 0.2380\n",
      "Epoch [2618001/10000000], Training Loss: 0.0550, Validation Loss: 0.0566, Training RMSE: 0.2346, Validation RMSE: 0.2380\n",
      "Epoch [2619001/10000000], Training Loss: 0.0554, Validation Loss: 0.0566, Training RMSE: 0.2354, Validation RMSE: 0.2380\n",
      "Epoch [2620001/10000000], Training Loss: 0.0545, Validation Loss: 0.0566, Training RMSE: 0.2334, Validation RMSE: 0.2380\n",
      "Epoch [2621001/10000000], Training Loss: 0.0550, Validation Loss: 0.0566, Training RMSE: 0.2345, Validation RMSE: 0.2380\n",
      "Epoch [2622001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2380\n",
      "Epoch [2623001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2380\n",
      "Epoch [2624001/10000000], Training Loss: 0.0547, Validation Loss: 0.0566, Training RMSE: 0.2339, Validation RMSE: 0.2380\n",
      "Epoch [2625001/10000000], Training Loss: 0.0546, Validation Loss: 0.0566, Training RMSE: 0.2337, Validation RMSE: 0.2380\n",
      "Epoch [2626001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2325, Validation RMSE: 0.2380\n",
      "Epoch [2627001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2327, Validation RMSE: 0.2380\n",
      "Epoch [2628001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2380\n",
      "Epoch [2629001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2325, Validation RMSE: 0.2380\n",
      "Epoch [2630001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2329, Validation RMSE: 0.2380\n",
      "Epoch [2631001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2330, Validation RMSE: 0.2380\n",
      "Epoch [2632001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2380\n",
      "Epoch [2633001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2333, Validation RMSE: 0.2380\n",
      "Epoch [2634001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2327, Validation RMSE: 0.2380\n",
      "Epoch [2635001/10000000], Training Loss: 0.0547, Validation Loss: 0.0566, Training RMSE: 0.2338, Validation RMSE: 0.2380\n",
      "Epoch [2636001/10000000], Training Loss: 0.0552, Validation Loss: 0.0566, Training RMSE: 0.2348, Validation RMSE: 0.2380\n",
      "Epoch [2637001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2324, Validation RMSE: 0.2380\n",
      "Epoch [2638001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2380\n",
      "Epoch [2639001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2380\n",
      "Epoch [2640001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2315, Validation RMSE: 0.2380\n",
      "Epoch [2641001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2332, Validation RMSE: 0.2380\n",
      "Epoch [2642001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2325, Validation RMSE: 0.2380\n",
      "Epoch [2643001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2331, Validation RMSE: 0.2380\n",
      "Epoch [2644001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2331, Validation RMSE: 0.2380\n",
      "Epoch [2645001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2380\n",
      "Epoch [2646001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2380\n",
      "Epoch [2647001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2380\n",
      "Epoch [2648001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2380\n",
      "Epoch [2649001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2327, Validation RMSE: 0.2380\n",
      "Epoch [2650001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2320, Validation RMSE: 0.2380\n",
      "Epoch [2651001/10000000], Training Loss: 0.0535, Validation Loss: 0.0566, Training RMSE: 0.2313, Validation RMSE: 0.2380\n",
      "Epoch [2652001/10000000], Training Loss: 0.0548, Validation Loss: 0.0566, Training RMSE: 0.2341, Validation RMSE: 0.2380\n",
      "Epoch [2653001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2324, Validation RMSE: 0.2380\n",
      "Epoch [2654001/10000000], Training Loss: 0.0534, Validation Loss: 0.0566, Training RMSE: 0.2312, Validation RMSE: 0.2380\n",
      "Epoch [2655001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2327, Validation RMSE: 0.2379\n",
      "Epoch [2656001/10000000], Training Loss: 0.0534, Validation Loss: 0.0566, Training RMSE: 0.2312, Validation RMSE: 0.2379\n",
      "Epoch [2657001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2302, Validation RMSE: 0.2379\n",
      "Epoch [2658001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2316, Validation RMSE: 0.2379\n",
      "Epoch [2659001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2306, Validation RMSE: 0.2379\n",
      "Epoch [2660001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2328, Validation RMSE: 0.2379\n",
      "Epoch [2661001/10000000], Training Loss: 0.0542, Validation Loss: 0.0566, Training RMSE: 0.2329, Validation RMSE: 0.2379\n",
      "Epoch [2662001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2379\n",
      "Epoch [2663001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2330, Validation RMSE: 0.2379\n",
      "Epoch [2664001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2315, Validation RMSE: 0.2379\n",
      "Epoch [2665001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2332, Validation RMSE: 0.2379\n",
      "Epoch [2666001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2379\n",
      "Epoch [2667001/10000000], Training Loss: 0.0533, Validation Loss: 0.0566, Training RMSE: 0.2310, Validation RMSE: 0.2379\n",
      "Epoch [2668001/10000000], Training Loss: 0.0544, Validation Loss: 0.0566, Training RMSE: 0.2333, Validation RMSE: 0.2379\n",
      "Epoch [2669001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2379\n",
      "Epoch [2670001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2379\n",
      "Epoch [2671001/10000000], Training Loss: 0.0545, Validation Loss: 0.0566, Training RMSE: 0.2334, Validation RMSE: 0.2379\n",
      "Epoch [2672001/10000000], Training Loss: 0.0533, Validation Loss: 0.0566, Training RMSE: 0.2309, Validation RMSE: 0.2379\n",
      "Epoch [2673001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2305, Validation RMSE: 0.2379\n",
      "Epoch [2674001/10000000], Training Loss: 0.0535, Validation Loss: 0.0566, Training RMSE: 0.2314, Validation RMSE: 0.2379\n",
      "Epoch [2675001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2379\n",
      "Epoch [2676001/10000000], Training Loss: 0.0535, Validation Loss: 0.0566, Training RMSE: 0.2314, Validation RMSE: 0.2379\n",
      "Epoch [2677001/10000000], Training Loss: 0.0541, Validation Loss: 0.0566, Training RMSE: 0.2326, Validation RMSE: 0.2379\n",
      "Epoch [2678001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2305, Validation RMSE: 0.2379\n",
      "Epoch [2679001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2321, Validation RMSE: 0.2379\n",
      "Epoch [2680001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2306, Validation RMSE: 0.2379\n",
      "Epoch [2681001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2321, Validation RMSE: 0.2379\n",
      "Epoch [2682001/10000000], Training Loss: 0.0533, Validation Loss: 0.0566, Training RMSE: 0.2308, Validation RMSE: 0.2379\n",
      "Epoch [2683001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2379\n",
      "Epoch [2684001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2379\n",
      "Epoch [2685001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2307, Validation RMSE: 0.2379\n",
      "Epoch [2686001/10000000], Training Loss: 0.0534, Validation Loss: 0.0566, Training RMSE: 0.2311, Validation RMSE: 0.2379\n",
      "Epoch [2687001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2307, Validation RMSE: 0.2379\n",
      "Epoch [2688001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2296, Validation RMSE: 0.2379\n",
      "Epoch [2689001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2307, Validation RMSE: 0.2379\n",
      "Epoch [2690001/10000000], Training Loss: 0.0537, Validation Loss: 0.0566, Training RMSE: 0.2317, Validation RMSE: 0.2379\n",
      "Epoch [2691001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2301, Validation RMSE: 0.2379\n",
      "Epoch [2692001/10000000], Training Loss: 0.0537, Validation Loss: 0.0566, Training RMSE: 0.2318, Validation RMSE: 0.2379\n",
      "Epoch [2693001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2379\n",
      "Epoch [2694001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2319, Validation RMSE: 0.2379\n",
      "Epoch [2695001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2306, Validation RMSE: 0.2379\n",
      "Epoch [2696001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2379\n",
      "Epoch [2697001/10000000], Training Loss: 0.0540, Validation Loss: 0.0566, Training RMSE: 0.2323, Validation RMSE: 0.2379\n",
      "Epoch [2698001/10000000], Training Loss: 0.0528, Validation Loss: 0.0566, Training RMSE: 0.2298, Validation RMSE: 0.2379\n",
      "Epoch [2699001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2302, Validation RMSE: 0.2379\n",
      "Epoch [2700001/10000000], Training Loss: 0.0528, Validation Loss: 0.0566, Training RMSE: 0.2298, Validation RMSE: 0.2379\n",
      "Epoch [2701001/10000000], Training Loss: 0.0529, Validation Loss: 0.0566, Training RMSE: 0.2301, Validation RMSE: 0.2379\n",
      "Epoch [2702001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2379\n",
      "Epoch [2703001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2379\n",
      "Epoch [2704001/10000000], Training Loss: 0.0526, Validation Loss: 0.0566, Training RMSE: 0.2294, Validation RMSE: 0.2379\n",
      "Epoch [2705001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2301, Validation RMSE: 0.2379\n",
      "Epoch [2706001/10000000], Training Loss: 0.0538, Validation Loss: 0.0566, Training RMSE: 0.2320, Validation RMSE: 0.2379\n",
      "Epoch [2707001/10000000], Training Loss: 0.0529, Validation Loss: 0.0566, Training RMSE: 0.2300, Validation RMSE: 0.2379\n",
      "Epoch [2708001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2379\n",
      "Epoch [2709001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2305, Validation RMSE: 0.2379\n",
      "Epoch [2710001/10000000], Training Loss: 0.0528, Validation Loss: 0.0566, Training RMSE: 0.2298, Validation RMSE: 0.2379\n",
      "Epoch [2711001/10000000], Training Loss: 0.0543, Validation Loss: 0.0566, Training RMSE: 0.2330, Validation RMSE: 0.2379\n",
      "Epoch [2712001/10000000], Training Loss: 0.0533, Validation Loss: 0.0566, Training RMSE: 0.2308, Validation RMSE: 0.2379\n",
      "Epoch [2713001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2315, Validation RMSE: 0.2379\n",
      "Epoch [2714001/10000000], Training Loss: 0.0526, Validation Loss: 0.0566, Training RMSE: 0.2294, Validation RMSE: 0.2379\n",
      "Epoch [2715001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2379\n",
      "Epoch [2716001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2296, Validation RMSE: 0.2379\n",
      "Epoch [2717001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2304, Validation RMSE: 0.2379\n",
      "Epoch [2718001/10000000], Training Loss: 0.0537, Validation Loss: 0.0566, Training RMSE: 0.2316, Validation RMSE: 0.2379\n",
      "Epoch [2719001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2302, Validation RMSE: 0.2379\n",
      "Epoch [2720001/10000000], Training Loss: 0.0539, Validation Loss: 0.0566, Training RMSE: 0.2322, Validation RMSE: 0.2379\n",
      "Epoch [2721001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2305, Validation RMSE: 0.2379\n",
      "Epoch [2722001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2296, Validation RMSE: 0.2379\n",
      "Epoch [2723001/10000000], Training Loss: 0.0522, Validation Loss: 0.0566, Training RMSE: 0.2284, Validation RMSE: 0.2379\n",
      "Epoch [2724001/10000000], Training Loss: 0.0534, Validation Loss: 0.0566, Training RMSE: 0.2311, Validation RMSE: 0.2379\n",
      "Epoch [2725001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2314, Validation RMSE: 0.2379\n",
      "Epoch [2726001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2296, Validation RMSE: 0.2379\n",
      "Epoch [2727001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2316, Validation RMSE: 0.2379\n",
      "Epoch [2728001/10000000], Training Loss: 0.0535, Validation Loss: 0.0566, Training RMSE: 0.2312, Validation RMSE: 0.2379\n",
      "Epoch [2729001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2307, Validation RMSE: 0.2379\n",
      "Epoch [2730001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2303, Validation RMSE: 0.2379\n",
      "Epoch [2731001/10000000], Training Loss: 0.0526, Validation Loss: 0.0566, Training RMSE: 0.2294, Validation RMSE: 0.2379\n",
      "Epoch [2732001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2315, Validation RMSE: 0.2379\n",
      "Epoch [2733001/10000000], Training Loss: 0.0529, Validation Loss: 0.0566, Training RMSE: 0.2299, Validation RMSE: 0.2379\n",
      "Epoch [2734001/10000000], Training Loss: 0.0521, Validation Loss: 0.0566, Training RMSE: 0.2283, Validation RMSE: 0.2379\n",
      "Epoch [2735001/10000000], Training Loss: 0.0529, Validation Loss: 0.0566, Training RMSE: 0.2299, Validation RMSE: 0.2379\n",
      "Epoch [2736001/10000000], Training Loss: 0.0529, Validation Loss: 0.0566, Training RMSE: 0.2299, Validation RMSE: 0.2379\n",
      "Epoch [2737001/10000000], Training Loss: 0.0528, Validation Loss: 0.0566, Training RMSE: 0.2297, Validation RMSE: 0.2379\n",
      "Epoch [2738001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2304, Validation RMSE: 0.2379\n",
      "Epoch [2739001/10000000], Training Loss: 0.0536, Validation Loss: 0.0566, Training RMSE: 0.2316, Validation RMSE: 0.2379\n",
      "Epoch [2740001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2297, Validation RMSE: 0.2379\n",
      "Epoch [2741001/10000000], Training Loss: 0.0530, Validation Loss: 0.0566, Training RMSE: 0.2302, Validation RMSE: 0.2379\n",
      "Epoch [2742001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2305, Validation RMSE: 0.2379\n",
      "Epoch [2743001/10000000], Training Loss: 0.0527, Validation Loss: 0.0566, Training RMSE: 0.2296, Validation RMSE: 0.2379\n",
      "Epoch [2744001/10000000], Training Loss: 0.0526, Validation Loss: 0.0566, Training RMSE: 0.2294, Validation RMSE: 0.2379\n",
      "Epoch [2745001/10000000], Training Loss: 0.0528, Validation Loss: 0.0566, Training RMSE: 0.2298, Validation RMSE: 0.2379\n",
      "Epoch [2746001/10000000], Training Loss: 0.0526, Validation Loss: 0.0566, Training RMSE: 0.2294, Validation RMSE: 0.2379\n",
      "Epoch [2747001/10000000], Training Loss: 0.0525, Validation Loss: 0.0566, Training RMSE: 0.2291, Validation RMSE: 0.2379\n",
      "Epoch [2748001/10000000], Training Loss: 0.0524, Validation Loss: 0.0566, Training RMSE: 0.2288, Validation RMSE: 0.2379\n",
      "Epoch [2749001/10000000], Training Loss: 0.0524, Validation Loss: 0.0566, Training RMSE: 0.2290, Validation RMSE: 0.2379\n",
      "Epoch [2750001/10000000], Training Loss: 0.0537, Validation Loss: 0.0566, Training RMSE: 0.2316, Validation RMSE: 0.2379\n",
      "Epoch [2751001/10000000], Training Loss: 0.0531, Validation Loss: 0.0566, Training RMSE: 0.2304, Validation RMSE: 0.2379\n",
      "Epoch [2752001/10000000], Training Loss: 0.0532, Validation Loss: 0.0566, Training RMSE: 0.2307, Validation RMSE: 0.2379\n",
      "Epoch [2753001/10000000], Training Loss: 0.0524, Validation Loss: 0.0566, Training RMSE: 0.2290, Validation RMSE: 0.2379\n",
      "Stopping training after 2753219 epochs. Best RMSE 0.2379 achieved at epoch 2743218.\n",
      "Training completed. Best RMSE 0.2379 achieved at epoch 2743218.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0000001\n",
    "epochs = 10000000\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the model and move to the device\n",
    "model_reg = RegressionNN(X_train.shape[1]).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.HuberLoss()  # Huber Loss gives 0.24 RMSE\n",
    "weight_decay_value = 1e-5  # This is a hyperparameter and can be adjusted\n",
    "optimizer = torch.optim.Adam(model_reg.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# Helper function to compute RMSE\n",
    "def compute_rmse(mse_loss):\n",
    "    return torch.sqrt(mse_loss)\n",
    "\n",
    "# Training loop\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_rmse = []\n",
    "validation_rmse = []\n",
    "\n",
    "best_rmse = float('inf')  # start with a very high value\n",
    "best_epoch = -1  # an invalid epoch number to start with\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode and ensure it's on the right device\n",
    "    model_reg.train().to(device)\n",
    "    \n",
    "    # Forward pass on training data\n",
    "    train_outputs = model_reg(X_train_tensor)\n",
    "    train_loss = criterion(train_outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model_reg.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass on validation data\n",
    "        val_outputs = model_reg(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Store the losses for plotting\n",
    "    training_losses.append(train_loss.item())\n",
    "    validation_losses.append(val_loss.item())\n",
    "    \n",
    "    # Store the RMSE for plotting\n",
    "    training_rmse.append(compute_rmse(train_loss).item())\n",
    "    validation_rmse.append(compute_rmse(val_loss).item())\n",
    "    \n",
    "    current_val_rmse = compute_rmse(val_loss).item()\n",
    "\n",
    "    # Check if the current RMSE is better than the best RMSE\n",
    "    if current_val_rmse < best_rmse:\n",
    "        best_rmse = current_val_rmse\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        # Stop training after a certain number of epochs without improvement\n",
    "        if epoch - best_epoch > 10000:\n",
    "            print(f\"Stopping training after {epoch} epochs. Best RMSE {best_rmse:.4f} achieved at epoch {best_epoch}.\")\n",
    "            break\n",
    "    \n",
    "    # Your original printing code\n",
    "    if 100 < epoch < 300 and epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}, Training RMSE: {compute_rmse(train_loss).item():.4f}, Validation RMSE: {compute_rmse(val_loss).item():.4f}')\n",
    "    elif 300 < epoch < 1000 and epoch % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}, Training RMSE: {compute_rmse(train_loss).item():.4f}, Validation RMSE: {compute_rmse(val_loss).item():.4f}')\n",
    "    elif epoch > 1000 and epoch % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}, Training RMSE: {compute_rmse(train_loss).item():.4f}, Validation RMSE: {compute_rmse(val_loss).item():.4f}')\n",
    "\n",
    "# If the loop completed without breaking, print the best RMSE achieved\n",
    "if best_epoch != epoch:\n",
    "    print(f\"Training completed. Best RMSE {best_rmse:.4f} achieved at epoch {best_epoch}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Training and Validation Loss (MSE)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (MSE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Training and Validation RMSE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_rmse, label='Training RMSE')\n",
    "plt.plot(validation_rmse, label='Validation RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the test sentences\n",
    "sentences = list(df_test.words_str.values)\n",
    "X_test = model_enc.encode(sentences)\n",
    "\n",
    "# Convert to a PyTorch tensor and move to the device\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get predictions with the neural network\n",
    "model_reg.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_hat_tensor = model_reg(X_test_tensor)\n",
    "\n",
    "# Convert the predictions back to a numpy array\n",
    "y_hat = y_hat_tensor.cpu().numpy()\n",
    "\n",
    "# Save the results with the specified format\n",
    "directory = 'results'\n",
    "np.save(os.path.join(directory, f'{team_id}__{split}__reg_pred.npy'), y_hat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
