{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 11:58:55.148697: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 11:58:55.821548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes ['negative' 'neutral' 'positive']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "X: (8000,)\n",
      "y: (8000,) [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "X = df['words_str']\n",
    "y_text = df['sentiment']\n",
    "# y_text = df.sentiment.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_text)\n",
    "print(f'Original classes {le.classes_}')\n",
    "print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "y =le.transform(y_text)\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape} {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_twitter = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_twitter\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.astype('int') # Change to integer type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to long type for classification\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)\n",
    "\n",
    "class BertClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class RobertaClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    \n",
    "class RobertaClassificationTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter, self).__init__()\n",
    "        self.num_labels = 3\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Function to compute f1_macro\n",
    "def f1_macro(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {'f1_macro': f1_score(labels, predictions, average='macro')}\n",
    "\n",
    "\n",
    "\n",
    "class RobertaClassificationTwitter_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter_2, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=4):\n",
    "        self.patience = patience\n",
    "        self.best_score = None\n",
    "        self.early_stop_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        rmse = metrics['eval_rmse']  # Make sure this key matches what's returned by your compute_metrics function\n",
    "        if self.best_score is None or rmse < self.best_score:\n",
    "            self.best_score = rmse\n",
    "            self.early_stop_counter = 0\n",
    "        else:\n",
    "            self.early_stop_counter += 1\n",
    "            if self.early_stop_counter >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "    \n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics['f1_macro'] # Make sure this key matches what's returned by your compute_metrics function\n",
    "        if f1 > 0.8:\n",
    "            control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertClassification.from_pretrained('bert-base-uncased')\n",
    "model_roberta = RobertaClassification.from_pretrained('roberta-base')\n",
    "model_twitter = RobertaClassificationTwitter_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22422' max='200000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22422/200000 1:15:29 < 9:57:57, 4.95 it/s, Epoch 112.11/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.387687</td>\n",
       "      <td>0.753129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.407478</td>\n",
       "      <td>0.751934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.416503</td>\n",
       "      <td>0.751790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.430577</td>\n",
       "      <td>0.757609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.433305</td>\n",
       "      <td>0.757738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.436715</td>\n",
       "      <td>0.756032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.442413</td>\n",
       "      <td>0.756381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.445989</td>\n",
       "      <td>0.754604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.451604</td>\n",
       "      <td>0.757205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.455440</td>\n",
       "      <td>0.754332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.458356</td>\n",
       "      <td>0.754332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.463412</td>\n",
       "      <td>0.757168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.467709</td>\n",
       "      <td>0.760591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.467507</td>\n",
       "      <td>0.754684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.478969</td>\n",
       "      <td>0.749302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.486041</td>\n",
       "      <td>0.752107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.485791</td>\n",
       "      <td>0.762465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.489527</td>\n",
       "      <td>0.762465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.491931</td>\n",
       "      <td>0.762465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.495843</td>\n",
       "      <td>0.760235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.500175</td>\n",
       "      <td>0.756136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.506314</td>\n",
       "      <td>0.756319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.507906</td>\n",
       "      <td>0.756206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.510060</td>\n",
       "      <td>0.756206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.512006</td>\n",
       "      <td>0.759039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.513747</td>\n",
       "      <td>0.756206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.515914</td>\n",
       "      <td>0.756206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.517750</td>\n",
       "      <td>0.756206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.518128</td>\n",
       "      <td>0.753115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.519573</td>\n",
       "      <td>0.752805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.532690</td>\n",
       "      <td>0.749338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.535001</td>\n",
       "      <td>0.752696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.536135</td>\n",
       "      <td>0.752696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.537490</td>\n",
       "      <td>0.752696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.538696</td>\n",
       "      <td>0.752696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.540659</td>\n",
       "      <td>0.752696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.543977</td>\n",
       "      <td>0.755430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.546327</td>\n",
       "      <td>0.758253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.547938</td>\n",
       "      <td>0.761046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.547223</td>\n",
       "      <td>0.756253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.548615</td>\n",
       "      <td>0.756253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.550074</td>\n",
       "      <td>0.756253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.556177</td>\n",
       "      <td>0.756093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.600019</td>\n",
       "      <td>0.752690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.597046</td>\n",
       "      <td>0.754441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.576349</td>\n",
       "      <td>0.759618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.577858</td>\n",
       "      <td>0.759618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.581285</td>\n",
       "      <td>0.761799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.582539</td>\n",
       "      <td>0.763109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.583987</td>\n",
       "      <td>0.761799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.585617</td>\n",
       "      <td>0.761799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.586988</td>\n",
       "      <td>0.761799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.588514</td>\n",
       "      <td>0.761799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.588529</td>\n",
       "      <td>0.755433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.589723</td>\n",
       "      <td>0.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.591034</td>\n",
       "      <td>0.753037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.592559</td>\n",
       "      <td>0.753037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.594286</td>\n",
       "      <td>0.753037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.596463</td>\n",
       "      <td>0.753389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.598364</td>\n",
       "      <td>0.753389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.617789</td>\n",
       "      <td>0.757494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.616316</td>\n",
       "      <td>0.757645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.617370</td>\n",
       "      <td>0.757645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.618310</td>\n",
       "      <td>0.757066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.619016</td>\n",
       "      <td>0.758355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.609143</td>\n",
       "      <td>0.747892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.609361</td>\n",
       "      <td>0.750104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.646613</td>\n",
       "      <td>0.753944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.649773</td>\n",
       "      <td>0.753483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.648247</td>\n",
       "      <td>0.753944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.640783</td>\n",
       "      <td>0.754513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.641607</td>\n",
       "      <td>0.754513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.642365</td>\n",
       "      <td>0.754513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.641610</td>\n",
       "      <td>0.754406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.639681</td>\n",
       "      <td>0.755006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.626240</td>\n",
       "      <td>0.758961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.628500</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.629813</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.630095</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.630576</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.631165</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.632314</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.633562</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.634177</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.633521</td>\n",
       "      <td>0.760018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.634072</td>\n",
       "      <td>0.758844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.637514</td>\n",
       "      <td>0.760018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.639178</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.640403</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.641107</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.641581</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.642431</td>\n",
       "      <td>0.759547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.653854</td>\n",
       "      <td>0.757432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.661996</td>\n",
       "      <td>0.756463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.663485</td>\n",
       "      <td>0.756463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.664975</td>\n",
       "      <td>0.756463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.666787</td>\n",
       "      <td>0.756463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.668722</td>\n",
       "      <td>0.755997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.671158</td>\n",
       "      <td>0.755997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.672440</td>\n",
       "      <td>0.755997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.692486</td>\n",
       "      <td>0.753834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.694285</td>\n",
       "      <td>0.753370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.694632</td>\n",
       "      <td>0.753370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.669273</td>\n",
       "      <td>0.758048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.669880</td>\n",
       "      <td>0.758048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.670725</td>\n",
       "      <td>0.755217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.671694</td>\n",
       "      <td>0.755217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.672747</td>\n",
       "      <td>0.755217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.673877</td>\n",
       "      <td>0.759829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.675254</td>\n",
       "      <td>0.756404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.683230</td>\n",
       "      <td>0.756731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.689596</td>\n",
       "      <td>0.755812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.690753</td>\n",
       "      <td>0.755812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.693657</td>\n",
       "      <td>0.755341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.694716</td>\n",
       "      <td>0.755341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.695448</td>\n",
       "      <td>0.755341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.696305</td>\n",
       "      <td>0.755341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.697121</td>\n",
       "      <td>0.755336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.697931</td>\n",
       "      <td>0.755336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.698706</td>\n",
       "      <td>0.755336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.700165</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.701263</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.704051</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.704890</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.705800</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.707451</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.707949</td>\n",
       "      <td>0.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.709350</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.710556</td>\n",
       "      <td>0.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.711580</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.712883</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.713602</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.714858</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.716055</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.717220</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.717799</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.718865</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.720244</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.721493</td>\n",
       "      <td>0.756630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.740405</td>\n",
       "      <td>0.754664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.743504</td>\n",
       "      <td>0.753283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.711361</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.711991</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.712600</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.714022</td>\n",
       "      <td>0.756850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.714161</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.716059</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.716484</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.717477</td>\n",
       "      <td>0.756264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.717905</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.718697</td>\n",
       "      <td>0.756264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.719043</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.719244</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.719934</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.720350</td>\n",
       "      <td>0.756733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.720829</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.720977</td>\n",
       "      <td>0.757203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.720710</td>\n",
       "      <td>0.757551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.721321</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.721922</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.722398</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.723274</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.724070</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.724651</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.725002</td>\n",
       "      <td>0.758490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.716063</td>\n",
       "      <td>0.756915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.716928</td>\n",
       "      <td>0.756915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.717238</td>\n",
       "      <td>0.756915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.717972</td>\n",
       "      <td>0.756915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.718320</td>\n",
       "      <td>0.756336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.718396</td>\n",
       "      <td>0.756336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.718724</td>\n",
       "      <td>0.756336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.719108</td>\n",
       "      <td>0.756802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.720190</td>\n",
       "      <td>0.756336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.726774</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.727201</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.728201</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.728574</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.729042</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.729929</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.730459</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.730805</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.731062</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.731435</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.731741</td>\n",
       "      <td>0.757785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.726482</td>\n",
       "      <td>0.755668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.727646</td>\n",
       "      <td>0.755668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.728142</td>\n",
       "      <td>0.755668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.747322</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.747582</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.747819</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.748166</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.748615</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.749743</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.749477</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.750170</td>\n",
       "      <td>0.754959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.755075</td>\n",
       "      <td>0.752902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.756316</td>\n",
       "      <td>0.751884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.756820</td>\n",
       "      <td>0.751884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.757091</td>\n",
       "      <td>0.751293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.760150</td>\n",
       "      <td>0.751412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.760417</td>\n",
       "      <td>0.751412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.760516</td>\n",
       "      <td>0.751412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.761142</td>\n",
       "      <td>0.751412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.761213</td>\n",
       "      <td>0.751412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.760631</td>\n",
       "      <td>0.752429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.762482</td>\n",
       "      <td>0.752861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.761293</td>\n",
       "      <td>0.752624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.762218</td>\n",
       "      <td>0.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.762560</td>\n",
       "      <td>0.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.762867</td>\n",
       "      <td>0.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.763453</td>\n",
       "      <td>0.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.770046</td>\n",
       "      <td>0.748374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.770333</td>\n",
       "      <td>0.748374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.770388</td>\n",
       "      <td>0.747690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.771865</td>\n",
       "      <td>0.750247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.772791</td>\n",
       "      <td>0.750847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.760384</td>\n",
       "      <td>0.749298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.759430</td>\n",
       "      <td>0.748825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.757338</td>\n",
       "      <td>0.749712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.759348</td>\n",
       "      <td>0.755306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.762859</td>\n",
       "      <td>0.757009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.764731</td>\n",
       "      <td>0.757009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.765446</td>\n",
       "      <td>0.757009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39m# callbacks=[ThresholdEarlyStoppingCallback()],\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     31\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(eval_results)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_twitter.to(device)\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=0.000001,\n",
    "    num_train_epochs=1000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps', # Evaluate every 'logging_steps'\n",
    "    logging_steps=100, # Set to evaluate every 100 steps\n",
    "    weight_decay=0.0001,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=f1_macro,\n",
    "    # callbacks=[ThresholdEarlyStoppingCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('pretrained_models/roberta-base-twitter-clf')\n",
    "\n",
    "# model = BertRegression.from_pretrained(\"./path/to/save/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset without labels for testing\n",
    "class ClassificationTestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the test sentences\n",
    "sentences = list(df_test.words_str.values)\n",
    "test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "# Convert to a PyTorch Dataset (using the renamed class)\n",
    "test_dataset = ClassificationTestDataset(test_encodings)\n",
    "\n",
    "# Get predictions with the neural network\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_hat_prob_tensor = torch.tensor(predictions.predictions, dtype=torch.float32)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "y_hat_labels = torch.argmax(y_hat_prob_tensor, dim=1).cpu().numpy()\n",
    "\n",
    "# revert the label encoding\n",
    "y_hat_labels = le.inverse_transform(y_hat_labels)\n",
    "\n",
    "# Save the results with the specified format\n",
    "directory = 'results'\n",
    "np.save(os.path.join(directory, f'{team_id}__{split}__clf_pred.npy'), y_hat_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 20__test_1__reg_pred.npy\n",
    "\n",
    "d = np.load('results/20__test_1__clf_pred.npy', allow_pickle=True)\n",
    "d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
