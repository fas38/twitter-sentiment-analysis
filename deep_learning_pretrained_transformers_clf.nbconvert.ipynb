{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.198172Z",
     "iopub.status.busy": "2023-08-15T11:07:09.197721Z",
     "iopub.status.idle": "2023-08-15T11:07:09.642917Z",
     "shell.execute_reply": "2023-08-15T11:07:09.641990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.647687Z",
     "iopub.status.busy": "2023-08-15T11:07:09.646992Z",
     "iopub.status.idle": "2023-08-15T11:07:09.709289Z",
     "shell.execute_reply": "2023-08-15T11:07:09.708295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.712425Z",
     "iopub.status.busy": "2023-08-15T11:07:09.711912Z",
     "iopub.status.idle": "2023-08-15T11:07:09.912665Z",
     "shell.execute_reply": "2023-08-15T11:07:09.911809Z"
    }
   },
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.915358Z",
     "iopub.status.busy": "2023-08-15T11:07:09.914852Z",
     "iopub.status.idle": "2023-08-15T11:07:09.917975Z",
     "shell.execute_reply": "2023-08-15T11:07:09.917358Z"
    }
   },
   "outputs": [],
   "source": [
    "# def preprocess(text):\n",
    "#     new_text = []\n",
    "#     for t in text.split(\" \"):\n",
    "#         t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "#         t = 'http' if t.startswith('http') else t\n",
    "#         new_text.append(t)\n",
    "#     return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.920235Z",
     "iopub.status.busy": "2023-08-15T11:07:09.919795Z",
     "iopub.status.idle": "2023-08-15T11:07:09.922553Z",
     "shell.execute_reply": "2023-08-15T11:07:09.921936Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['words_str'] = df['text'].apply(preprocess)\n",
    "# df_test['words_str'] = df_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:09.924719Z",
     "iopub.status.busy": "2023-08-15T11:07:09.924322Z",
     "iopub.status.idle": "2023-08-15T11:07:14.268783Z",
     "shell.execute_reply": "2023-08-15T11:07:14.267852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 11:07:12.358218: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 11:07:13.287633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:14.272026Z",
     "iopub.status.busy": "2023-08-15T11:07:14.271369Z",
     "iopub.status.idle": "2023-08-15T11:07:14.275300Z",
     "shell.execute_reply": "2023-08-15T11:07:14.274673Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:14.277392Z",
     "iopub.status.busy": "2023-08-15T11:07:14.277059Z",
     "iopub.status.idle": "2023-08-15T11:07:14.283558Z",
     "shell.execute_reply": "2023-08-15T11:07:14.282893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes ['negative' 'neutral' 'positive']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "X: (8000,)\n",
      "y: (8000,) [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "X = df['words_str']\n",
    "y_text = df['sentiment']\n",
    "# y_text = df.sentiment.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_text)\n",
    "print(f'Original classes {le.classes_}')\n",
    "print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "y =le.transform(y_text)\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape} {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:14.286446Z",
     "iopub.status.busy": "2023-08-15T11:07:14.286269Z",
     "iopub.status.idle": "2023-08-15T11:07:14.291367Z",
     "shell.execute_reply": "2023-08-15T11:07:14.290738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:14.293562Z",
     "iopub.status.busy": "2023-08-15T11:07:14.293137Z",
     "iopub.status.idle": "2023-08-15T11:07:16.530740Z",
     "shell.execute_reply": "2023-08-15T11:07:16.529605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_twitter = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "tokenizer_twitter_sentiment = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "tokenizer_mpnet = AutoTokenizer.from_pretrained('sentence-transformers/stsb-mpnet-base-v2')\n",
    "tokenizer_bert_twitter = AutoTokenizer.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:16.534214Z",
     "iopub.status.busy": "2023-08-15T11:07:16.534010Z",
     "iopub.status.idle": "2023-08-15T11:07:17.690568Z",
     "shell.execute_reply": "2023-08-15T11:07:17.689505Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_bert_twitter\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:17.694290Z",
     "iopub.status.busy": "2023-08-15T11:07:17.694102Z",
     "iopub.status.idle": "2023-08-15T11:07:17.732170Z",
     "shell.execute_reply": "2023-08-15T11:07:17.731507Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.astype('int') # Change to integer type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to long type for classification\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            targets,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "\n",
    "class BertClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class BertClassificationTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationTwitter, self).__init__()\n",
    "        self.num_labels = 3\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "class BertClassificationTwitter_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationTwitter_2, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "class BertClassificationTwitter_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationTwitter_3, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Bi-directional LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Deeper Feed-Forward layers\n",
    "        self.hidden1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Passing through deeper layers\n",
    "        hidden_output = F.leaky_relu(self.hidden1(lstm_out))\n",
    "        hidden_output = self.batchnorm1(hidden_output)\n",
    "        hidden_output = self.dropout1(hidden_output)\n",
    "        \n",
    "        hidden_output = F.leaky_relu(self.hidden2(hidden_output))\n",
    "        hidden_output = self.batchnorm2(hidden_output)\n",
    "        hidden_output = self.dropout2(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2, reduction='mean')\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "class BertClassificationTwitter_4(nn.Module):\n",
    "    def __init__(self, num_classes=3, alpha=1, gamma=2):\n",
    "        super(BertClassificationTwitter_4, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Adding LSTM layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, batch_first=True)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(hidden_size//2, num_classes)\n",
    "        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Instead of using pooled_output, we'll utilize the last hidden state (sequence of embeddings)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "\n",
    "        # Taking the last hidden state of LSTM for classification\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        logits = self.classifier(lstm_out)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.focal_loss(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "class RobertaClassification(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    \n",
    "class RobertaClassificationTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter, self).__init__()\n",
    "        self.num_labels = 3\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Function to compute f1_macro\n",
    "def f1_macro(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {'f1_macro': f1_score(labels, predictions, average='macro')}\n",
    "\n",
    "\n",
    "\n",
    "class RobertaClassificationTwitter_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter_2, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "class RobertaClassificationTwitter_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClassificationTwitter_3, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "class MpnetClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MpnetClassification, self).__init__()\n",
    "        self.mpnet = AutoModel.from_pretrained('sentence-transformers/stsb-mpnet-base-v2')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.mpnet.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final classification layer with 3 classes\n",
    "        self.classifier = nn.Linear(hidden_size//2, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.mpnet(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.classifier(hidden_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "    \n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics['eval_f1_macro']\n",
    "        if f1 > 0.78:\n",
    "            control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:17.735012Z",
     "iopub.status.busy": "2023-08-15T11:07:17.734451Z",
     "iopub.status.idle": "2023-08-15T11:07:17.737634Z",
     "shell.execute_reply": "2023-08-15T11:07:17.736999Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:17.739802Z",
     "iopub.status.busy": "2023-08-15T11:07:17.739452Z",
     "iopub.status.idle": "2023-08-15T11:07:23.363606Z",
     "shell.execute_reply": "2023-08-15T11:07:23.362561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertClassification.from_pretrained('bert-base-uncased')\n",
    "model_roberta = RobertaClassification.from_pretrained('roberta-base')\n",
    "model_twitter = RobertaClassificationTwitter_3()\n",
    "model_mpnet = MpnetClassification()\n",
    "model_bert_twitter = BertClassificationTwitter_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T11:07:23.366939Z",
     "iopub.status.busy": "2023-08-15T11:07:23.366501Z",
     "iopub.status.idle": "2023-08-15T19:14:28.247403Z",
     "shell.execute_reply": "2023-08-15T19:14:28.246720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 8:06:55, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.329238</td>\n",
       "      <td>0.654251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.230923</td>\n",
       "      <td>0.721448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.250575</td>\n",
       "      <td>0.735680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.334922</td>\n",
       "      <td>0.737611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.313544</td>\n",
       "      <td>0.758616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.620169</td>\n",
       "      <td>0.708838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.532077</td>\n",
       "      <td>0.750848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.503720</td>\n",
       "      <td>0.749223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.517863</td>\n",
       "      <td>0.748215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.617731</td>\n",
       "      <td>0.746763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.572313</td>\n",
       "      <td>0.749325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.663154</td>\n",
       "      <td>0.747640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.698595</td>\n",
       "      <td>0.746739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.713094</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.620914</td>\n",
       "      <td>0.757744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.719187</td>\n",
       "      <td>0.743329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.653353</td>\n",
       "      <td>0.759865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.746270</td>\n",
       "      <td>0.743013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.689192</td>\n",
       "      <td>0.753366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.922885</td>\n",
       "      <td>0.734844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.852563</td>\n",
       "      <td>0.728644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.696239</td>\n",
       "      <td>0.753479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.642293</td>\n",
       "      <td>0.758487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.776924</td>\n",
       "      <td>0.741101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.817741</td>\n",
       "      <td>0.738961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.650864</td>\n",
       "      <td>0.761930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.693666</td>\n",
       "      <td>0.754772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.787651</td>\n",
       "      <td>0.760201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.799626</td>\n",
       "      <td>0.741265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.737356</td>\n",
       "      <td>0.738689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.910048</td>\n",
       "      <td>0.738538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.708711</td>\n",
       "      <td>0.751047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.687681</td>\n",
       "      <td>0.747783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.749323</td>\n",
       "      <td>0.742096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.920735</td>\n",
       "      <td>0.725153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.719022</td>\n",
       "      <td>0.742952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.650322</td>\n",
       "      <td>0.741912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.893919</td>\n",
       "      <td>0.737908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.817302</td>\n",
       "      <td>0.739396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.708111</td>\n",
       "      <td>0.750127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.808522</td>\n",
       "      <td>0.765806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.668244</td>\n",
       "      <td>0.759253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.726255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.812885</td>\n",
       "      <td>0.745662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.938953</td>\n",
       "      <td>0.744744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.879609</td>\n",
       "      <td>0.754301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.747711</td>\n",
       "      <td>0.751337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.710221</td>\n",
       "      <td>0.759085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.755547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.785913</td>\n",
       "      <td>0.760476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.814401</td>\n",
       "      <td>0.746364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.764941</td>\n",
       "      <td>0.739088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.721434</td>\n",
       "      <td>0.731584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.648690</td>\n",
       "      <td>0.756353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.888517</td>\n",
       "      <td>0.743792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.793052</td>\n",
       "      <td>0.744006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.765251</td>\n",
       "      <td>0.750328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.709855</td>\n",
       "      <td>0.743676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.824014</td>\n",
       "      <td>0.735797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.748897</td>\n",
       "      <td>0.755309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.923989</td>\n",
       "      <td>0.750712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.659894</td>\n",
       "      <td>0.753169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.785769</td>\n",
       "      <td>0.738864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.797594</td>\n",
       "      <td>0.743530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.722854</td>\n",
       "      <td>0.741170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.740557</td>\n",
       "      <td>0.744968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.008618</td>\n",
       "      <td>0.729578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.750235</td>\n",
       "      <td>0.748838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.756822</td>\n",
       "      <td>0.743741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.745416</td>\n",
       "      <td>0.751455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.768930</td>\n",
       "      <td>0.737124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.697319</td>\n",
       "      <td>0.765287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.696732</td>\n",
       "      <td>0.757897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.751324</td>\n",
       "      <td>0.753260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.754710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.990043</td>\n",
       "      <td>0.737385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.754426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.753372</td>\n",
       "      <td>0.762268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.104101</td>\n",
       "      <td>0.734893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.718090</td>\n",
       "      <td>0.754492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.667627</td>\n",
       "      <td>0.753062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.028657</td>\n",
       "      <td>0.739192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.794574</td>\n",
       "      <td>0.740131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.692778</td>\n",
       "      <td>0.738861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.676390</td>\n",
       "      <td>0.752333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.639900</td>\n",
       "      <td>0.759398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.633836</td>\n",
       "      <td>0.767907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.599785</td>\n",
       "      <td>0.775773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.754329</td>\n",
       "      <td>0.756914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.692401</td>\n",
       "      <td>0.759442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.015056</td>\n",
       "      <td>0.741956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.608877</td>\n",
       "      <td>0.762836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.682820</td>\n",
       "      <td>0.769787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.656037</td>\n",
       "      <td>0.754097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.706432</td>\n",
       "      <td>0.762497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.680474</td>\n",
       "      <td>0.766828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.738451</td>\n",
       "      <td>0.766734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.655384</td>\n",
       "      <td>0.777388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.797813</td>\n",
       "      <td>0.743731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.729994</td>\n",
       "      <td>0.752598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.859588</td>\n",
       "      <td>0.739327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.812598</td>\n",
       "      <td>0.750447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.721349</td>\n",
       "      <td>0.763683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.723451</td>\n",
       "      <td>0.762768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.786976</td>\n",
       "      <td>0.762227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.775168</td>\n",
       "      <td>0.762849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>0.746521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.750168</td>\n",
       "      <td>0.761776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.674570</td>\n",
       "      <td>0.762536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.924205</td>\n",
       "      <td>0.749838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.769590</td>\n",
       "      <td>0.756788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.744446</td>\n",
       "      <td>0.761122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.789752</td>\n",
       "      <td>0.761125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.809515</td>\n",
       "      <td>0.760435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.811787</td>\n",
       "      <td>0.757657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.803587</td>\n",
       "      <td>0.756415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>1.231447</td>\n",
       "      <td>0.739923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.900849</td>\n",
       "      <td>0.751092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.756855</td>\n",
       "      <td>0.756728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.745829</td>\n",
       "      <td>0.766855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.691280</td>\n",
       "      <td>0.762933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.837103</td>\n",
       "      <td>0.754816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.795150</td>\n",
       "      <td>0.754266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.782393</td>\n",
       "      <td>0.756831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.849477</td>\n",
       "      <td>0.752368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.662790</td>\n",
       "      <td>0.762580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.841293</td>\n",
       "      <td>0.753882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.963262</td>\n",
       "      <td>0.742779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.772860</td>\n",
       "      <td>0.762260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.841412</td>\n",
       "      <td>0.761764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.825549</td>\n",
       "      <td>0.752832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.800583</td>\n",
       "      <td>0.758908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.660130</td>\n",
       "      <td>0.746942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.777859</td>\n",
       "      <td>0.756123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.742988</td>\n",
       "      <td>0.758546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.781048</td>\n",
       "      <td>0.752067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.736009</td>\n",
       "      <td>0.751501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.798379</td>\n",
       "      <td>0.751128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.803256</td>\n",
       "      <td>0.752931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.779296</td>\n",
       "      <td>0.761070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.873535</td>\n",
       "      <td>0.753532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.849972</td>\n",
       "      <td>0.759550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.752092</td>\n",
       "      <td>0.766571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.704641</td>\n",
       "      <td>0.757187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.749735</td>\n",
       "      <td>0.746182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.801115</td>\n",
       "      <td>0.755822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.815031</td>\n",
       "      <td>0.755357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.714386</td>\n",
       "      <td>0.754660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.786074</td>\n",
       "      <td>0.750198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.723735</td>\n",
       "      <td>0.758197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.766835</td>\n",
       "      <td>0.753443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.834187</td>\n",
       "      <td>0.762778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.790684</td>\n",
       "      <td>0.760126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.828055</td>\n",
       "      <td>0.756735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.856375</td>\n",
       "      <td>0.758175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.802246</td>\n",
       "      <td>0.763377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.806607</td>\n",
       "      <td>0.765594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.879196</td>\n",
       "      <td>0.759208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.976770</td>\n",
       "      <td>0.751809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.772335</td>\n",
       "      <td>0.759213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.791406</td>\n",
       "      <td>0.761921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.914033</td>\n",
       "      <td>0.759061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.857209</td>\n",
       "      <td>0.772468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.734853</td>\n",
       "      <td>0.763754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.997477</td>\n",
       "      <td>0.757710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.884874</td>\n",
       "      <td>0.764568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.797947</td>\n",
       "      <td>0.770870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>0.764525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.843709</td>\n",
       "      <td>0.770106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.783915</td>\n",
       "      <td>0.773337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.798701</td>\n",
       "      <td>0.774638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.782997</td>\n",
       "      <td>0.770042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.763906</td>\n",
       "      <td>0.767092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.888347</td>\n",
       "      <td>0.759325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.896692</td>\n",
       "      <td>0.752105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.949193</td>\n",
       "      <td>0.750463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.814207</td>\n",
       "      <td>0.762249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.741274</td>\n",
       "      <td>0.758742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.777792</td>\n",
       "      <td>0.757511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.808283</td>\n",
       "      <td>0.760132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.847198</td>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.781468</td>\n",
       "      <td>0.753997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.911709</td>\n",
       "      <td>0.760246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.925382</td>\n",
       "      <td>0.764728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.891375</td>\n",
       "      <td>0.761695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.906063</td>\n",
       "      <td>0.764089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.911376</td>\n",
       "      <td>0.763535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.906486</td>\n",
       "      <td>0.764003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.820958</td>\n",
       "      <td>0.760722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.819156</td>\n",
       "      <td>0.764439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.775492</td>\n",
       "      <td>0.767546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.805229</td>\n",
       "      <td>0.753367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.840113</td>\n",
       "      <td>0.751651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.827874</td>\n",
       "      <td>0.753059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.826527</td>\n",
       "      <td>0.751886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.839472</td>\n",
       "      <td>0.754601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.890999</td>\n",
       "      <td>0.767473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.875087</td>\n",
       "      <td>0.759822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.870186</td>\n",
       "      <td>0.754358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.926091</td>\n",
       "      <td>0.759666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.927753</td>\n",
       "      <td>0.760006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.895306</td>\n",
       "      <td>0.761768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.875014</td>\n",
       "      <td>0.761978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.892535</td>\n",
       "      <td>0.755267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.905403</td>\n",
       "      <td>0.754937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.882701</td>\n",
       "      <td>0.758647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.764826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.820262</td>\n",
       "      <td>0.760394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.793409</td>\n",
       "      <td>0.762265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.792624</td>\n",
       "      <td>0.763025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.826366</td>\n",
       "      <td>0.764514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.840315</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.894048</td>\n",
       "      <td>0.764457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.906749</td>\n",
       "      <td>0.768841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.845183</td>\n",
       "      <td>0.766614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.791087</td>\n",
       "      <td>0.758952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.781842</td>\n",
       "      <td>0.759768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.762302</td>\n",
       "      <td>0.756149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.790007</td>\n",
       "      <td>0.760573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.801531</td>\n",
       "      <td>0.762289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.819704</td>\n",
       "      <td>0.765546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.825856</td>\n",
       "      <td>0.763436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.817480</td>\n",
       "      <td>0.765204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.823517</td>\n",
       "      <td>0.764387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.839545</td>\n",
       "      <td>0.763709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.862796</td>\n",
       "      <td>0.763481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.814151</td>\n",
       "      <td>0.761227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.814683</td>\n",
       "      <td>0.763943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.811390</td>\n",
       "      <td>0.761227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.880839</td>\n",
       "      <td>0.766935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.882240</td>\n",
       "      <td>0.766935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.882839</td>\n",
       "      <td>0.764349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.766910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.865715</td>\n",
       "      <td>0.766913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.861613</td>\n",
       "      <td>0.769625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.877384</td>\n",
       "      <td>0.764349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.870243</td>\n",
       "      <td>0.765970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.871711</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.867666</td>\n",
       "      <td>0.766913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.765157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.863371</td>\n",
       "      <td>0.768208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.866279</td>\n",
       "      <td>0.766913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.869592</td>\n",
       "      <td>0.766913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.864174</td>\n",
       "      <td>0.768208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.868641</td>\n",
       "      <td>0.766913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.862759</td>\n",
       "      <td>0.769152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.865409</td>\n",
       "      <td>0.768208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.870266</td>\n",
       "      <td>0.766441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.868647</td>\n",
       "      <td>0.765157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.863011</td>\n",
       "      <td>0.768208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.863010585308075, 'eval_f1_macro': 0.7682082053350711, 'eval_runtime': 0.8892, 'eval_samples_per_second': 1799.336, 'eval_steps_per_second': 7.872, 'epoch': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "model = model_bert_twitter.to(device)\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=1000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type='cosine',  # Using a cosine scheduler\n",
    "    warmup_steps=100  # Number of warmup steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=f1_macro,\n",
    "    callbacks=[ThresholdEarlyStoppingCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T19:14:28.250053Z",
     "iopub.status.busy": "2023-08-15T19:14:28.249378Z",
     "iopub.status.idle": "2023-08-15T19:14:28.800176Z",
     "shell.execute_reply": "2023-08-15T19:14:28.799161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('pretrained_models/bert-twitter-clf')\n",
    "\n",
    "# model = BertRegression.from_pretrained(\"./path/to/save/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T19:14:28.802652Z",
     "iopub.status.busy": "2023-08-15T19:14:28.802298Z",
     "iopub.status.idle": "2023-08-15T19:14:28.806542Z",
     "shell.execute_reply": "2023-08-15T19:14:28.805906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a dataset without labels for testing\n",
    "class ClassificationTestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T19:14:28.808770Z",
     "iopub.status.busy": "2023-08-15T19:14:28.808224Z",
     "iopub.status.idle": "2023-08-15T19:14:29.507160Z",
     "shell.execute_reply": "2023-08-15T19:14:29.506349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the test sentences\n",
    "sentences = list(df_test.words_str.values)\n",
    "test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "# Convert to a PyTorch Dataset (using the renamed class)\n",
    "test_dataset = ClassificationTestDataset(test_encodings)\n",
    "\n",
    "# Get predictions with the neural network\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_hat_prob_tensor = torch.tensor(predictions.predictions, dtype=torch.float32)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "y_hat_labels = torch.argmax(y_hat_prob_tensor, dim=1).cpu().numpy()\n",
    "\n",
    "# revert the label encoding\n",
    "y_hat_labels = le.inverse_transform(y_hat_labels)\n",
    "\n",
    "# Save the results with the specified format\n",
    "directory = 'results'\n",
    "np.save(os.path.join(directory, f'{team_id}__{split}__clf_pred.npy'), y_hat_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T19:14:29.509537Z",
     "iopub.status.busy": "2023-08-15T19:14:29.509178Z",
     "iopub.status.idle": "2023-08-15T19:14:29.513929Z",
     "shell.execute_reply": "2023-08-15T19:14:29.513288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 20__test_1__reg_pred.npy\n",
    "\n",
    "d = np.load('results/20__test_1__clf_pred.npy', allow_pickle=True)\n",
    "d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
