{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "# df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for ROberta based models\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_str'] = df['text'].apply(preprocess)\n",
    "df_test['words_str'] = df_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 15:26:18.353168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 15:26:19.584394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['words_str'], df['score_compound'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_twitter = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "tokenizer_twitter_sentiment = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "tokenizer_mpnet = AutoTokenizer.from_pretrained('sentence-transformers/stsb-mpnet-base-v2')\n",
    "tokenizer_bert_twitter = AutoTokenizer.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_twitter_sentiment\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.values.astype('float32') # Convert Series to NumPy array and then to float32\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]) # This is already float32\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BertRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        logits = logits.squeeze()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.sqrt(nn.MSELoss()(logits, labels))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "class RobertaRegression(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        logits = logits.squeeze()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.sqrt(nn.MSELoss()(logits, labels))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "    \n",
    "class RobertaRegressionTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaRegressionTwitter, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        logits = logits.squeeze()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.sqrt(nn.MSELoss()(logits, labels))\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "class BertRegressionTwitter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressionTwitter, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        logits = logits.squeeze()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.sqrt(nn.MSELoss()(logits, labels))\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Define a function to compute RMSE\n",
    "def compute_rmse(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {'rmse': np.sqrt(mean_squared_error(labels, predictions))}\n",
    "\n",
    "\n",
    "class BertRegressionTwitter_2(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(BertRegressionTwitter_2, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta) # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels) # Using Huber Loss here\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "class BertRegressionTwitter_3(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(BertRegressionTwitter_3, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Bi-directional LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Deeper Feed-Forward layers\n",
    "        self.hidden1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Passing through deeper layers\n",
    "        hidden_output = F.relu(self.hidden1(lstm_out))\n",
    "        hidden_output = self.batchnorm1(hidden_output)\n",
    "        hidden_output = self.dropout(hidden_output)\n",
    "        \n",
    "        hidden_output = F.relu(self.hidden2(hidden_output))\n",
    "        hidden_output = self.batchnorm2(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "class BertRegressionTwitter_4(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(BertRegressionTwitter_4, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Adding LSTM layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, batch_first=True)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta) # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Instead of using pooled_output, we'll utilize the last hidden state (sequence of embeddings)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        \n",
    "        # Taking the last hidden state of LSTM for regression\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        logits = self.regressor(lstm_out)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels) # Using Huber Loss here\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RobertaRegressionTwitter_2(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(RobertaRegressionTwitter_2, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta) # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels) # Using Huber Loss here\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "    \n",
    "class RobertaRegressionTwitter_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaRegressionTwitter_3, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.sqrt(nn.MSELoss()(logits, labels))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "\n",
    "class MpnetRegression(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(MpnetRegression, self).__init__()\n",
    "        self.mpnet = AutoModel.from_pretrained('sentence-transformers/stsb-mpnet-base-v2')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.mpnet.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        self.huber_loss = nn.SmoothL1Loss(delta) # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.mpnet(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels) # Using Huber Loss here\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=4):\n",
    "        self.patience = patience\n",
    "        self.best_score = None\n",
    "        self.early_stop_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        rmse = metrics['eval_rmse']  # Make sure this key matches what's returned by your compute_metrics function\n",
    "        if self.best_score is None or rmse < self.best_score:\n",
    "            self.best_score = rmse\n",
    "            self.early_stop_counter = 0\n",
    "        else:\n",
    "            self.early_stop_counter += 1\n",
    "            if self.early_stop_counter >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "    \n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        rmse = metrics['eval_rmse'] # Make sure this key matches what's returned by your compute_metrics function\n",
    "        if rmse < 0.18:\n",
    "            control.should_training_stop = True\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(train_encodings, train_labels)\n",
    "val_dataset = RegressionDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertRegression were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['regressor.weight', 'regressor.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaRegression were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'regressor.weight', 'regressor.bias', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertRegression.from_pretrained('bert-base-uncased')\n",
    "model_roberta = RobertaRegression.from_pretrained('roberta-base')\n",
    "model_twitter = RobertaRegressionTwitter_2()\n",
    "model_mpnet = MpnetRegression()\n",
    "model_bert_twitter = BertRegressionTwitter_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8968' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8968/25000 3:09:44 < 5:39:16, 0.79 it/s, Epoch 358.68/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>0.286928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.035377</td>\n",
       "      <td>0.266785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.033861</td>\n",
       "      <td>0.261238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.034527</td>\n",
       "      <td>0.263859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.257093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.032470</td>\n",
       "      <td>0.256033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.251728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>0.249205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.252412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.030880</td>\n",
       "      <td>0.249546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.030141</td>\n",
       "      <td>0.246647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.245410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>0.242444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.029499</td>\n",
       "      <td>0.243956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.028998</td>\n",
       "      <td>0.241847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.027801</td>\n",
       "      <td>0.236780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>0.239340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>0.238993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.027759</td>\n",
       "      <td>0.236684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.237181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>0.237991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.028523</td>\n",
       "      <td>0.239922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>0.232713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.026769</td>\n",
       "      <td>0.232351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.234262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.026926</td>\n",
       "      <td>0.233097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.026740</td>\n",
       "      <td>0.232217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.027459</td>\n",
       "      <td>0.235351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.026988</td>\n",
       "      <td>0.233289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.026562</td>\n",
       "      <td>0.231403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.026641</td>\n",
       "      <td>0.231784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.229611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.230837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.026366</td>\n",
       "      <td>0.230570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>0.229196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>0.228649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.228563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.227991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>0.227906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025841</td>\n",
       "      <td>0.228252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>0.228540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.025386</td>\n",
       "      <td>0.226265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.025568</td>\n",
       "      <td>0.227072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>0.228221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.025582</td>\n",
       "      <td>0.227111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.225155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024940</td>\n",
       "      <td>0.224197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>0.226341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.225755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.223586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024905</td>\n",
       "      <td>0.224064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.228377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.024743</td>\n",
       "      <td>0.223390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>0.226640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.226629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.025129</td>\n",
       "      <td>0.225157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.225588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.025548</td>\n",
       "      <td>0.226954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.024941</td>\n",
       "      <td>0.224212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.224796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>0.224531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.025035</td>\n",
       "      <td>0.224722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024478</td>\n",
       "      <td>0.222160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024673</td>\n",
       "      <td>0.223032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024766</td>\n",
       "      <td>0.223443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024584</td>\n",
       "      <td>0.222667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>0.223304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024718</td>\n",
       "      <td>0.223209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024690</td>\n",
       "      <td>0.223083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>0.222948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024664</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.222674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.223430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024871</td>\n",
       "      <td>0.223879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>0.224278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>0.222107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.223093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.025378</td>\n",
       "      <td>0.226154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.222779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.222557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024682</td>\n",
       "      <td>0.223054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024679</td>\n",
       "      <td>0.223048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024581</td>\n",
       "      <td>0.222585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.222365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>0.223135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.024922</td>\n",
       "      <td>0.224105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>0.223314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.024935</td>\n",
       "      <td>0.224162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     callbacks\u001b[39m=\u001b[39m[ThresholdEarlyStoppingCallback()],\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     34\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(eval_results)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_twitter.to(device)\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=1000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type='cosine',  # Using a cosine scheduler\n",
    "    warmup_steps=100  # Number of warmup steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_rmse,\n",
    "    callbacks=[ThresholdEarlyStoppingCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('pretrained_models/bert-twitter-regression')\n",
    "\n",
    "# model = BertRegression.from_pretrained(\"./path/to/save/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset without labels for testing\n",
    "class RegressionTestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize the test sentences\n",
    "# sentences = list(df_test.words_str.values)\n",
    "# test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "# # Convert to a PyTorch Dataset\n",
    "# test_dataset = RegressionTestDataset(test_encodings)\n",
    "\n",
    "# # Get predictions with the neural network\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "# y_hat_tensor = torch.tensor(predictions.predictions, dtype=torch.float32)\n",
    "\n",
    "# # Convert the predictions back to a numpy array\n",
    "# y_hat = y_hat_tensor.cpu().numpy()\n",
    "\n",
    "# # Save the results with the specified format\n",
    "# directory = 'results'\n",
    "# np.save(os.path.join(directory, f'{team_id}__{split}__reg_pred.npy'), np.squeeze(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 20__test_1__reg_pred.npy\n",
    "\n",
    "d = np.load('results/20__test_1__reg_pred.npy', allow_pickle=True)\n",
    "d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
