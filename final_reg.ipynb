{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 12:26:58.326372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 12:26:59.192141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, AutoTokenizer, AutoModel, PreTrainedModel\n",
    "from transformers import TrainerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "team_id = '20' #put your team id here\n",
    "split = 'test_2' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('dataset/tweets_train.csv')\n",
    "df_test = pd.read_csv(f'dataset/tweets_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for ROberta based models\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_str'] = df['text'].apply(preprocess)\n",
    "df_test['words_str'] = df_test['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['words_str'], df['score_compound'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(text, num_replacements=1):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(random_word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(set(synonyms)))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= num_replacements: \n",
    "            break\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_swap(text, n=1):\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return text\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    augmented_texts.append(text)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    # Add 4 augmented versions\n",
    "    for _ in range(4):\n",
    "        new_text = text\n",
    "        if random.random() > 0.5:\n",
    "            new_text = synonym_replacement(new_text)\n",
    "        if random.random() > 0.5:\n",
    "            new_text = random_swap(new_text)\n",
    "        augmented_texts.append(new_text)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "train_texts = augmented_texts\n",
    "train_labels = augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_twitter_sentiment = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_twitter_sentiment\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = np.array(labels).astype('float32')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]) # This is already float32\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "class StopOnZeroLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        # Check if the training loss is exactly zero\n",
    "        if logs.get(\"loss\", 1) == 0: \n",
    "            print(\"Training loss reached zero, stopping training!\")\n",
    "            control.should_training_stop = True\n",
    "    \n",
    "# Define a function to compute RMSE\n",
    "def compute_rmse(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {'rmse': np.sqrt(mean_squared_error(labels, predictions))}\n",
    "\n",
    "class ThresholdEarlyStoppingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        rmse = metrics['eval_rmse'] # Make sure this key matches what's returned by your compute_metrics function\n",
    "        if rmse < 0.2:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "\n",
    "class BertRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Define and initialize the hidden layers\n",
    "        self.hidden_layer1 = nn.Linear(config.hidden_size, config.hidden_size * 2)\n",
    "        init.kaiming_normal_(self.hidden_layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.hidden_activation1 = nn.ReLU()\n",
    "\n",
    "        self.hidden_layer2 = nn.Linear(config.hidden_size * 2, config.hidden_size * 2)\n",
    "        init.kaiming_normal_(self.hidden_layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.hidden_activation2 = nn.ReLU()\n",
    "\n",
    "        self.hidden_layer3 = nn.Linear(config.hidden_size * 2, config.hidden_size * 2)\n",
    "        init.kaiming_normal_(self.hidden_layer3.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.hidden_activation3 = nn.ReLU()\n",
    "\n",
    "        self.regressor = nn.Linear(config.hidden_size * 2, 1)\n",
    "\n",
    "        self.loss_fn = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        hidden_output1 = self.hidden_activation1(self.hidden_layer1(pooled_output))\n",
    "        hidden_output2 = self.hidden_activation2(self.hidden_layer2(hidden_output1))\n",
    "        hidden_output3 = self.hidden_activation3(self.hidden_layer3(hidden_output2))\n",
    "\n",
    "        logits = self.regressor(hidden_output3)\n",
    "        logits = logits.squeeze()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "\n",
    "class RobertaRegressionTwitter_2(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(RobertaRegressionTwitter_2, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Adding an additional hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size//2)\n",
    "        \n",
    "        # Adding L2 regularization (weight decay) to the hidden layer\n",
    "        self.regularization = nn.LayerNorm(hidden_size//2)\n",
    "        \n",
    "        # Final regression layer\n",
    "        self.regressor = nn.Linear(hidden_size//2, 1)\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta) # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Passing through the hidden layer with ReLU activation\n",
    "        hidden_output = self.hidden_layer(pooled_output)\n",
    "        hidden_output = F.relu(hidden_output)\n",
    "        \n",
    "        # Applying Layer Normalization (regularization)\n",
    "        hidden_output = self.regularization(hidden_output)\n",
    "        \n",
    "        logits = self.regressor(hidden_output)\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels) # Using Huber Loss here\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \n",
    "class RobertaRegressionTwitter_3(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(RobertaRegressionTwitter_3, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Getting RoBERTa's hidden size for reference\n",
    "        roberta_hidden_size = self.roberta.config.hidden_size\n",
    "\n",
    "        # Define the hidden layers\n",
    "        self.hidden_layer1 = nn.Linear(roberta_hidden_size, roberta_hidden_size)  # This is essentially a pass-through layer\n",
    "        self.hidden_layer2 = nn.Linear(roberta_hidden_size, roberta_hidden_size // 2)  # Half of RoBERTa's hidden size\n",
    "        self.hidden_layer3 = nn.Linear(roberta_hidden_size // 2, 160)\n",
    "        self.hidden_layer4 = nn.Linear(160, 80)\n",
    "        self.hidden_layer5 = nn.Linear(80, 40)\n",
    "        self.penultimate_layer = nn.Linear(40, 20)\n",
    "        self.regressor = nn.Linear(20, 1)\n",
    "\n",
    "        # After each hidden layer\n",
    "        self.norm1 = nn.LayerNorm(roberta_hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(roberta_hidden_size // 2)\n",
    "        self.norm3 = nn.LayerNorm(160)\n",
    "        self.norm4 = nn.LayerNorm(80)\n",
    "        self.norm5 = nn.LayerNorm(40)\n",
    "        self.norm6 = nn.LayerNorm(20)\n",
    "\n",
    "        self.huber_loss = nn.HuberLoss(delta=delta)  # Delta controls the transition point in the loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Passing pooled output through the hidden layers with ReLU and LayerNorm\n",
    "        hidden_output1 = F.relu(self.hidden_layer1(pooled_output))\n",
    "        hidden_output1 = self.norm1(hidden_output1)\n",
    "\n",
    "        hidden_output2 = F.relu(self.hidden_layer2(hidden_output1))\n",
    "        hidden_output2 = self.norm2(hidden_output2)\n",
    "\n",
    "        hidden_output3 = F.relu(self.hidden_layer3(hidden_output2))\n",
    "        hidden_output3 = self.norm3(hidden_output3)\n",
    "\n",
    "        hidden_output4 = F.relu(self.hidden_layer4(hidden_output3))\n",
    "        hidden_output4 = self.norm4(hidden_output4)\n",
    "\n",
    "        hidden_output5 = F.relu(self.hidden_layer5(hidden_output4))\n",
    "        hidden_output5 = self.norm5(hidden_output5)\n",
    "\n",
    "        penultimate_output = F.relu(self.penultimate_layer(hidden_output5))\n",
    "        penultimate_output = self.norm6(penultimate_output)\n",
    "\n",
    "        logits = self.regressor(penultimate_output).squeeze()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.huber_loss(logits, labels)  # Using Huber Loss here\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaRegressionTwitter_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(train_encodings, train_labels)\n",
    "val_dataset = RegressionDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1806' max='188000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1806/188000 41:23 < 71:12:47, 0.73 it/s, Epoch 19.20/2000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>0.317520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>0.284956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.040141</td>\n",
       "      <td>0.283994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.038156</td>\n",
       "      <td>0.277102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>0.265584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>0.281408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.261443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.265326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.033868</td>\n",
       "      <td>0.261167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.249478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.032150</td>\n",
       "      <td>0.254373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.251280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.032115</td>\n",
       "      <td>0.254235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.030268</td>\n",
       "      <td>0.246762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.030699</td>\n",
       "      <td>0.248502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.029429</td>\n",
       "      <td>0.243230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.030007</td>\n",
       "      <td>0.245642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.029405</td>\n",
       "      <td>0.243218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     callbacks\u001b[39m=\u001b[39m[ThresholdEarlyStoppingCallback(), StopOnZeroLossCallback()],\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     32\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(eval_results)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mlStuff/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=2000,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.0001,\n",
    "    lr_scheduler_type='cosine',  # Using a cosine scheduler\n",
    "    warmup_steps=100  # Number of warmup steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_rmse,\n",
    "    callbacks=[ThresholdEarlyStoppingCallback(), StopOnZeroLossCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('pretrained_models/best-reg_0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset without labels for testing\n",
    "class RegressionTestDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the test sentences\n",
    "sentences = list(df_test.words_str.values)\n",
    "test_encodings = tokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "# Convert to a PyTorch Dataset\n",
    "test_dataset = RegressionTestDataset(test_encodings)\n",
    "\n",
    "# Get predictions with the neural network\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_hat_tensor = torch.tensor(predictions.predictions, dtype=torch.float32)\n",
    "\n",
    "# Convert the predictions back to a numpy array\n",
    "y_hat = y_hat_tensor.cpu().numpy()\n",
    "\n",
    "# Save the results with the specified format\n",
    "directory = 'results'\n",
    "np.save(os.path.join(directory, f'{team_id}__{split}__reg_pred.npy'), np.squeeze(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 20__test_2__reg_pred.npy\n",
    "\n",
    "d = np.load('results/20__test_2__reg_pred.npy', allow_pickle=True)\n",
    "d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
